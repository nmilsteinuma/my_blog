[
  {
    "path": "posts/welcome/",
    "title": "Welcome to My Blog",
    "description": "Welcome to our new blog, My Blog. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Nora Jones",
        "url": {}
      }
    ],
    "date": "2022-03-29",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "preview": "posts/welcome/welcome_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-03-29T13:23:16-04:00",
    "input_file": "welcome.knit.md"
  },
  {
    "path": "posts/2022-03-29-blog-post-7-networks/",
    "title": "Blog Post 7 Networks",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-03-29",
    "categories": [],
    "contents": "\n\n\n\n\n\n\nAn Introduction to\nthe Project and Dataset\nThe project that I am doing involves conflict in\nthe high middle ages. This was the period between 1000 and 1200\n\n\n\nPart 1:\nDescribe the Dataset You Are\nUsing:\nThe Dataset Being Used: The dataset that I am using\nis wikipedia list of wars throughout history, this article is the “List\nof wars: 1000–1499” which acts as a subset of the “2nd-millennium\nconflicts” I chose this dataset as an exemplar of popular history’s\ndepiction of the centralization of worldwide conflict. Wikipedia, being\nan accessible source generally created from relevant citations makes it\na good case study to see where historical writers and academics center\ntheir world are relevant conflicts.\nIdentify initial network\nformat:\nAnswer: The initial network format is as an edge\nlist, the first, in column contains the winners of each\nwar while the second, out column contains the losers of\neach. These sets of belligerents are directed\nNetwork\nStructure: Wars Startings in the 1000s\n\n Network attributes:\n  vertices = 111 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 153 \n    missing edges= 0 \n    non-missing edges= 153 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nNetwork\nStructure: Wars Startings in the 1100s\n\n Network attributes:\n  vertices = 97 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 238 \n    missing edges= 0 \n    non-missing edges= 238 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nNetwork Structure:\nWars Starting in the 1200s\n\n Network attributes:\n  vertices = 161 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 313 \n    missing edges= 0 \n    non-missing edges= 313 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nIdentify Nodes: Describe and identify the nodes\n(including how many nodes are in the dataset)\nAnswer: Nodes or vertices in these datasets\nrepresent belligerents in wars throughout history, the involved parties\nin each conflict can be a nation, province, individual, or group so long\nas they are listed as involved in the conflict. In the 1000s there are\n117, in the 1100s there are 78 and in the 1200s there are 161.\nWhat Constitutes a Tie: What constitutes a tie or\nedge (including how many ties, whether ties are directed/undirected and\nweighted/binary, and how to interpret the value of the tie if any)\nAnswer: A tie or edge in this dataset represents a\nwar, this war can be between two nations or groups within a nation.\nThese edges can represent a war that involved many more nations but are\nalways tied to each and every party involved on both sides. These edges\nare directed and the direction indicates which side “won” the conflict\n(if an edge has an arrow pointing to another the node that originated\nthat arrow won the war against them. There are 153 edges in the 1000s,\n225 edges in 1100s and 313 edges in the 1200s.\nEdge Attributes and Subset: Whether or not there are\nedge attributes that might be used to subset data or stack multiple\nnetworks (e.g., tie type, year, etc).\nAnswer: There are a number of attributes that could\nbe used to subset the data, year that the conflict began or the length\nof time it lasted are available. Aspects like each side’s religion and\nthe area where the conflict took place could be used to subset the data\nitself.\nPart 2:\nBrokerage and Betweeness\ncentrality\nWhat are betweeness and brokerage cenrrality\nCalculate brokerage and betweenneess centrality measures for one or more\nsubsets of your network data, and write up the results and your\ninterpretation of them.\nAnswer: I will be calculating these measures for\nwars in 1000-1099, 1100-1199, and 1200-1399.\n\n\n\nBrokerage scores in the\n1000s\n\n\n\n\n\n(wars_in_1000s.nodes.stat_2%>%\n  arrange(desc(broker.tot))%>%\n  slice(1:10))[,c(1,11:15)] %>%kable()\n\n\nname\nbroker.tot\nbroker.coord\nbroker.itin\nbroker.rep\nbroker.gate\nByzantine Empire\n22.7376579\nNaN\n3.1654785\nNaN\nNaN\nHoly Roman Empire\n9.2813605\nNaN\n2.2468427\nNaN\nNaN\nSultanate of Rum\n9.2813605\nNaN\n-0.5090648\nNaN\nNaN\nEngland\n6.9745666\nNaN\n5.0036896\n-0.0853606\n-0.0853606\nKingdom of Sicily\n5.0522384\n-0.0176111\n4.0866123\n-0.1201631\n-0.1201631\nSeljuk Empire\n1.9765133\n-0.0176111\n-0.5084146\n3.4677529\n-0.1201631\nKingdom of France\n1.9765133\nNaN\n-0.5090648\nNaN\nNaN\nKingdom of Georgia\n0.8231164\n-0.0176111\n-0.5084146\n-0.1201631\n-0.1201631\nPapal States\n0.4386507\n-0.0176111\n-0.5084146\n-0.1201631\n10.6435850\nGhaznavids\n0.0541851\n-0.1380791\n-0.4907567\n-0.2903366\n-0.2903366\n\nBrokerage scores in the\n1100s\n\n\n\n\n\n\n\n\n(wars_in_1100s.nodes.stat_2%>%\n  arrange(desc(broker.tot))%>%\n  slice(1:10))[,c(1,10:14)] %>%kable()\n\n\nname\nbroker.tot\nbroker.coord\nbroker.itin\nbroker.rep\nbroker.gate\nKingdom of Jerusalem\n17.1050061\nNaN\n2.8705599\n24.5610650\n-0.1357675\nFatimid Caliphate\n10.2415178\nNaN\n-0.6472506\nNaN\nNaN\nAyyubid Dynasty\n9.3615834\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nZengid Dynasty\n7.4257278\nNaN\n0.7591543\nNaN\nNaN\nByzantine Empire\n6.8977671\nNaN\n0.7602887\n-0.1357675\n-0.1357675\nEngland\n5.8418459\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nHoly Roman Empire\n3.0260558\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nKingdom of France\n1.6181608\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nKingdom of Sicily\n0.5622395\n-0.1467125\n-0.6293842\n-0.3476788\n-0.3476788\nPapal States\n0.0342789\n-0.1264908\n-0.6336748\n-0.3236913\n2.5014147\n\nBrokerage scores in the\n1200s\n\n\n\n\n\n\n\n\n\n\nname\nbroker.tot\nbroker.coord\nbroker.itin\nbroker.rep\nbroker.gate\nMongol Empire\n47.964825\nNaN\n-0.5966483\nNaN\nNaN\nKingdom of France\n28.663539\nNaN\n-0.5966483\nNaN\nNaN\nAyyubid Dynasty\n26.995527\nNaN\n2.3528915\nNaN\nNaN\nKingdom of England\n21.991489\nNaN\n8.9893561\nNaN\nNaN\nRepublic of Genoa\n11.983415\nNaN\n-0.5966483\nNaN\nNaN\nKnights Templar\n10.077115\nNaN\n1.6155066\nNaN\nNaN\nHoly Roman Empire\n4.834790\n-0.0170801\n-0.5961482\n10.865523\n10.865523\nPrincipality of Antioch\n4.834790\n-0.0170801\n2.3541101\n13.613565\n-0.126648\nKingdom of Cyprus\n4.596503\n58.5391124\n0.1414163\n13.613565\n10.865523\nArmenian Kingdom of Cilicia\n3.881640\n-0.0170801\n-0.5961482\n-0.126648\n-0.126648\n\n\nname\nbroker.gate\nPapal States\n10.6435850\nCounty of Aversa\n-0.0853606\nCounty of Sicily\n-0.0853606\nEngland\n-0.0853606\nChola Empire\n-0.0853606\nCounty of Apulia\n-0.1201631\nKingdom of Sicily\n-0.1201631\nKingdom of Georgia\n-0.1201631\nGreat Seljuq Empire\n-0.1201631\nSeljuk Empire\n-0.1201631\nname\nbroker.tot\nByzantine Empire\n22.7376579\nHoly Roman Empire\n9.2813605\nSultanate of Rum\n9.2813605\nEngland\n6.9745666\nKingdom of Sicily\n5.0522384\nSeljuk Empire\n1.9765133\nKingdom of France\n1.9765133\nKingdom of Georgia\n0.8231164\nPapal States\n0.4386507\nGhaznavids\n0.0541851\n\nOption 2.A\nFor a Specific Research Question: If you have a\nspecific research question, please feel free to use that to guide your\nanalysis. Otherwise, you may want to orient your analysis as follows in\norder to identify a compelling question or noteworthy pattern in the\ndata that can be interpreted.\nAnswer: Since I am interested in the relative power\nof nations by their relative position ad centrality in the worldwide\nconflict, network brokerage can be used to illustrate significant\npositions in global conflict. Below I wanted to look at 4 kinds of\nbrokerage, these are broker.gate or gatekeeper, coordinator, liason, and\nitinerant. I am interested to see if these specific coordination types\nare primarily done by specific nations.\n\n\n\n\n\n\n\n\n\nTotal Brokerage\nExplanation: Looking at total brokerage in this\ndataset gives a sense of which factions were responsible for highest\nconnection of unconnected actors through conflict. Given the crusades\nigniting conflict between Europe and the middle east it is sensible that\nthe Byzantine Empire in the center of both connects the most unconnected\nactors through conflict closely followed by the Sultanate of Rum, a\nmajor Muslim faction that fought against the crusades and third being\nthe Holy Roman Empire who participated in many conflicts including the\ncrusades. These are followed by England who centered the wars in the\nBritish isles and the Kingdom of Sicily who were also in a position of\nconflict.\n\nname\nbroker.tot\nByzantine Empire\n22.737658\nHoly Roman Empire\n9.281360\nSultanate of Rum\n9.281360\nEngland\n6.974567\nKingdom of Sicily\n5.052238\n\nCoordinator Brokerage\nExplanation: In this case no particular country is\nvery high above any other in terms of their coordinator brokerage,\nmeaning that within groups no particular nations appear to be brokering\nmore within the groups.\n\nname\nbroker.coord\nCounty of Apulia\n-0.0176111\nKingdom of Sicily\n-0.0176111\nKingdom of Georgia\n-0.0176111\nGreat Seljuq Empire\n-0.0176111\nPapal States\n-0.0176111\n\nItinerant Brokerage\nExplanation: Itinerant brokerage represents when a\nnon-group actor connects 2 actors in a group it is no in to each other,\nin this case England has the highest score. Looking at the network graph\nthey do appear to connect 2 actors in a group together.\n\nname\nbroker.itin\nEngland\n5.0036896\nKingdom of Sicily\n4.0866123\nByzantine Empire\n3.1654785\nHoly Roman Empire\n2.2468427\nPrincipality of Kiev\n0.4812412\n\nRepresentative Brokerage\nExplanation: Representative brokerage indicates that\nthe broker, or nation in question loses a war to another in their group,\nbut wins another against a faction outside of their group. This can be\nthough of as their directed connections to them. In this case the Seljuk\nEmpire and Kingdom of Aragon have instances in which they lose to\nfactions within their group before beating those outside of it.\n\nname\nbroker.rep\nSeljuk Empire\n3.4677529\nKingdom of Aragon\n0.9281821\nCounty of Aversa\n-0.0853606\nCounty of Sicily\n-0.0853606\nEngland\n-0.0853606\n\nGatekeeper Brokerage\nExplanation: The Papal states being ranked highest\nin gatekeeper brokerage is an interesting observation as no other nation\nin the dataset appears to be close to their level as most are negative\nin this category. In this cae being a gatekeeper means that they are in\nat conflict in a group with another while the nation in a different\ngroup of conflicts is only at war with them from the group. This is an\ninteresting observation given the Papal states role as a coordinator of\nthe war, but not a participant in the conflcit as directly as other\nbelligerents. (This being the crusade given the period)\n\nname\nbroker.gate\nPapal States\n10.6435850\nCounty of Aversa\n-0.0853606\nCounty of Sicily\n-0.0853606\nEngland\n-0.0853606\nChola Empire\n-0.0853606\n\nLiaison Brokerage\nExplanation: A liaison broker, in this case, is a\nfaction that loses a war to a group they do not belong to and wins a war\nagainst a different group than the first that they also do not belong\nto. The Byzantine Empire, Sultanate of Rum, and Holy Roman Empire are\nhighest in this category likely owing to their frequent states of\nconflict beyond the crusades against a variety of groups.\n\nname\nbroker.lia\nByzantine Empire\n28.140866\nSultanate of Rum\n12.477603\nHoly Roman Empire\n10.961803\nEngland\n6.548214\nKingdom of Sicily\n4.589419\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork\n1000s Plot Grouping Determined with No Cluster Method\n\n\n\nNetwork\n1000s Plot Grouping Determined with the Average Cluster Method\n\n\n\nNetwork\n1000s Plot Grouping Determined with the Single Cluster Method\n\n\n\nNetwork\n1000s Plot Grouping Determined with the Ward.D Cluster Method\n\n\n\nNetwork 1000s Plot igraph\n\n\n\nNetwork Graphing 1100s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork\n1100s Plot Grouping Determined with No Cluster Method\n\n\n\nNetwork\n1100s Plot Grouping Determined with the Average Cluster Method\n\n\n\nNetwork\n1100s Plot Grouping Determined with the Single Cluster Method\n\n\n\nNetwork\n1100s Plot Grouping Determined with the Ward.D Cluster Method\n\n\n\nNetwork 1100s Plot igraph\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwars_in_1000s_edgelist <- as.matrix(wars_in_1000s)\n\nwars_in_1000s_edgelist_network_edgelist <- graph.edgelist(wars_in_1000s_edgelist, directed=TRUE)\n\nwars_in_1000s.ig<-graph_from_data_frame(wars_in_1000s)\n\nwars_in_1000s_network <- asNetwork(wars_in_1000s.ig)\n\n\n\n\n\naspects_of_1000s_states <- read_excel(\"~/Desktop/Spring 2022/Networks/aspects_of_1000s_states.xlsx\")\n\ntotal_1000s <- merge(aspects_of_1000s_states, wars_in_1000s.nodes.stat_2, by=\"name\")\n\n\n\n\n\ntotal_1000s_brokerag_reg<-total_1000s\n\ntotal_1000s_brokerag_reg$win_rate <- (total_1000s_brokerag_reg$outdegree/total_1000s_brokerag_reg$totdegree)\n\ntotal_1000s_brokerag_reg$loss_rate <- (total_1000s_brokerag_reg$indegree/total_1000s_brokerag_reg$totdegree)\n\ntotal_1000s_brokerag_reg_binom <- total_1000s_brokerag_reg %>% mutate(more_win_or_loss = case_when(\n  win_rate < 0.5 ~ 0,\n    win_rate >= 0.5 ~ 1))\n\nFirst_1000s_regression <- glm(more_win_or_loss~.-name-totdegree-indegree-outdegree-dc-eigen.dc-win_rate-loss_rate, total_1000s_brokerag_reg_binom, family=binomial)\n\nFirst_1000s_regression\n\n\n\nCall:  glm(formula = more_win_or_loss ~ . - name - totdegree - indegree - \n    outdegree - dc - eigen.dc - win_rate - loss_rate, family = binomial, \n    data = total_1000s_brokerag_reg_binom)\n\nCoefficients:\n (Intercept)      Catholic         Islam      Orthodox      Buddhist  \n  -2.090e+01     1.446e-01    -7.108e-02    -4.043e-01    -8.572e-02  \n       Pagan      Tengrism        Shinto         Hindu     Shamanism  \n   5.506e-01    -5.656e+01     1.820e+00    -2.142e+00    -1.506e+00  \n       eigen         close            rc      eigen.rc    broker.tot  \n  -1.877e+03     5.146e+03    -3.979e+00     1.574e+03     2.378e+02  \nbroker.coord   broker.itin    broker.rep   broker.gate    broker.lia  \n  -9.610e+01    -9.449e+01    -7.164e+01    -2.810e+01    -1.298e+02  \n\nDegrees of Freedom: 101 Total (i.e. Null);  82 Residual\n  (8 observations deleted due to missingness)\nNull Deviance:      140.8 \nResidual Deviance: 4.53e-09     AIC: 40\n\n\n\nset.seed(292)\n\ntotal_1000s_for_regression <- total_1000s[,-c(1, 20:25)]\n\ntotal_1000s_for_regression$win_rate <- (total_1000s_for_regression$outdegree/total_1000s_for_regression$totdegree)\n\ntotal_1000s_for_regression$loss_rate <- (total_1000s_for_regression$indegree/total_1000s_for_regression$totdegree)\n\ntotal_1000s_for_regression <- total_1000s_for_regression %>% mutate(more_win_or_loss = case_when(\n  win_rate < 0.5 ~ 0,\n    win_rate >= 0.5 ~ 1))\n\nFirst_1000s_regression <- glm(more_win_or_loss~.-loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, total_1000s_for_regression, family=binomial)\n\nFirst_1000s_regression\n\n\n\nCall:  glm(formula = more_win_or_loss ~ . - loss_rate - win_rate - totdegree - \n    indegree - outdegree - dc - eigen.dc, family = binomial, \n    data = total_1000s_for_regression)\n\nCoefficients:\n(Intercept)     Catholic        Islam     Orthodox     Buddhist  \n   -15.1948      13.9008      12.7531      14.6893      15.0858  \n      Pagan     Tengrism       Shinto        Hindu    Shamanism  \n     0.9610      11.6691      16.0623       9.1358      -0.1497  \n      eigen        close           rc     eigen.rc  \n   -82.1100     256.5294      -3.3322     -17.3152  \n\nDegrees of Freedom: 109 Total (i.e. Null);  96 Residual\nNull Deviance:      152.3 \nResidual Deviance: 58.4     AIC: 86.4\n\n\n\nset.seed(6738)\n\nin_training<- sample(1:nrow(total_1000s_for_regression),  nrow(total_1000s_for_regression) * 0.7 )\n\ntraining_1000s <- total_1000s_for_regression[in_training,]\n\ntest_1000s <- total_1000s_for_regression[-in_training,]\n\nlm_1000s_binom_subset_1 <- glm(more_win_or_loss~.-loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, total_1000s_for_regression, family=binomial, subset = in_training )\n\nlogsitic_1_1000s_prob <- predict(lm_1000s_binom_subset_1, test_1000s,\ntype = \"response\")\n\nlog_preds_1<-ifelse(logsitic_1_1000s_prob >= 0.5, 1, 0)\n\nprediction_1_logs <-mean(log_preds_1 == test_1000s$more_win_or_loss)\n\nprediction_1_logs %>% kable()\n\n\nx\n0.9090909\n\n\n\nlibrary(glmnet)\nlibrary(MASS)\n\n\n\n\n\nset.seed(246)\n\nx_ridge <- model.matrix(more_win_or_loss ~ .-loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, total_1000s_for_regression)[, -1] \n\ny_ridge <- total_1000s_for_regression$more_win_or_loss\n\ngrid <- 10^seq(10, -2, length = 100)\n\nridge.mod <- glmnet(x_ridge, y_ridge, alpha = 0, lambda = grid)\n\ndim(coef(ridge.mod))\n\n\n[1]  14 100\n\n\n\nset.seed(729)\ntrain_ridge <- sample(1:nrow(x_ridge), nrow(x_ridge)*0.8 ) \n\ntest_ridge <- (-train_ridge)\n\ny.test_ridge <- y_ridge[test_ridge]\n\n\n\n\n\nset.seed(9292)\n\nridge.mod <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], \n                    alpha = 0, lambda = grid, thresh = 1e-12)\n\nridge.pred <- predict(ridge.mod, s = 4, newx = x_ridge[test_ridge,])\n\nmean((ridge.pred - y.test_ridge)^2) %>% kable()\n\n\nx\n0.2416376\n\n\n\nset.seed(231)\nridge.pred <- predict(ridge.mod, s = 0, newx = x_ridge[test_ridge, ], \n                      exact = T, x = x_ridge[train_ridge, ], y = y_ridge[train_ridge])\n\npredict(ridge.mod, s = 0, exact = T, type = \"coefficients\", \n        x = x_ridge[train_ridge, ], y = y_ridge[train_ridge])[1:14, ]\n\n\n(Intercept)    Catholic       Islam    Orthodox    Buddhist \n 0.21024033  0.21827317 -0.01160454  0.21312966  0.35601806 \n      Pagan    Tengrism      Shinto       Hindu   Shamanism \n 0.08955257  0.14069809  0.38278477 -0.07034364 -0.01038790 \n      eigen       close          rc    eigen.rc \n-4.61480591 12.51011844 -0.29977861  4.64835194 \n\n\n\nset.seed(9292)\n\ncv.out <- cv.glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], alpha = 0) \n\nplot(cv.out)\n\n\n\n\n\n\nset.seed(9292)\n\nbestlam <- cv.out$lambda.min\n\nbestlam\n\n\n[1] 0.415338\n\n\n\nset.seed(9292)\n\nridge.pred <- predict(cv.out, s = bestlam, newx = x_ridge[test_ridge,])\n\nmean((ridge.pred - y.test_ridge)^2) %>% kable()\n\n\nx\n0.174632\n\n\n\nset.seed(2897)\n\nx_lasso <- model.matrix(more_win_or_loss ~ .-loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, total_1000s_for_regression)[, -1] \n\ny_lasso <- total_1000s_for_regression$more_win_or_loss\n\ngrid <- 10^seq(10, -2, length = 100)\n\nlasso.mod <- glmnet(x_lasso, y_lasso, alpha = 0, lambda = grid)\n\ndim(coef(lasso.mod))\n\n\n[1]  14 100\n\n\n\nset.seed(729)\n\ntrain_lasso <- sample(1:nrow(x_ridge), nrow(x_ridge)*0.8 ) \n\ntest_lasso <- (-train_lasso)\n\ny.test_lasso <- y_lasso[test_lasso]\n\n\n\n\n\nset.seed(9292)\n\nlasso.mod <- glmnet(x_lasso[train_lasso, ], y_lasso[train_lasso], \n                    alpha = 1, lambda = grid)\n\nplot(lasso.mod)\n\n\n\n\n\n\nset.seed(1029)\n\ncv.out_2 <- cv.glmnet(x_lasso[train_lasso, ], y_lasso[train_lasso], alpha = 1) \n\nplot(cv.out_2)\n\n\n\n\n\n\nset.seed(1920)\n\nbestlam_2 <- cv.out_2$lambda.min\n\nlasso.pred <- predict(cv.out_2, s = bestlam_2, newx = x_ridge[test_ridge,])\n\nmean((lasso.pred - y.test_ridge)^2) %>% kable()\n\n\nx\n0.1749583\n\n\n\nset.seed(2739)\n\nout <- glmnet(x_lasso[train_lasso, ], y_lasso[train_lasso], \n              alpha = 1, lambda = grid)\n\nlasso.coef <- predict(out, type = \"coefficients\", s = bestlam_2)[1:14, ]\n\nlasso.coef\n\n\n(Intercept)    Catholic       Islam    Orthodox    Buddhist \n 0.42561685  0.05577020 -0.09275344  0.00000000  0.00000000 \n      Pagan    Tengrism      Shinto       Hindu   Shamanism \n 0.00000000  0.00000000  0.00000000  0.00000000  0.00000000 \n      eigen       close          rc    eigen.rc \n 0.00000000  3.22570629 -0.21240622  0.00000000 \n\n\n\naspects_of_1100s_states <- read_excel(\"~/Desktop/Spring 2022/Networks/aspects_of_1100s_states.xlsx\")\n\ntotal_1100s <- merge(aspects_of_1100s_states, wars_in_1100s.nodes.stat_2, by=\"name\")\n\n\n\n\n\naspects_of_1200s_states <- read_excel(\"~/Desktop/Spring 2022/Networks/aspects_of_1200s_states.xlsx\")\n\ntotal_1200s <- merge(aspects_of_1200s_states, wars_in_1200s.nodes.stat_2, by=\"name\")\n\n\n\nCommunity Grouping\nLabel Propagation 1000s:\nThe first community cluster below is done using label propagation.\nThis results in 39 groups\n\n\nset.seed(23)\ncomm.lab<-label.propagation.community(wars_in_1000s.ig)\n#Inspect clustering object\n# igraph::groups(comm.lab)\n\n\n\n\n\n\nWalktrap 1000s:\nWalktrap classification as seen below results in 19 distinct\ncommunities.\n\n\nset.seed(238)\n#Run clustering algorithm: fast_greedy\nwars_in_1000s.wt<-walktrap.community(wars_in_1000s.ig)\n\n#igraph::groups(wars_in_1000s.wt)\n\n\n\nAdding more steps resulted in 19 groups for both 10 and 20 steps.\n\n\n#Run & inspect clustering algorithm: 10 steps\n#igraph::groups(walktrap.community(wars_in_1000s.ig, steps=10)) \n#Run & inspect clustering algorithm: 20 steps\n#igraph::groups(walktrap.community(wars_in_1000s.ig ,steps=20))\n#Run & inspect clustering algorithm\n\n\n\n\n\n\nMachine\nLearning, Regression and Principle Components:\n\n\ntotal_1000s_for_PCA <- total_1000s_brokerag_reg_binom[-c(20:27)]\n\napply(total_1000s_for_PCA[-1], 2, mean)\n\n\n        Catholic            Islam         Orthodox         Buddhist \n     0.454545455      0.181818182      0.154545455      0.063636364 \n           Pagan         Tengrism           Shinto            Hindu \n     0.036363636      0.018181818      0.054545455      0.045454545 \n       Shamanism        totdegree         indegree        outdegree \n     0.009090909      2.754545455      1.336363636      1.418181818 \n           eigen            close               rc         eigen.rc \n     0.028058711      0.023546832      0.287358773      0.003637773 \n              dc         eigen.dc more_win_or_loss \n     0.712641227      0.024420939      0.481818182 \n\n\n\napply(total_1000s_for_PCA[-1], 2, var)\n\n\n        Catholic            Islam         Orthodox         Buddhist \n    0.2502085071     0.1501251043     0.1318598832     0.0601334445 \n           Pagan         Tengrism           Shinto            Hindu \n    0.0353628023     0.0180150125     0.0520433695     0.0437864887 \n       Shamanism        totdegree         indegree        outdegree \n    0.0090909091     8.9208507089     2.6656380317     6.3189324437 \n           eigen            close               rc         eigen.rc \n    0.0076304265     0.0019575460     0.1260782284     0.0004728954 \n              dc         eigen.dc more_win_or_loss \n    0.1260782284     0.0056490031     0.2519599666 \n\n\n\npr.out <- prcomp(total_1000s_for_PCA[-1], scale = TRUE)\n\n\n\n\n\nnames(pr.out)\n\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\n\npr.out$center\n\n\n        Catholic            Islam         Orthodox         Buddhist \n     0.454545455      0.181818182      0.154545455      0.063636364 \n           Pagan         Tengrism           Shinto            Hindu \n     0.036363636      0.018181818      0.054545455      0.045454545 \n       Shamanism        totdegree         indegree        outdegree \n     0.009090909      2.754545455      1.336363636      1.418181818 \n           eigen            close               rc         eigen.rc \n     0.028058711      0.023546832      0.287358773      0.003637773 \n              dc         eigen.dc more_win_or_loss \n     0.712641227      0.024420939      0.481818182 \n\n\n\npr.out$scale\n\n\n        Catholic            Islam         Orthodox         Buddhist \n      0.50020846       0.38745981       0.36312516       0.24522122 \n           Pagan         Tengrism           Shinto            Hindu \n      0.18805000       0.13422002       0.22813016       0.20925221 \n       Shamanism        totdegree         indegree        outdegree \n      0.09534626       2.98677932       1.63267818       2.51374868 \n           eigen            close               rc         eigen.rc \n      0.08735231       0.04424416       0.35507496       0.02174616 \n              dc         eigen.dc more_win_or_loss \n      0.35507496       0.07515985       0.50195614 \n\n\n\n\n\n\nbiplot(pr.out, scale = 0)\n\n\n\n\n\n\npr.out$rotation = -pr.out$rotation \n\npr.out$x = -pr.out$x\n\nbiplot(pr.out, scale = 0)\n\n\n\n\n\n\npr.out$sdev\n\n\n [1] 2.217501e+00 1.681548e+00 1.239242e+00 1.211199e+00 1.065982e+00\n [6] 1.037692e+00 1.029507e+00 1.011117e+00 1.005425e+00 9.514802e-01\n[11] 8.848499e-01 7.782431e-01 6.162540e-01 4.426224e-01 2.541422e-01\n[16] 1.091189e-01 7.597269e-16 6.258811e-16 2.174635e-16\n\n\n\npr.var <- pr.out$sdev^2\n\npr.var\n\n\n [1] 4.917311e+00 2.827605e+00 1.535720e+00 1.467004e+00 1.136318e+00\n [6] 1.076804e+00 1.059884e+00 1.022359e+00 1.010879e+00 9.053146e-01\n[11] 7.829594e-01 6.056623e-01 3.797690e-01 1.959146e-01 6.458828e-02\n[16] 1.190694e-02 5.771849e-31 3.917271e-31 4.729037e-32\n\n\n\npve <- pr.var / sum(pr.var)\n\npve\n\n\n [1] 2.588059e-01 1.488213e-01 8.082739e-02 7.721075e-02 5.980622e-02\n [6] 5.667390e-02 5.578337e-02 5.380835e-02 5.320417e-02 4.764814e-02\n[11] 4.120839e-02 3.187696e-02 1.998784e-02 1.031129e-02 3.399383e-03\n[16] 6.266808e-04 3.037815e-32 2.061722e-32 2.488967e-33\n\n\n\npar(mfrow = c(1, 2))\nplot(pve, xlab = \"Principal Component\",\nylab = \"Proportion of Variance Explained\", ylim = c(0, 1),\ntype = \"b\")\n\nplot(cumsum(pve), xlab = \"Principal Component\",\nylab = \"Cumulative Proportion of Variance Explained\", ylim = c(0, 1), type = \"b\")\n\n\n\n\n\n\nnames(total_1200s)\n\n\n [1] \"name\"         \"Catholic\"     \"Islam\"        \"Orthodox\"    \n [5] \"Buddhist\"     \"Pagan\"        \"Tengrism\"     \"Shinto\"      \n [9] \"Hindu\"        \"Shamanism\"    \"totdegree\"    \"indegree\"    \n[13] \"outdegree\"    \"eigen\"        \"rc\"           \"eigen.rc\"    \n[17] \"dc\"           \"eigen.dc\"     \"broker.tot\"   \"broker.coord\"\n[21] \"broker.itin\"  \"broker.rep\"   \"broker.gate\"  \"broker.lia\"  \n\n\n\ntotal_1200s_brokerag_reg<-total_1200s\n\n\n\n\n\ntotal_1200s_brokerag_reg$win_rate <- (total_1200s_brokerag_reg$outdegree/total_1200s_brokerag_reg$totdegree)\n\n\n\n\n\ntotal_1200s_brokerag_reg$loss_rate <- (total_1200s_brokerag_reg$indegree/total_1200s_brokerag_reg$totdegree)\n\n\n\n\n\ntotal_1200s_brokerag_reg_binom <- total_1200s_brokerag_reg %>% mutate(more_win_or_loss = case_when(\n  win_rate < 0.5 ~ 0,\n    win_rate >= 0.5 ~ 1))\n\n\n\n\n\ntotal_1200s_for_PCA <- total_1200s_brokerag_reg_binom[-c(20:27)]\n\n\napply(total_1200s_for_PCA[-1], 2, mean)\n\n\n   Catholic       Islam    Orthodox    Buddhist       Pagan \n0.712500000 0.068750000 0.087500000 0.087500000 0.012500000 \n   Tengrism      Shinto       Hindu   Shamanism   totdegree \n0.025000000 0.000000000 0.006250000 0.000000000 3.918750000 \n   indegree   outdegree       eigen          rc    eigen.rc \n1.962500000 1.956250000 0.025567955 0.158754617 0.002192746 \n         dc    eigen.dc  broker.tot \n0.841245383 0.023375209 0.341581810 \n\n\n\napply(total_1200s_for_PCA[-1], 2, var)\n\n\n    Catholic        Islam     Orthodox     Buddhist        Pagan \n2.061321e-01 6.442610e-02 8.034591e-02 8.034591e-02 1.242138e-02 \n    Tengrism       Shinto        Hindu    Shamanism    totdegree \n2.452830e-02 0.000000e+00 6.250000e-03 0.000000e+00 2.666631e+01 \n    indegree    outdegree        eigen           rc     eigen.rc \n6.237579e+00 1.595405e+01 5.631476e-03 7.141295e-02 7.316162e-05 \n          dc     eigen.dc   broker.tot \n7.141295e-02 4.574350e-03 3.001236e+01 \n\n\n\n# I cannot scale variables with \n\ntotal_1200s_for_PCA<-total_1200s_for_PCA[-c(8,10)]\n\n\n\n\n\npr.out_2 <- prcomp(total_1200s_for_PCA[-1], scale = TRUE)\n\n\n\n\n\nnames(pr.out_2)\n\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\n\npr.out_2$center\n\n\n   Catholic       Islam    Orthodox    Buddhist       Pagan \n0.712500000 0.068750000 0.087500000 0.087500000 0.012500000 \n   Tengrism       Hindu   totdegree    indegree   outdegree \n0.025000000 0.006250000 3.918750000 1.962500000 1.956250000 \n      eigen          rc    eigen.rc          dc    eigen.dc \n0.025567955 0.158754617 0.002192746 0.841245383 0.023375209 \n broker.tot \n0.341581810 \n\n\n\npr.out_2$scale\n\n\n   Catholic       Islam    Orthodox    Buddhist       Pagan \n0.454017704 0.253822971 0.283453545 0.283453545 0.111451261 \n   Tengrism       Hindu   totdegree    indegree   outdegree \n0.156615139 0.079056942 5.163943541 2.497514488 3.994251963 \n      eigen          rc    eigen.rc          dc    eigen.dc \n0.075043164 0.267232010 0.008553457 0.267232010 0.067633938 \n broker.tot \n5.478353760 \n\n\n\n\n\n\nbiplot(pr.out_2, scale = 0)\n\n\n\n\n\n\npr.out_2$rotation = -pr.out_2$rotation \n\npr.out_2$x = -pr.out_2$x\n\nbiplot(pr.out_2, scale = 0)\n\n\n\n\n\n\npr.out$sdev\n\n\n [1] 2.217501e+00 1.681548e+00 1.239242e+00 1.211199e+00 1.065982e+00\n [6] 1.037692e+00 1.029507e+00 1.011117e+00 1.005425e+00 9.514802e-01\n[11] 8.848499e-01 7.782431e-01 6.162540e-01 4.426224e-01 2.541422e-01\n[16] 1.091189e-01 7.597269e-16 6.258811e-16 2.174635e-16\n\n\n\npr.var_2 <- pr.out_2$sdev^2\n\npr.var_2\n\n\n [1] 4.903737e+00 2.344663e+00 1.670548e+00 1.250176e+00 1.132904e+00\n [6] 1.097802e+00 1.011326e+00 9.460639e-01 8.661454e-01 5.139677e-01\n[11] 1.659928e-01 9.667541e-02 2.916516e-30 4.832251e-31 2.292490e-31\n[16] 1.889562e-32\n\n\n\npve_2 <- pr.var_2 / sum(pr.var_2)\n\npve_2\n\n\n [1] 3.064835e-01 1.465414e-01 1.044092e-01 7.813602e-02 7.080651e-02\n [6] 6.861260e-02 6.320785e-02 5.912899e-02 5.413409e-02 3.212298e-02\n[11] 1.037455e-02 6.042213e-03 1.822822e-31 3.020157e-32 1.432806e-32\n[16] 1.180977e-33\n\n\n\npar(mfrow = c(1, 2))\nplot(pve_2, xlab = \"Principal Component\",\nylab = \"Proportion of Variance Explained\", ylim = c(0, 1),\ntype = \"b\")\n\nplot(cumsum(pve_2), xlab = \"Principal Component\",\nylab = \"Cumulative Proportion of Variance Explained\", ylim = c(0, 1), type = \"b\")\n\n\n\n\n(information regarding the meaning of each type of brokerage was\nacquired from https://edis.ifas.ufl.edu/publication/WC197)\n\n\n\n",
    "preview": "posts/2022-03-29-blog-post-7-networks/blog-post-7-networks_files/figure-html5/unnamed-chunk-18-1.png",
    "last_modified": "2022-03-29T12:51:28-04:00",
    "input_file": "blog-post-7-networks.knit.md"
  },
  {
    "path": "posts/2022-03-29-tad-post-4/",
    "title": "TAD Post 4",
    "description": "In last weeks post I combined a the material for blog posts 2, 3, and began on the 4th post. As a result this will be a continuation of dictionary validation methods.",
    "author": [
      {
        "name": "Nora Jones",
        "url": {}
      }
    ],
    "date": "2022-03-29",
    "categories": [],
    "contents": "\n\n\n\nBlog Post 3 and 4:\nThird Blog Post and\nContinuation\nExplanation: This post covers material from week 5\npre-processing, week 6 representing text, and week 7 dictionary methods.\nIn this case I have slightly modified the data I am working with, this\nweek I am using 1000 new posts for a dictionary analysis rather than the\n“top” posts to see if there is a tangible difference in content.\nInital Loading and\nProcessing\nExplanation: My code below illustrates how I\ninitially got my information from reddit by scraping. In this case I\nused RedditExtractoR. The author of this packages describes it as a\nminimalist r wrapper it scrapes a limited number of posts from reddit.\nThe api on reddit itself only allows 60 requests per minute. In this\ncase I chose posts that were “new” as of march 26 2022 at 11:17 P.M.\nThis has resulted in 980 reddit posts, this subreddit is described as\n“Firearms and related articles” and much of the subreddit is\ndescriptions of, reviews, and highlights of firearms owned by users.\n\n\n# New_guns_urls <- find_thread_urls(subreddit=\"guns\", sort_by=\"new\")\n\n\nloadRData <- function(fileName){\n#loads an RData file, and returns it\n    load(fileName)\n    get(ls()[ls() != \"fileName\"])\n}\nNew_guns_urls_df <- loadRData(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/New_reddit_posts_3_26.RData\")\n\n\n\n\n\nstr(New_guns_urls_df)\n\n\n'data.frame':   980 obs. of  7 variables:\n $ date_utc : chr  \"2022-03-15\" \"2022-03-15\" \"2022-03-15\" \"2022-03-15\" ...\n $ timestamp: num  1.65e+09 1.65e+09 1.65e+09 1.65e+09 1.65e+09 ...\n $ title    : chr  \"With tiny ar15s and mcx/similar, where does a pcc fit in the arsenal now?\" \"I still don\\031t feel like this is good enough. Ha. Anyone else this anal about this stuff?\" \"Best Home Defense Round in 5.56?\" \"5\\0365*5/555&53 5\\r50535.5\\\"5-\" ...\n $ text     : chr  \"With the advent of small ar15s and similar foldy boys like the mcx, where does the pcc fit in now?\\n\\nSeems tha\"| __truncated__ \"\" \"I'm looking for some defensive rounds for my AR-15 that are reliable. Lately I've been shooting 55 Grain XTac g\"| __truncated__ \"\" ...\n $ subreddit: chr  \"guns\" \"guns\" \"guns\" \"guns\" ...\n $ comments : num  32 23 73 12 51 29 25 13 32 7 ...\n $ url      : chr  \"https://www.reddit.com/r/guns/comments/teruk1/with_tiny_ar15s_and_mcxsimilar_where_does_a_pcc/\" \"https://www.reddit.com/r/guns/comments/ter4zi/i_still_dont_feel_like_this_is_good_enough_ha/\" \"https://www.reddit.com/r/guns/comments/ter48v/best_home_defense_round_in_556/\" \"https://www.reddit.com/r/guns/comments/teqkl1/5\\0365*5/555&53_5\\r50535.5\\\"5-/\" ...\n\nConversion From Data Frame\nto Corpus\nExplanation: Below I have processed my initial\ndataframe from Reddit into a corpus and saved a summary of the resulting\ndata.\n\n\nnew_guns_urls_df<-New_guns_urls_df[,c(\"title\", \"date_utc\", \"comments\")]\n\nnew_guns_corpus<-corpus(new_guns_urls_df$title)\n\nnew_guns_documents<-new_guns_corpus[1:10,]\n\nnew_guns_corpus_summary <- summary(new_guns_corpus)\n\n\n\nBroad Characteristics\nExplanation: In order to clean the documents for\npre-processing and analysis I have removed punctuation, converted to\nlowercase and removed stopwords. Though the language of firearms is\noften associated with punctuation, such as 5.56, 3.57, and a variety of\nother calibers, which represent the diameter of the barrel required to\nfire each ammunition. However, losing the punctuation in caliber and\nfirearm titles would not reduce their comprehensibility in analysis if\nthey retain their form as 5.56 and 556 can be considered equally while\nreducing the complexity of tokens and potentially even sentences.\nConverting the documents to lowercase can also simplifies the data.\nHowever, for my inital analysis I will just being making a lowercase\ndocument feature matrix and a more edited one.\n\n\nnew_guns_corpus_dfm_tl<-tokens(new_guns_corpus) %>%  dfm(tolower=TRUE) \n\nnew_guns_corpus_dfm_punct_tl_sw <- tokens(new_guns_corpus,\n                                    remove_punct = TRUE,) %>%\n                           dfm(tolower=TRUE) %>%\n                           dfm_remove(stopwords('english'))\n\n\n\nTop features\nExplanation: Examining the top 20 features below we\nsee a fairly predictable set of response, as may be expected from the\ngun subreddit, the most used word is gun. Rifle and pistol are also in\nthe top 20. Individual letters such as “ar”, “s”, and “m” appear\nfrequently as they are commonly used designations for types of firearms\nor model names, ar-12, ar-15, m-4, m-16, m1911, 5-mm, and s559, s-12 and\nother designations. This indicates that these numbers are valuable, if\ndifficult to comprehend on their own. With no punctuation removed the\nfirst 20 features are not informative.\n\n\ntopfeatures(new_guns_corpus_dfm_tl, 20)\n\n\n    .     ?     a   the    my     ,     i    to   for   and    is \n  425   260   259   211   183   175   175   158   112   108    99 \n   in    it     /    of  with   gun  this    on first \n   93    89    84    81    80    78    76    76    60 \n\n\n\ntopfeatures(new_guns_corpus_dfm_punct_tl_sw, 20)\n\n\n     gun    first      new       22      can        s question \n      78       60       57       43       41       35       31 \n   rifle     just      amp     help   anyone       ar   pistol \n      29       29       28       27       26       26       26 \n    time     guns     know     good      got        m \n      25       24       23       22       22       22 \n\nWorld Cloud\nExplanation: Though not necessarily statistically\ninformative, the wordcloud below can give some sense of comparative\nfrequency using the limit of minimum count being 6. Reading through\nthese can give a sense of both the communal nature of the forum in\nasking for recommendations, but also the importance of the word purchase\nand other words associated with working with, and buying firearms. In\nthe case of the only lowercased data we can gather much less\ninformation.\n\n\nset.seed(123456)\n\ntextplot_wordcloud(new_guns_corpus_dfm_tl, min_count = 12, random_order = T, rotation = 0)\n\n\n\n\n\n\nset.seed(123456)\n\ntextplot_wordcloud(new_guns_corpus_dfm_punct_tl_sw, min_count = 6, random_order = T, rotation = 0)\n\n\n\n\nTypes, Tokens, and Sentances\nTypes\nExplanation: The gun corpus summary gives 3 counting\ncategories that we can interpret in order to get a sense of the\ncomplexity of the documents that we are using. Looking at the number of\ntypes on average we see a mean of 9.18 and qunantiles that indicate a\nrange of 2-45 with 50% being between 4 and 13 types.\n\n\nmean(new_guns_corpus_summary$Types)\n\n\n[1] 9.18\n\nquantile(new_guns_corpus_summary$Types)\n\n\n  0%  25%  50%  75% 100% \n 2.0  4.0  7.5 13.0 45.0 \n\nTokens\nExplanation: Tokens are relatively similar to types\nin this case. Here there is a mean of 9.72 but a range of 2-55 with the\nmiddle 50% ranging from 4-13 tokens, as was the case for types.\n\n\nmean(new_guns_corpus_summary$Tokens)\n\n\n[1] 9.72\n\nquantile(new_guns_corpus_summary$Tokens)\n\n\n  0%  25%  50%  75% 100% \n   2    4    8   13   55 \n\nSentances\nExplanation: As is indicated below, it appears that\nthe number of sentences in each post is generally one. Arroding to the\nqunatile statistics the most sentences in any post\n\n\nmean(new_guns_corpus_summary$Sentences)\n\n\n[1] 1.18\n\nquantile(new_guns_corpus_summary$Sentences)\n\n\n  0%  25%  50%  75% 100% \n   1    1    1    1    3 \n\nWord counts\nExplanation: Looking at word counts we see a similar\ntrend reflected where including stopwords and punctuation decreases the\nquality of data as little information but punctuation and stopwords are\nincluded.\n\n\nword_counts_new_1 <- as.data.frame(sort(colSums(new_guns_corpus_dfm_tl),dec=T))\n\ncolnames(word_counts_new_1) <- c(\"Frequency\")\n\nword_counts_new_1$Rank <- c(1:ncol(new_guns_corpus_dfm_tl))\n\nhead(word_counts_new_1)\n\n\n    Frequency Rank\n.         425    1\n?         260    2\na         259    3\nthe       211    4\nmy        183    5\n,         175    6\n\n\n\nword_counts_new <- as.data.frame(sort(colSums(new_guns_corpus_dfm_punct_tl_sw),dec=T))\n\ncolnames(word_counts_new) <- c(\"Frequency\")\n\nword_counts_new$Rank <- c(1:ncol(new_guns_corpus_dfm_punct_tl_sw))\n\nhead(word_counts_new)\n\n\n      Frequency Rank\ngun          78    1\nfirst        60    2\nnew          57    3\n22           43    4\ncan          41    5\ns            35    6\n\nZipf’s Law\nExplanation: As can be seen from the frequency\ngraphs below, Ziph’s Law of inverse proportion. In this case a words\nrank in freqency is inversely prorportional to the number of times it is\nobserved. Though the uncleaned dataset has far more frequency for its\nmost common words (much of which is punctuation) it appears to follow\nthe law.\n\n\nggplot(word_counts_new, mapping = aes(x = Rank, y = Frequency)) + \n  geom_point() +\n  labs(title = \"Zipf's Law\", x = \"Rank\", y = \"Frequency\") + \n  theme_bw()\n\n\n\n\n\n\nggplot(word_counts_new_1, mapping = aes(x = Rank, y = Frequency)) + \n  geom_point() +\n  labs(title = \"Zipf's Law\", x = \"Rank\", y = \"Frequency\") + \n  theme_bw()\n\n\n\n\nData Trimming\nExplanation: Much of what I do here will be\nexplained in the code and results. Many words appear with a minimum\nfrequency of 4, though non are included in 10% and only 3 words are\nincluded in 5%. At a level of 2.5% we get 4 words.\n\n\n# First I trim the data to only include words that appear at least 4 times\n\nsmaller_dfm_4_freq <- dfm_trim(new_guns_corpus_dfm_punct_tl_sw, min_termfreq = 4)\n\n# Next I will look at proportions are see if there are words that are seen in\n# More than 10% and 5% of documents\nsmaller_dfm_10_p <- dfm_trim(smaller_dfm_4_freq, min_docfreq = 0.1, docfreq_type = \"prop\")\n\nsmaller_dfm_5_p <- dfm_trim(smaller_dfm_4_freq, min_docfreq = 0.05, docfreq_type = \"prop\")\n\nsmaller_dfm_2.5_p <- dfm_trim(smaller_dfm_4_freq, min_docfreq = 0.025,  docfreq_type = \"prop\")\n\n\n\nData Readability\nExplanation: Before making general modifications to\nthe data, it is valuable to also get a sense of readability, as in week\n5. In this case we will calculate readability scores based on 3\ndifferent measures, FOG, Coleman Liau, and Flesch Kincaid. Though this\nstep will not indicate what sort of pre-processing is best, or how the\ndata should be reduced, it does give us some insight into the complexity\nof the language in our data. In this case we just observe the\nreadability based on the post number.\n\n\nreadability_new_guns <- textstat_readability(new_guns_corpus, \n                                    measure = c(\"Flesch.Kincaid\", \"FOG\", \"Coleman.Liau.grade\")) \n\n# add in a chapter number\n\nreadability_new_guns$reddit_post <- c(1:nrow(readability_new_guns))\n\n# plot results\nggplot(readability_new_guns, aes(x = reddit_post)) +\n  geom_line(aes(y = Flesch.Kincaid), color = \"black\",  alpha=0.3) + \n  geom_line(aes(y = FOG), color = \"red\", alpha=0.3) + \n  geom_line(aes(y = Coleman.Liau.grade), color = \"blue\", alpha=0.3) + \n  theme_bw()\n\n\n\n\nExplanation: In this part we will add dates to our\ndata to see how the complexity changes over time or if it was relatively\nconstant. As can be seen below, the amount of complexity in the data\nvaries more smoothly when the data are sorted by data and not\narbitrarily by their post number, in this case all 3 complexity method\nexhibit similar trends.\n\n\nreadability_new_guns$added_dates <- as.Date(New_guns_urls_df$date_utc)\n\nggplot(readability_new_guns, aes(x = added_dates)) +\n  geom_smooth(aes(y = Flesch.Kincaid), color = \"black\") + \n  geom_smooth(aes(y = FOG), color = \"red\") + \n  geom_smooth(aes(y = Coleman.Liau.grade), color = \"blue\") + \n  theme_minimal()\n\n\n\n\nExplanation: Looking at the readability of the we\nsee that all correlations between the methods of complexity measurement\nare similar except of FOG and Coleman Liau, however the graphs above do\nindicate some similarity in trend between them, though not direct\ncorrelation in their estimates potentially.\n\n\ncor(readability_new_guns$Flesch.Kincaid, readability_new_guns$FOG, use = \"complete.obs\")\n\n\n[1] 0.8321041\n\n\n\ncor(readability_new_guns$Flesch.Kincaid, readability_new_guns$Coleman.Liau.grade, use = \"complete.obs\")\n\n\n[1] 0.751674\n\n\n\ncor(readability_new_guns$FOG, readability_new_guns$Coleman.Liau.grade, use = \"complete.obs\")\n\n\n[1] 0.5859859\n\nPre-processing\nBefore Reduction and Co-Occurance\nExplanation: Next I used the\nfactorial_preprocessing() command to both use n-grams processing and use\nan infrequent term threshold. This is in order to see what techniques,\nsuch as removing punctuation, stopwords, etc lead to a pre-text score\ndevised by Denny and Spirling. This pre-text score indicatess how many\nk-pairs of terms change the most when the pre-processing strategy is\nchanged. Lower scores indicate more usual results while higher scores\nindicate more unusual results and they are between 0 and 1. Here we have\nused n-grams and set an infreqent term threshold. Because of the nature\nof our data I will use 30% of documents as\n\n\n?factorial_preprocessing\npreprocessed_documents <- factorial_preprocessing(\n    new_guns_corpus,\n    use_ngrams = TRUE,\n    infrequent_term_threshold = 0.3,\n    verbose = FALSE)\n\n\nPreprocessing 980 documents 128 different ways...\n\n\n\nnames(preprocessed_documents)\n\n\n[1] \"choices\"  \"dfm_list\" \"labels\"  \n\nExplanation: As can be seen below the possible\nchoices are coded on the first column with each subsequent column\nindicating whether or not each choice includes each of the specified\nchoices in its assessment.\n\n\nhead(preprocessed_documents$choices)\n\n\n              removePunctuation removeNumbers lowercase stem\nP-N-L-S-W-I-3              TRUE          TRUE      TRUE TRUE\nN-L-S-W-I-3               FALSE          TRUE      TRUE TRUE\nP-L-S-W-I-3                TRUE         FALSE      TRUE TRUE\nL-S-W-I-3                 FALSE         FALSE      TRUE TRUE\nP-N-S-W-I-3                TRUE          TRUE     FALSE TRUE\nN-S-W-I-3                 FALSE          TRUE     FALSE TRUE\n              removeStopwords infrequent_terms use_ngrams\nP-N-L-S-W-I-3            TRUE             TRUE       TRUE\nN-L-S-W-I-3              TRUE             TRUE       TRUE\nP-L-S-W-I-3              TRUE             TRUE       TRUE\nL-S-W-I-3                TRUE             TRUE       TRUE\nP-N-S-W-I-3              TRUE             TRUE       TRUE\nN-S-W-I-3                TRUE             TRUE       TRUE\n\nExplanation: Next preText is calculated using 50\ncomparisons and a cosine distance calculation.\n\n\n#set.seed(12366)\n#preText_results <- preText(\n#    preprocessed_documents,\n#   dataset_name = \"Gun Pretext Results\",\n#   distance_method = \"cosine\",\n#   num_comparisons = 50,\n#  verbose = TRUE)\n\n\n\n\n\n#save(preText_results, file=\"preText_results_3_27_gun_50_comp.RData\")\n\n\n\n\n\npreText_results <- loadRData(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/preText_results_3_27_gun_50_comp.RData\")\n\npreText_results\n\n\n$preText_scores\n    preText_score preprocessing_steps\n1      0.04157825       P-N-L-S-W-I-3\n2      0.04157825         N-L-S-W-I-3\n3      0.04157825         P-L-S-W-I-3\n4      0.04157825           L-S-W-I-3\n5      0.04157825         P-N-S-W-I-3\n6      0.04157825           N-S-W-I-3\n7      0.04157825           P-S-W-I-3\n8      0.04157825             S-W-I-3\n9      0.04157825         P-N-L-W-I-3\n10     0.04157825           N-L-W-I-3\n11     0.04157825           P-L-W-I-3\n12     0.04157825             L-W-I-3\n13     0.04157825           P-N-W-I-3\n14     0.04157825             N-W-I-3\n15     0.04157825             P-W-I-3\n16     0.04157825               W-I-3\n17     0.04157825         P-N-L-S-I-3\n18     0.04157825           N-L-S-I-3\n19     0.04157825           P-L-S-I-3\n20     0.04157825             L-S-I-3\n21     0.04157825           P-N-S-I-3\n22     0.04157825             N-S-I-3\n23     0.04157825             P-S-I-3\n24     0.04157825               S-I-3\n25     0.04157825           P-N-L-I-3\n26     0.04157825             N-L-I-3\n27     0.04157825             P-L-I-3\n28     0.04157825               L-I-3\n29     0.04157825             P-N-I-3\n30     0.04157825               N-I-3\n31     0.04157825               P-I-3\n32     0.04157825                 I-3\n33     0.04815400         P-N-L-S-W-3\n34     0.08058534           N-L-S-W-3\n35     0.04403401           P-L-S-W-3\n36     0.08427290             L-S-W-3\n37     0.04864279           P-N-S-W-3\n38     0.07599185             N-S-W-3\n39     0.04470926             P-S-W-3\n40     0.08186325               S-W-3\n41     0.02723578           P-N-L-W-3\n42     0.02629912             N-L-W-3\n43     0.02280946             P-L-W-3\n44     0.02777490               L-W-3\n45     0.02723578             P-N-W-3\n46     0.02629912               N-W-3\n47     0.02280946               P-W-3\n48     0.02777490                 W-3\n49     0.02647384           P-N-L-S-3\n50     0.08304385             N-L-S-3\n51     0.02334213             P-L-S-3\n52     0.07225887               L-S-3\n53     0.02706859             P-N-S-3\n54     0.08697989               N-S-3\n55     0.02384351               P-S-3\n56     0.07767802                 S-3\n57     0.02647384             P-N-L-3\n58     0.02777727               N-L-3\n59     0.02334213               P-L-3\n60     0.01888648                 L-3\n61     0.02647384               P-N-3\n62     0.02777727                 N-3\n63     0.02334213                 P-3\n64     0.01888648                   3\n65     0.04157825         P-N-L-S-W-I\n66     0.04157825           N-L-S-W-I\n67     0.04157825           P-L-S-W-I\n68     0.04157825             L-S-W-I\n69     0.04157825           P-N-S-W-I\n70     0.04157825             N-S-W-I\n71     0.04157825             P-S-W-I\n72     0.04157825               S-W-I\n73     0.04157825           P-N-L-W-I\n74     0.04157825             N-L-W-I\n75     0.04157825             P-L-W-I\n76     0.04157825               L-W-I\n77     0.04157825             P-N-W-I\n78     0.04157825               N-W-I\n79     0.04157825               P-W-I\n80     0.04157825                 W-I\n81     0.04157825           P-N-L-S-I\n82     0.04157825             N-L-S-I\n83     0.04157825             P-L-S-I\n84     0.04157825               L-S-I\n85     0.04157825             P-N-S-I\n86     0.04157825               N-S-I\n87     0.04157825               P-S-I\n88     0.04157825                 S-I\n89     0.04157825             P-N-L-I\n90     0.04157825               N-L-I\n91     0.04157825               P-L-I\n92     0.04157825                 L-I\n93     0.04157825               P-N-I\n94     0.04157825                 N-I\n95     0.04157825                 P-I\n96     0.04157825                   I\n97     0.07109905           P-N-L-S-W\n98     0.16021925             N-L-S-W\n99     0.06767075             P-L-S-W\n100    0.17095529               L-S-W\n101    0.07098343             P-N-S-W\n102    0.15776534               N-S-W\n103    0.06802275               P-S-W\n104    0.16850136                 S-W\n105    0.02723578             P-N-L-W\n106    0.02344916               N-L-W\n107    0.02280946               P-L-W\n108    0.02739263                 L-W\n109    0.02723578               P-N-W\n110    0.02344916                 N-W\n111    0.02280946                 P-W\n112    0.02739263                   W\n113    0.05006683             P-N-L-S\n114    0.22396649               N-L-S\n115    0.04704759               P-L-S\n116    0.23717659                 L-S\n117    0.05064631               P-N-S\n118    0.21022978                 N-S\n119    0.04752479                 P-S\n120    0.23146371                   S\n121    0.02635536               P-N-L\n122    0.03527452                 N-L\n123    0.02342946                 P-L\n124    0.10226138                   L\n125    0.02635536                 P-N\n126    0.03527452                   N\n127    0.02342946                   P\n\n$ranked_preText_scores\n    preText_score preprocessing_steps\n1      0.23717659                 L-S\n2      0.23146371                   S\n3      0.22396649               N-L-S\n4      0.21022978                 N-S\n5      0.17095529               L-S-W\n6      0.16850136                 S-W\n7      0.16021925             N-L-S-W\n8      0.15776534               N-S-W\n9      0.10226138                   L\n10     0.08697989               N-S-3\n11     0.08427290             L-S-W-3\n12     0.08304385             N-L-S-3\n13     0.08186325               S-W-3\n14     0.08058534           N-L-S-W-3\n15     0.07767802                 S-3\n16     0.07599185             N-S-W-3\n17     0.07225887               L-S-3\n18     0.07109905           P-N-L-S-W\n19     0.07098343             P-N-S-W\n20     0.06802275               P-S-W\n21     0.06767075             P-L-S-W\n22     0.05064631               P-N-S\n23     0.05006683             P-N-L-S\n24     0.04864279           P-N-S-W-3\n25     0.04815400         P-N-L-S-W-3\n26     0.04752479                 P-S\n27     0.04704759               P-L-S\n28     0.04470926             P-S-W-3\n29     0.04403401           P-L-S-W-3\n30     0.04157825       P-N-L-S-W-I-3\n31     0.04157825         N-L-S-W-I-3\n32     0.04157825         P-L-S-W-I-3\n33     0.04157825           L-S-W-I-3\n34     0.04157825         P-N-S-W-I-3\n35     0.04157825           N-S-W-I-3\n36     0.04157825           P-S-W-I-3\n37     0.04157825             S-W-I-3\n38     0.04157825         P-N-L-W-I-3\n39     0.04157825           N-L-W-I-3\n40     0.04157825           P-L-W-I-3\n41     0.04157825             L-W-I-3\n42     0.04157825           P-N-W-I-3\n43     0.04157825             N-W-I-3\n44     0.04157825             P-W-I-3\n45     0.04157825               W-I-3\n46     0.04157825         P-N-L-S-I-3\n47     0.04157825           N-L-S-I-3\n48     0.04157825           P-L-S-I-3\n49     0.04157825             L-S-I-3\n50     0.04157825           P-N-S-I-3\n51     0.04157825             N-S-I-3\n52     0.04157825             P-S-I-3\n53     0.04157825               S-I-3\n54     0.04157825           P-N-L-I-3\n55     0.04157825             N-L-I-3\n56     0.04157825             P-L-I-3\n57     0.04157825               L-I-3\n58     0.04157825             P-N-I-3\n59     0.04157825               N-I-3\n60     0.04157825               P-I-3\n61     0.04157825                 I-3\n62     0.04157825         P-N-L-S-W-I\n63     0.04157825           N-L-S-W-I\n64     0.04157825           P-L-S-W-I\n65     0.04157825             L-S-W-I\n66     0.04157825           P-N-S-W-I\n67     0.04157825             N-S-W-I\n68     0.04157825             P-S-W-I\n69     0.04157825               S-W-I\n70     0.04157825           P-N-L-W-I\n71     0.04157825             N-L-W-I\n72     0.04157825             P-L-W-I\n73     0.04157825               L-W-I\n74     0.04157825             P-N-W-I\n75     0.04157825               N-W-I\n76     0.04157825               P-W-I\n77     0.04157825                 W-I\n78     0.04157825           P-N-L-S-I\n79     0.04157825             N-L-S-I\n80     0.04157825             P-L-S-I\n81     0.04157825               L-S-I\n82     0.04157825             P-N-S-I\n83     0.04157825               N-S-I\n84     0.04157825               P-S-I\n85     0.04157825                 S-I\n86     0.04157825             P-N-L-I\n87     0.04157825               N-L-I\n88     0.04157825               P-L-I\n89     0.04157825                 L-I\n90     0.04157825               P-N-I\n91     0.04157825                 N-I\n92     0.04157825                 P-I\n93     0.04157825                   I\n94     0.03527452                 N-L\n95     0.03527452                   N\n96     0.02777727               N-L-3\n97     0.02777727                 N-3\n98     0.02777490               L-W-3\n99     0.02777490                 W-3\n100    0.02739263                 L-W\n101    0.02739263                   W\n102    0.02723578           P-N-L-W-3\n103    0.02723578             P-N-W-3\n104    0.02723578             P-N-L-W\n105    0.02723578               P-N-W\n106    0.02706859             P-N-S-3\n107    0.02647384           P-N-L-S-3\n108    0.02647384             P-N-L-3\n109    0.02647384               P-N-3\n110    0.02635536               P-N-L\n111    0.02635536                 P-N\n112    0.02629912             N-L-W-3\n113    0.02629912               N-W-3\n114    0.02384351               P-S-3\n115    0.02344916               N-L-W\n116    0.02344916                 N-W\n117    0.02342946                 P-L\n118    0.02342946                   P\n119    0.02334213             P-L-S-3\n120    0.02334213               P-L-3\n121    0.02334213                 P-3\n122    0.02280946             P-L-W-3\n123    0.02280946               P-W-3\n124    0.02280946               P-L-W\n125    0.02280946                 P-W\n126    0.01888648                 L-3\n127    0.01888648                   3\n\n$choices\n              removePunctuation removeNumbers lowercase  stem\nP-N-L-S-W-I-3              TRUE          TRUE      TRUE  TRUE\nN-L-S-W-I-3               FALSE          TRUE      TRUE  TRUE\nP-L-S-W-I-3                TRUE         FALSE      TRUE  TRUE\nL-S-W-I-3                 FALSE         FALSE      TRUE  TRUE\nP-N-S-W-I-3                TRUE          TRUE     FALSE  TRUE\nN-S-W-I-3                 FALSE          TRUE     FALSE  TRUE\nP-S-W-I-3                  TRUE         FALSE     FALSE  TRUE\nS-W-I-3                   FALSE         FALSE     FALSE  TRUE\nP-N-L-W-I-3                TRUE          TRUE      TRUE FALSE\nN-L-W-I-3                 FALSE          TRUE      TRUE FALSE\nP-L-W-I-3                  TRUE         FALSE      TRUE FALSE\nL-W-I-3                   FALSE         FALSE      TRUE FALSE\nP-N-W-I-3                  TRUE          TRUE     FALSE FALSE\nN-W-I-3                   FALSE          TRUE     FALSE FALSE\nP-W-I-3                    TRUE         FALSE     FALSE FALSE\nW-I-3                     FALSE         FALSE     FALSE FALSE\nP-N-L-S-I-3                TRUE          TRUE      TRUE  TRUE\nN-L-S-I-3                 FALSE          TRUE      TRUE  TRUE\nP-L-S-I-3                  TRUE         FALSE      TRUE  TRUE\nL-S-I-3                   FALSE         FALSE      TRUE  TRUE\nP-N-S-I-3                  TRUE          TRUE     FALSE  TRUE\nN-S-I-3                   FALSE          TRUE     FALSE  TRUE\nP-S-I-3                    TRUE         FALSE     FALSE  TRUE\nS-I-3                     FALSE         FALSE     FALSE  TRUE\nP-N-L-I-3                  TRUE          TRUE      TRUE FALSE\nN-L-I-3                   FALSE          TRUE      TRUE FALSE\nP-L-I-3                    TRUE         FALSE      TRUE FALSE\nL-I-3                     FALSE         FALSE      TRUE FALSE\nP-N-I-3                    TRUE          TRUE     FALSE FALSE\nN-I-3                     FALSE          TRUE     FALSE FALSE\nP-I-3                      TRUE         FALSE     FALSE FALSE\nI-3                       FALSE         FALSE     FALSE FALSE\nP-N-L-S-W-3                TRUE          TRUE      TRUE  TRUE\nN-L-S-W-3                 FALSE          TRUE      TRUE  TRUE\nP-L-S-W-3                  TRUE         FALSE      TRUE  TRUE\nL-S-W-3                   FALSE         FALSE      TRUE  TRUE\nP-N-S-W-3                  TRUE          TRUE     FALSE  TRUE\nN-S-W-3                   FALSE          TRUE     FALSE  TRUE\nP-S-W-3                    TRUE         FALSE     FALSE  TRUE\nS-W-3                     FALSE         FALSE     FALSE  TRUE\nP-N-L-W-3                  TRUE          TRUE      TRUE FALSE\nN-L-W-3                   FALSE          TRUE      TRUE FALSE\nP-L-W-3                    TRUE         FALSE      TRUE FALSE\nL-W-3                     FALSE         FALSE      TRUE FALSE\nP-N-W-3                    TRUE          TRUE     FALSE FALSE\nN-W-3                     FALSE          TRUE     FALSE FALSE\nP-W-3                      TRUE         FALSE     FALSE FALSE\nW-3                       FALSE         FALSE     FALSE FALSE\nP-N-L-S-3                  TRUE          TRUE      TRUE  TRUE\nN-L-S-3                   FALSE          TRUE      TRUE  TRUE\nP-L-S-3                    TRUE         FALSE      TRUE  TRUE\nL-S-3                     FALSE         FALSE      TRUE  TRUE\nP-N-S-3                    TRUE          TRUE     FALSE  TRUE\nN-S-3                     FALSE          TRUE     FALSE  TRUE\nP-S-3                      TRUE         FALSE     FALSE  TRUE\nS-3                       FALSE         FALSE     FALSE  TRUE\nP-N-L-3                    TRUE          TRUE      TRUE FALSE\nN-L-3                     FALSE          TRUE      TRUE FALSE\nP-L-3                      TRUE         FALSE      TRUE FALSE\nL-3                       FALSE         FALSE      TRUE FALSE\nP-N-3                      TRUE          TRUE     FALSE FALSE\nN-3                       FALSE          TRUE     FALSE FALSE\nP-3                        TRUE         FALSE     FALSE FALSE\n3                         FALSE         FALSE     FALSE FALSE\nP-N-L-S-W-I                TRUE          TRUE      TRUE  TRUE\nN-L-S-W-I                 FALSE          TRUE      TRUE  TRUE\nP-L-S-W-I                  TRUE         FALSE      TRUE  TRUE\nL-S-W-I                   FALSE         FALSE      TRUE  TRUE\nP-N-S-W-I                  TRUE          TRUE     FALSE  TRUE\nN-S-W-I                   FALSE          TRUE     FALSE  TRUE\nP-S-W-I                    TRUE         FALSE     FALSE  TRUE\nS-W-I                     FALSE         FALSE     FALSE  TRUE\nP-N-L-W-I                  TRUE          TRUE      TRUE FALSE\nN-L-W-I                   FALSE          TRUE      TRUE FALSE\nP-L-W-I                    TRUE         FALSE      TRUE FALSE\nL-W-I                     FALSE         FALSE      TRUE FALSE\nP-N-W-I                    TRUE          TRUE     FALSE FALSE\nN-W-I                     FALSE          TRUE     FALSE FALSE\nP-W-I                      TRUE         FALSE     FALSE FALSE\nW-I                       FALSE         FALSE     FALSE FALSE\nP-N-L-S-I                  TRUE          TRUE      TRUE  TRUE\nN-L-S-I                   FALSE          TRUE      TRUE  TRUE\nP-L-S-I                    TRUE         FALSE      TRUE  TRUE\nL-S-I                     FALSE         FALSE      TRUE  TRUE\nP-N-S-I                    TRUE          TRUE     FALSE  TRUE\nN-S-I                     FALSE          TRUE     FALSE  TRUE\nP-S-I                      TRUE         FALSE     FALSE  TRUE\nS-I                       FALSE         FALSE     FALSE  TRUE\nP-N-L-I                    TRUE          TRUE      TRUE FALSE\nN-L-I                     FALSE          TRUE      TRUE FALSE\nP-L-I                      TRUE         FALSE      TRUE FALSE\nL-I                       FALSE         FALSE      TRUE FALSE\nP-N-I                      TRUE          TRUE     FALSE FALSE\nN-I                       FALSE          TRUE     FALSE FALSE\nP-I                        TRUE         FALSE     FALSE FALSE\nI                         FALSE         FALSE     FALSE FALSE\nP-N-L-S-W                  TRUE          TRUE      TRUE  TRUE\nN-L-S-W                   FALSE          TRUE      TRUE  TRUE\nP-L-S-W                    TRUE         FALSE      TRUE  TRUE\nL-S-W                     FALSE         FALSE      TRUE  TRUE\nP-N-S-W                    TRUE          TRUE     FALSE  TRUE\nN-S-W                     FALSE          TRUE     FALSE  TRUE\nP-S-W                      TRUE         FALSE     FALSE  TRUE\nS-W                       FALSE         FALSE     FALSE  TRUE\nP-N-L-W                    TRUE          TRUE      TRUE FALSE\nN-L-W                     FALSE          TRUE      TRUE FALSE\nP-L-W                      TRUE         FALSE      TRUE FALSE\nL-W                       FALSE         FALSE      TRUE FALSE\nP-N-W                      TRUE          TRUE     FALSE FALSE\nN-W                       FALSE          TRUE     FALSE FALSE\nP-W                        TRUE         FALSE     FALSE FALSE\nW                         FALSE         FALSE     FALSE FALSE\nP-N-L-S                    TRUE          TRUE      TRUE  TRUE\nN-L-S                     FALSE          TRUE      TRUE  TRUE\nP-L-S                      TRUE         FALSE      TRUE  TRUE\nL-S                       FALSE         FALSE      TRUE  TRUE\nP-N-S                      TRUE          TRUE     FALSE  TRUE\nN-S                       FALSE          TRUE     FALSE  TRUE\nP-S                        TRUE         FALSE     FALSE  TRUE\nS                         FALSE         FALSE     FALSE  TRUE\nP-N-L                      TRUE          TRUE      TRUE FALSE\nN-L                       FALSE          TRUE      TRUE FALSE\nP-L                        TRUE         FALSE      TRUE FALSE\nL                         FALSE         FALSE      TRUE FALSE\nP-N                        TRUE          TRUE     FALSE FALSE\nN                         FALSE          TRUE     FALSE FALSE\nP                          TRUE         FALSE     FALSE FALSE\n                          FALSE         FALSE     FALSE FALSE\n              removeStopwords infrequent_terms use_ngrams\nP-N-L-S-W-I-3            TRUE             TRUE       TRUE\nN-L-S-W-I-3              TRUE             TRUE       TRUE\nP-L-S-W-I-3              TRUE             TRUE       TRUE\nL-S-W-I-3                TRUE             TRUE       TRUE\nP-N-S-W-I-3              TRUE             TRUE       TRUE\nN-S-W-I-3                TRUE             TRUE       TRUE\nP-S-W-I-3                TRUE             TRUE       TRUE\nS-W-I-3                  TRUE             TRUE       TRUE\nP-N-L-W-I-3              TRUE             TRUE       TRUE\nN-L-W-I-3                TRUE             TRUE       TRUE\nP-L-W-I-3                TRUE             TRUE       TRUE\nL-W-I-3                  TRUE             TRUE       TRUE\nP-N-W-I-3                TRUE             TRUE       TRUE\nN-W-I-3                  TRUE             TRUE       TRUE\nP-W-I-3                  TRUE             TRUE       TRUE\nW-I-3                    TRUE             TRUE       TRUE\nP-N-L-S-I-3             FALSE             TRUE       TRUE\nN-L-S-I-3               FALSE             TRUE       TRUE\nP-L-S-I-3               FALSE             TRUE       TRUE\nL-S-I-3                 FALSE             TRUE       TRUE\nP-N-S-I-3               FALSE             TRUE       TRUE\nN-S-I-3                 FALSE             TRUE       TRUE\nP-S-I-3                 FALSE             TRUE       TRUE\nS-I-3                   FALSE             TRUE       TRUE\nP-N-L-I-3               FALSE             TRUE       TRUE\nN-L-I-3                 FALSE             TRUE       TRUE\nP-L-I-3                 FALSE             TRUE       TRUE\nL-I-3                   FALSE             TRUE       TRUE\nP-N-I-3                 FALSE             TRUE       TRUE\nN-I-3                   FALSE             TRUE       TRUE\nP-I-3                   FALSE             TRUE       TRUE\nI-3                     FALSE             TRUE       TRUE\nP-N-L-S-W-3              TRUE            FALSE       TRUE\nN-L-S-W-3                TRUE            FALSE       TRUE\nP-L-S-W-3                TRUE            FALSE       TRUE\nL-S-W-3                  TRUE            FALSE       TRUE\nP-N-S-W-3                TRUE            FALSE       TRUE\nN-S-W-3                  TRUE            FALSE       TRUE\nP-S-W-3                  TRUE            FALSE       TRUE\nS-W-3                    TRUE            FALSE       TRUE\nP-N-L-W-3                TRUE            FALSE       TRUE\nN-L-W-3                  TRUE            FALSE       TRUE\nP-L-W-3                  TRUE            FALSE       TRUE\nL-W-3                    TRUE            FALSE       TRUE\nP-N-W-3                  TRUE            FALSE       TRUE\nN-W-3                    TRUE            FALSE       TRUE\nP-W-3                    TRUE            FALSE       TRUE\nW-3                      TRUE            FALSE       TRUE\nP-N-L-S-3               FALSE            FALSE       TRUE\nN-L-S-3                 FALSE            FALSE       TRUE\nP-L-S-3                 FALSE            FALSE       TRUE\nL-S-3                   FALSE            FALSE       TRUE\nP-N-S-3                 FALSE            FALSE       TRUE\nN-S-3                   FALSE            FALSE       TRUE\nP-S-3                   FALSE            FALSE       TRUE\nS-3                     FALSE            FALSE       TRUE\nP-N-L-3                 FALSE            FALSE       TRUE\nN-L-3                   FALSE            FALSE       TRUE\nP-L-3                   FALSE            FALSE       TRUE\nL-3                     FALSE            FALSE       TRUE\nP-N-3                   FALSE            FALSE       TRUE\nN-3                     FALSE            FALSE       TRUE\nP-3                     FALSE            FALSE       TRUE\n3                       FALSE            FALSE       TRUE\nP-N-L-S-W-I              TRUE             TRUE      FALSE\nN-L-S-W-I                TRUE             TRUE      FALSE\nP-L-S-W-I                TRUE             TRUE      FALSE\nL-S-W-I                  TRUE             TRUE      FALSE\nP-N-S-W-I                TRUE             TRUE      FALSE\nN-S-W-I                  TRUE             TRUE      FALSE\nP-S-W-I                  TRUE             TRUE      FALSE\nS-W-I                    TRUE             TRUE      FALSE\nP-N-L-W-I                TRUE             TRUE      FALSE\nN-L-W-I                  TRUE             TRUE      FALSE\nP-L-W-I                  TRUE             TRUE      FALSE\nL-W-I                    TRUE             TRUE      FALSE\nP-N-W-I                  TRUE             TRUE      FALSE\nN-W-I                    TRUE             TRUE      FALSE\nP-W-I                    TRUE             TRUE      FALSE\nW-I                      TRUE             TRUE      FALSE\nP-N-L-S-I               FALSE             TRUE      FALSE\nN-L-S-I                 FALSE             TRUE      FALSE\nP-L-S-I                 FALSE             TRUE      FALSE\nL-S-I                   FALSE             TRUE      FALSE\nP-N-S-I                 FALSE             TRUE      FALSE\nN-S-I                   FALSE             TRUE      FALSE\nP-S-I                   FALSE             TRUE      FALSE\nS-I                     FALSE             TRUE      FALSE\nP-N-L-I                 FALSE             TRUE      FALSE\nN-L-I                   FALSE             TRUE      FALSE\nP-L-I                   FALSE             TRUE      FALSE\nL-I                     FALSE             TRUE      FALSE\nP-N-I                   FALSE             TRUE      FALSE\nN-I                     FALSE             TRUE      FALSE\nP-I                     FALSE             TRUE      FALSE\nI                       FALSE             TRUE      FALSE\nP-N-L-S-W                TRUE            FALSE      FALSE\nN-L-S-W                  TRUE            FALSE      FALSE\nP-L-S-W                  TRUE            FALSE      FALSE\nL-S-W                    TRUE            FALSE      FALSE\nP-N-S-W                  TRUE            FALSE      FALSE\nN-S-W                    TRUE            FALSE      FALSE\nP-S-W                    TRUE            FALSE      FALSE\nS-W                      TRUE            FALSE      FALSE\nP-N-L-W                  TRUE            FALSE      FALSE\nN-L-W                    TRUE            FALSE      FALSE\nP-L-W                    TRUE            FALSE      FALSE\nL-W                      TRUE            FALSE      FALSE\nP-N-W                    TRUE            FALSE      FALSE\nN-W                      TRUE            FALSE      FALSE\nP-W                      TRUE            FALSE      FALSE\nW                        TRUE            FALSE      FALSE\nP-N-L-S                 FALSE            FALSE      FALSE\nN-L-S                   FALSE            FALSE      FALSE\nP-L-S                   FALSE            FALSE      FALSE\nL-S                     FALSE            FALSE      FALSE\nP-N-S                   FALSE            FALSE      FALSE\nN-S                     FALSE            FALSE      FALSE\nP-S                     FALSE            FALSE      FALSE\nS                       FALSE            FALSE      FALSE\nP-N-L                   FALSE            FALSE      FALSE\nN-L                     FALSE            FALSE      FALSE\nP-L                     FALSE            FALSE      FALSE\nL                       FALSE            FALSE      FALSE\nP-N                     FALSE            FALSE      FALSE\nN                       FALSE            FALSE      FALSE\nP                       FALSE            FALSE      FALSE\n                        FALSE            FALSE      FALSE\n\n$regression_results\n    Coefficient          SE                Variable\n1  0.0692437554 0.008765654               Intercept\n2 -0.0248352010 0.006026387      Remove Punctuation\n3 -0.0012912608 0.006026387          Remove Numbers\n4  0.0008042152 0.006026387               Lowercase\n5  0.0315263364 0.006026387                Stemming\n6 -0.0031236074 0.006026387        Remove Stopwords\n7 -0.0194667091 0.006026387 Remove Infrequent Terms\n8 -0.0194780796 0.006026387              Use NGrams\n                Model\n1 Gun Pretext Results\n2 Gun Pretext Results\n3 Gun Pretext Results\n4 Gun Pretext Results\n5 Gun Pretext Results\n6 Gun Pretext Results\n7 Gun Pretext Results\n8 Gun Pretext Results\n\n\n\n#load(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/preText_results_gun_50_comps.RData\")\n\npreText_score_plot(preText_results)\n\n\n\n\nExplanation: After plotting we access the pretext\nscore with the minimum score, which is least unusual. This is the row\nwith the pre-processing steps refered to as “3” in the data. In addition\nL-3 results in the same preText Score.\n\n\nscores_new_pretext<-preText_results$preText_score \n\n# head(sort(scores_new_pretext))\n\n\n\nExplanation: Looking at the choices below I see that\n“3” does not do anything but use n-grams. L-3 does use lowercase and\nn-grams.\n\n\n# preprocessed_documents$choices\n\n\n\nExplanation Continued: Looking at the regression\ncoefficients we see negative scores as usual results and positive\ncoefficients as unusual ones. In this case removing puncuation,\nstopwords, and n-grams would not lead to a great deal of abnormality.\nThe scores below indicate that stemming would result in the most\nabnormality while all others but lowercase is the only other that has a\nnon-negative coefficinet.\n\n\nregression_coefficient_plot(preText_results,\n                            remove_intercept = TRUE)\n\n\n\n\nFeature Co-occurance Matrix\nExplanation: The feature co-occurance matrix can\ngive us a sense of which words in the dataset are occurring together\n\n\n# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of chapters\nsmaller_dfm <- dfm_trim(new_guns_corpus_dfm_punct_tl_sw, min_termfreq = 5)\n\n#smaller_dfm <- dfm_trim(smaller_dfm, min_docfreq = .3, docfreq_type = \"prop\")\n\n# create fcm from dfm\nsmaller_fcm <- fcm(smaller_dfm)\n\n# check the dimensions (i.e., the number of rows and the number of columnns)\n# of the matrix we created\ndim(smaller_fcm)\n\n\n[1] 226 226\n\n\n\n# pull the top features\nmyFeatures <- names(topfeatures(smaller_fcm, 40))\n\n# retain only those top features as part of our matrix\neven_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = \"keep\")\n\n# check dimensions\ndim(even_smaller_fcm)\n\n\n[1] 40 40\n\n# compute size weight for vertices in network\nsize <- log(colSums(even_smaller_fcm))\n\n# create plot\ntextplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)\n\n\n\n\nSentiment Results Using NRC\n\n\n# get_sentiments(\"nrc\")\n# get_sentiments(\"bing\")\n# get_sentiments(\"afinn\")\n\n\n\n\n\nsentimetnsdf <- read_csv(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/sentimetnsdf.csv\")\n\n\n\n\n\nnew_guns_urls_df_2<-new_guns_urls_df\n\nnew_guns_urls_df_2$text<- seq(1, 980, by=1)\n\nnrc_joy <- sentimetnsdf %>% \n  filter(sentiment == \"joy\")\n\ntidy_posts_for_guns <- new_guns_urls_df_2 %>%\n  unnest_tokens(word, title) \n\ntidy_posts_for_guns %>%\n  inner_join(nrc_joy) %>%\n  count(word, sort = TRUE) %>% head() %>% kable()\n\n\nword\nn\ngood\n22\nlove\n12\nfinally\n11\nsafe\n10\nfun\n7\nfavorite\n6\n\n\n\ntidy_posts_for_guns_sentiment <- tidy_posts_for_guns %>%\n  inner_join(sentimetnsdf) %>%\n  count(text, sentiment) %>%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% \n  mutate(sentiment = positive - negative)\n\n\n\n\n\nnrc_sentiment <- get_sentiments(\"nrc\")\n\n\nnrc_guns_word_counts <- tidy_posts_for_guns %>%\n  inner_join(nrc_sentiment) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  ungroup()\n\nnrc_guns_word_counts %>%\n  group_by(sentiment) %>%\n  slice_max(n, n = 10) %>% \n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL)\n\n\n\n\n\n\nBing_sentiments<-get_sentiments(\"bing\")\n\ntidy_posts_for_guns_sentiment <- tidy_posts_for_guns %>%\n  inner_join(Bing_sentiments) %>%\n  count(text, sentiment) %>%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% \n  mutate(sentiment = positive - negative)\n\nbing_word_counts <- tidy_posts_for_guns %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  ungroup()\n\nbing_word_counts %>%\n  group_by(sentiment) %>%\n  slice_max(n, n = 10) %>% \n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL)\n\n\n\n\n\n\ntidy_posts_for_guns$added_dates <- as.Date(tidy_posts_for_guns$date_utc)\n\n\nafinn <- tidy_posts_for_guns %>% \n  inner_join(get_sentiments(\"afinn\")) %>% \n  group_by(index = added_dates) %>% \n  summarise(sentiment = sum(value)) %>% \n  mutate(method = \"AFINN\")\nafinn\n\n\n# A tibble: 14 × 3\n   index      sentiment method\n   <date>         <dbl> <chr> \n 1 2022-03-14         9 AFINN \n 2 2022-03-15        17 AFINN \n 3 2022-03-16         0 AFINN \n 4 2022-03-17        23 AFINN \n 5 2022-03-18         7 AFINN \n 6 2022-03-19        30 AFINN \n 7 2022-03-20        13 AFINN \n 8 2022-03-21        11 AFINN \n 9 2022-03-22        19 AFINN \n10 2022-03-23         8 AFINN \n11 2022-03-24        32 AFINN \n12 2022-03-25         7 AFINN \n13 2022-03-26         6 AFINN \n14 2022-03-27         0 AFINN \n\n\n\nafinn %>%\n  ggplot(aes(index, sentiment, fill = method)) +\n  geom_col(show.legend = FALSE,   width = 0.7)  + \n  geom_smooth(aes(y = sentiment), color = \"black\")+\nfacet_wrap(~method, ncol = 1, scales = \"free_y\")+\n  theme_minimal()\n\n\n\n\nSentiment Results Using BING\nExplanation: Using nrc appears to have had some\nunintended effects that may require an analysis of the specific words\nused to describe sentiment. One difficult part of the data being used is\nthat firearms, and the words used to describe them, are percieved\n\n\nlibrary(methods)\n\ntoo_gun_dfm<- quanteda::dfm(new_guns_corpus, verbose = FALSE)\n\ntoo_gun_dfm\n\n\n\n\n\nlibrary(topicmodels)\n\ngun_dfm_lda <- LDA(too_gun_dfm, k = 2, control = list(seed = 777))\ngun_dfm_lda\n\n\n\n\n\ngun_dfm_lda_topics <- tidy(gun_dfm_lda, matrix = \"beta\")\ngun_dfm_lda_topics\n\n\n\n\n\ngun_top_terms <- gun_dfm_lda_topics %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>% \n  ungroup() %>%\n  arrange(topic, -beta)\n\ngun_top_terms %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\nbeta_wide <- gun_dfm_lda_topics %>%\n  mutate(topic = paste0(\"topic\", topic)) %>%\n  pivot_wider(names_from = topic, values_from = beta) %>% \n  filter(topic1 > .001 | topic2 > .001) %>%\n  mutate(log_ratio = log2(topic2 / topic1))\n\nbeta_wide\n\n\n\n\n\nbeta_wide %>%\n  group_by(direction = log_ratio > 0) %>%\n  slice_max(abs(log_ratio), n = 10) %>% \n  ungroup() %>%\n  mutate(term = reorder(term, log_ratio)) %>%\n  ggplot(aes(log_ratio, term)) +\n  geom_col() +\n  labs(x = \"Log2 ratio of beta in topic 2 / topic 1\", y = NULL)\n\n\n\nTopic Modeling analysis\nResponse: As can be seen above topic modeling may\nbenefit from some data reduction, removing punctuation and stop words\nwould likely be beneficial as can be seen above where a number of the\ndifferences between topics are modeled as punctuation and stop\nwords.\n\n\ngun_tokens_stopwords_and_punct_removed <- tokens_remove(tokens(new_guns_corpus, remove_punct = TRUE), c(stopwords(\"english\")))\n\ngun_corpus_stopwords_and_punct_removed <- corpus(sapply(gun_tokens_stopwords_and_punct_removed, paste, collapse=\" \")\n)\n\ngun_corpus_stopwords_and_punct_removed\n\n\n\n\n\nlibrary(methods)\n\ntoo_gun_dfm_no_punct_stopwords<- quanteda::dfm(tokens(gun_corpus_stopwords_and_punct_removed), verbose = FALSE)\n\ntoo_gun_dfm_no_punct_stopwords\n\n\n\n\n\nlibrary(topicmodels)\n\ngun_dfm_lda_nopunct_stop <- LDA(too_gun_dfm_no_punct_stopwords, k = 2, control = list(seed = 777))\n\ngun_dfm_lda_nopunct_stop\n\n\n\n\n\ngun_dfm_lda_topics_nopunct_stop <- tidy(gun_dfm_lda_nopunct_stop, matrix = \"beta\")\n\ngun_dfm_lda_topics_nopunct_stop\n\n\n\n\n\ngun_top_terms_no_punct_or_stop<- gun_dfm_lda_topics_nopunct_stop %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>% \n  ungroup() %>%\n  arrange(topic, -beta)\n\ngun_top_terms_no_punct_or_stop %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\nbeta_wide_no_punct_stop <- gun_dfm_lda_topics_nopunct_stop %>%\n  mutate(topic = paste0(\"topic\", topic)) %>%\n  pivot_wider(names_from = topic, values_from = beta) %>% \n  filter(topic1 > .001 | topic2 > .001) %>%\n  mutate(log_ratio = log2(topic2 / topic1))\n\n\n\n\n\nbeta_wide_no_punct_stop %>%\n  group_by(direction = log_ratio > 0) %>%\n  slice_max(abs(log_ratio), n = 10) %>% \n  ungroup() %>%\n  mutate(term = reorder(term, log_ratio)) %>%\n  ggplot(aes(log_ratio, term)) +\n  geom_col() +\n  labs(x = \"Log2 ratio of beta in topic 2 / topic 1\", y = NULL)\n\n\n\nTokens and Corpus Work\n\n\ntop_guns_tokens <- tokens(new_guns_corpus)\n\nprint(top_guns_tokens)\n\n\nTokens consisting of 980 documents.\ntext1 :\n [1] \"With\"    \"tiny\"    \"ar15s\"   \"and\"     \"mcx\"     \"/\"      \n [7] \"similar\" \",\"       \"where\"   \"does\"    \"a\"       \"pcc\"    \n[ ... and 6 more ]\n\ntext2 :\n [1] \"I\"      \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"this\"  \n [8] \"is\"     \"good\"   \"enough\" \".\"      \"Ha\"    \n[ ... and 9 more ]\n\ntext3 :\n[1] \"Best\"    \"Home\"    \"Defense\" \"Round\"   \"in\"      \"5.56\"   \n[7] \"?\"      \n\ntext4 :\n [1] \"5\"       \"5\"       \"*\"       \"5\"       \"/\"       \"555\"    \n [7] \"&\"       \"53\"      \"5\"       \"50535.5\" \"\\\"\"      \"5-\"     \n\ntext5 :\n [1] \"My\"         \"suppressed\" \"12.5\"       \"AR\"         \"is\"        \n [6] \"the\"        \"same\"       \"size\"       \"as\"         \"my\"        \n[11] \"14.5\"       \"\\\"\"        \n[ ... and 1 more ]\n\ntext6 :\n [1] \"Springfield\" \"Hellcat\"     \"RDP\"         \"with\"       \n [5] \"SilencerCo\"  \"Omega\"       \"9k\"          \"and\"        \n [9] \"Streamlight\" \"TLR-7\"       \".\"          \n\n[ reached max_ndoc ... 974 more documents ]\n\ntop_guns_tokens_no_punct <- tokens(new_guns_corpus, \n    remove_punct = T)\n\nprint(top_guns_tokens_no_punct)\n\n\nTokens consisting of 980 documents.\ntext1 :\n [1] \"With\"    \"tiny\"    \"ar15s\"   \"and\"     \"mcx\"     \"similar\"\n [7] \"where\"   \"does\"    \"a\"       \"pcc\"     \"fit\"     \"in\"     \n[ ... and 3 more ]\n\ntext2 :\n [1] \"I\"      \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"this\"  \n [8] \"is\"     \"good\"   \"enough\" \"Ha\"     \"Anyone\"\n[ ... and 6 more ]\n\ntext3 :\n[1] \"Best\"    \"Home\"    \"Defense\" \"Round\"   \"in\"      \"5.56\"   \n\ntext4 :\n[1] \"5\"       \"5\"       \"5\"       \"555\"     \"53\"      \"5\"      \n[7] \"50535.5\" \"5-\"     \n\ntext5 :\n [1] \"My\"         \"suppressed\" \"12.5\"       \"AR\"         \"is\"        \n [6] \"the\"        \"same\"       \"size\"       \"as\"         \"my\"        \n[11] \"14.5\"       \"AR\"        \n\ntext6 :\n [1] \"Springfield\" \"Hellcat\"     \"RDP\"         \"with\"       \n [5] \"SilencerCo\"  \"Omega\"       \"9k\"          \"and\"        \n [9] \"Streamlight\" \"TLR-7\"      \n\n[ reached max_ndoc ... 974 more documents ]\n\n\n\ntop_guns_tokens_no_punct_no_upper <- tokens_tolower(top_guns_tokens_no_punct)\n\nprint(top_guns_tokens_no_punct_no_upper)\n\n\nTokens consisting of 980 documents.\ntext1 :\n [1] \"with\"    \"tiny\"    \"ar15s\"   \"and\"     \"mcx\"     \"similar\"\n [7] \"where\"   \"does\"    \"a\"       \"pcc\"     \"fit\"     \"in\"     \n[ ... and 3 more ]\n\ntext2 :\n [1] \"i\"      \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"this\"  \n [8] \"is\"     \"good\"   \"enough\" \"ha\"     \"anyone\"\n[ ... and 6 more ]\n\ntext3 :\n[1] \"best\"    \"home\"    \"defense\" \"round\"   \"in\"      \"5.56\"   \n\ntext4 :\n[1] \"5\"       \"5\"       \"5\"       \"555\"     \"53\"      \"5\"      \n[7] \"50535.5\" \"5-\"     \n\ntext5 :\n [1] \"my\"         \"suppressed\" \"12.5\"       \"ar\"         \"is\"        \n [6] \"the\"        \"same\"       \"size\"       \"as\"         \"my\"        \n[11] \"14.5\"       \"ar\"        \n\ntext6 :\n [1] \"springfield\" \"hellcat\"     \"rdp\"         \"with\"       \n [5] \"silencerco\"  \"omega\"       \"9k\"          \"and\"        \n [9] \"streamlight\" \"tlr-7\"      \n\n[ reached max_ndoc ... 974 more documents ]\n\n\n\ntop_guns_tokens_no_punct_no_upper_no_stop <-    tokens_select(top_guns_tokens_no_punct_no_upper, pattern = stopwords(\"en\"), selection = \"remove\")\n\nlength(top_guns_tokens_no_punct_no_upper_no_stop)\n\n\n[1] 980\n\nprint(top_guns_tokens_no_punct_no_upper_no_stop)\n\n\nTokens consisting of 980 documents.\ntext1 :\n[1] \"tiny\"    \"ar15s\"   \"mcx\"     \"similar\" \"pcc\"     \"fit\"    \n[7] \"arsenal\" \"now\"    \n\ntext2 :\n [1] \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"good\"   \"enough\"\n [8] \"ha\"     \"anyone\" \"else\"   \"anal\"   \"stuff\" \n\ntext3 :\n[1] \"best\"    \"home\"    \"defense\" \"round\"   \"5.56\"   \n\ntext4 :\n[1] \"5\"       \"5\"       \"5\"       \"555\"     \"53\"      \"5\"      \n[7] \"50535.5\" \"5-\"     \n\ntext5 :\n[1] \"suppressed\" \"12.5\"       \"ar\"         \"size\"       \"14.5\"      \n[6] \"ar\"        \n\ntext6 :\n[1] \"springfield\" \"hellcat\"     \"rdp\"         \"silencerco\" \n[5] \"omega\"       \"9k\"          \"streamlight\" \"tlr-7\"      \n\n[ reached max_ndoc ... 974 more documents ]\n\n\n\ntop_guns_corpus_tokens <- tokens(new_guns_corpus)\n\nprint(top_guns_corpus_tokens)\n\n\nTokens consisting of 980 documents.\ntext1 :\n [1] \"With\"    \"tiny\"    \"ar15s\"   \"and\"     \"mcx\"     \"/\"      \n [7] \"similar\" \",\"       \"where\"   \"does\"    \"a\"       \"pcc\"    \n[ ... and 6 more ]\n\ntext2 :\n [1] \"I\"      \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"this\"  \n [8] \"is\"     \"good\"   \"enough\" \".\"      \"Ha\"    \n[ ... and 9 more ]\n\ntext3 :\n[1] \"Best\"    \"Home\"    \"Defense\" \"Round\"   \"in\"      \"5.56\"   \n[7] \"?\"      \n\ntext4 :\n [1] \"5\"       \"5\"       \"*\"       \"5\"       \"/\"       \"555\"    \n [7] \"&\"       \"53\"      \"5\"       \"50535.5\" \"\\\"\"      \"5-\"     \n\ntext5 :\n [1] \"My\"         \"suppressed\" \"12.5\"       \"AR\"         \"is\"        \n [6] \"the\"        \"same\"       \"size\"       \"as\"         \"my\"        \n[11] \"14.5\"       \"\\\"\"        \n[ ... and 1 more ]\n\ntext6 :\n [1] \"Springfield\" \"Hellcat\"     \"RDP\"         \"with\"       \n [5] \"SilencerCo\"  \"Omega\"       \"9k\"          \"and\"        \n [9] \"Streamlight\" \"TLR-7\"       \".\"          \n\n[ reached max_ndoc ... 974 more documents ]\n\n\n\nhead(annotated.guns_corpus$token)\n\n\n# A tibble: 6 × 11\n  doc_id   sid tid   token token_with_ws lemma upos  xpos  feats      \n   <int> <int> <chr> <chr> <chr>         <chr> <chr> <chr> <chr>      \n1      1     1 1     With  \"With \"       with  ADP   IN    <NA>       \n2      1     1 2     tiny  \"tiny \"       tiny  ADJ   JJ    Degree=Pos \n3      1     1 3     ar    \"ar\"          ar    NOUN  NN    Number=Sing\n4      1     1 4     15s   \"15s \"        15s   NOUN  NNS   Number=Plur\n5      1     1 5     and   \"and \"        and   CCONJ CC    <NA>       \n6      1     1 6     mcx/  \"mcx/\"        mcx/  SYM   NFP   <NA>       \n# … with 2 more variables: tid_source <chr>, relation <chr>\n\nhead(annotated.guns_corpus$document)\n\n\n  doc_id\n1      1\n2      2\n3      3\n4      4\n5      5\n6      6\n\ndoc_id_guns<-annotated.guns_corpus$document\n\ndoc_id_guns$date<-new_guns_urls_df$date_utc\n\nannoData <- left_join(doc_id_guns, annotated.guns_corpus$token, by = \"doc_id\")\n\nannoData$date<-as.Date(annoData$date)\n\n\n\n\n\nannoData %>% \n  group_by(date) %>% \n  summarize(Sentences = max(sid)) %>%\n  ggplot(aes(date, Sentences)) +\n    geom_line() +\n    geom_smooth() +\n    theme_bw()\n\n\n\n\n\n\n#sentimetnsdf<-get_sentiments(\"nrc\")\n\n#write.csv(sentimetnsdf, file = \"sentimetnsdf.csv\")\n\n#save(sentimetnsdf, file=\"sentimetnsdf_2\")\n\n\n\n`\nTopic\nModeling analysis with stopwords and punctuation removed\nResponse: As can be seen from the results above,\nremoving stopwords and punctuation removes a good deal of the unwanted\nlanguage from the corpus and does a slightly more comprehensible job in\ndisplaying the information. However, any kind of stemming or reduction\nwill be difficult with posts about firearms for a number of reasons.\nFirstly the language surrounding firearms involves numbers for model\nnumbers, ammunition calibers and the capacity of magazines and other\ndevices that hold bullets. This results in difficulty removing both\npunctuation and numbers from the data as they give a sense of what sort\nof each of the aforementioned items people are interesting in talking\nabout. As a results removing the punctuation is difficult because it\nallows for more comprehensible data by reducing the usage of unneeded\npunctuation like exclamaintion points and questions marks that are\ncommon on a forum of this nature but not useful in analyzing the common\ntopics and language.\n\n\n\n",
    "preview": "posts/2022-03-29-tad-post-4/tad-post-4_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2022-03-29T13:20:33-04:00",
    "input_file": "tad-post-4.knit.md"
  }
]
