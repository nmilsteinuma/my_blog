[
  {
    "path": "posts/2022-03-31-networks-of-creativity/",
    "title": "Networks of Creativity",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-03-31",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\n\nFashion_house_CDs <- read_excel(\"~/Desktop/Spring 2022/Networks/Fashion_house_CDs.xlsx\")\n\n\n\n\n\n\n\n\nset.seed(777)\nggplot(Fashion_house_CDs_network, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_edges(color = \"grey25\", alpha = 0.25, arrow = arrow(length = unit(2, \"pt\"), type = \"closed\"))+\n  geom_nodes(color = \"blue\", alpha = 0.3, size = 2) +\n  geom_nodetext(aes( label = vertex.names ), size=1)+\n    ggtitle(\"Networks of Creativity\") +\n  theme_blank()\n\n\n\n\n\n\n\n",
    "preview": "posts/2022-03-31-networks-of-creativity/networks-of-creativity_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-03-31T13:57:55-04:00",
    "input_file": "networks-of-creativity.knit.md"
  },
  {
    "path": "posts/2022-03-29-blog-post-7-networks/",
    "title": "Blog Post 7 Networks",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-03-29",
    "categories": [],
    "contents": "\n\n\n\n\n\n\nAn Introduction to\nthe Project and Dataset\nThe project that I am doing involves conflict in\nthe high middle ages. This was the period between 1000 and 1200\n\n\n\nPart 1:\nDescribe the Dataset You Are\nUsing:\nThe Dataset Being Used: The dataset that I am using\nis wikipedia list of wars throughout history, this article is the “List\nof wars: 1000–1499” which acts as a subset of the “2nd-millennium\nconflicts” I chose this dataset as an exemplar of popular history’s\ndepiction of the centralization of worldwide conflict. Wikipedia, being\nan accessible source generally created from relevant citations makes it\na good case study to see where historical writers and academics center\ntheir world are relevant conflicts.\nIdentify initial network\nformat:\nAnswer: The initial network format is as an edge\nlist, the first, in column contains the winners of each\nwar while the second, out column contains the losers of\neach. These sets of belligerents are directed\nNetwork\nStructure: Wars Startings in the 1000s\n\n Network attributes:\n  vertices = 111 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 153 \n    missing edges= 0 \n    non-missing edges= 153 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nNetwork\nStructure: Wars Startings in the 1100s\n\n Network attributes:\n  vertices = 97 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 238 \n    missing edges= 0 \n    non-missing edges= 238 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nNetwork Structure:\nWars Starting in the 1200s\n\n Network attributes:\n  vertices = 161 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 313 \n    missing edges= 0 \n    non-missing edges= 313 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nIdentify Nodes: Describe and identify the nodes\n(including how many nodes are in the dataset)\nAnswer: Nodes or vertices in these datasets\nrepresent belligerents in wars throughout history, the involved parties\nin each conflict can be a nation, province, individual, or group so long\nas they are listed as involved in the conflict. In the 1000s there are\n117, in the 1100s there are 78 and in the 1200s there are 161.\nWhat Constitutes a Tie: What constitutes a tie or\nedge (including how many ties, whether ties are directed/undirected and\nweighted/binary, and how to interpret the value of the tie if any)\nAnswer: A tie or edge in this dataset represents a\nwar, this war can be between two nations or groups within a nation.\nThese edges can represent a war that involved many more nations but are\nalways tied to each and every party involved on both sides. These edges\nare directed and the direction indicates which side “won” the conflict\n(if an edge has an arrow pointing to another the node that originated\nthat arrow won the war against them. There are 153 edges in the 1000s,\n225 edges in 1100s and 313 edges in the 1200s.\nEdge Attributes and Subset: Whether or not there are\nedge attributes that might be used to subset data or stack multiple\nnetworks (e.g., tie type, year, etc).\nAnswer: There are a number of attributes that could\nbe used to subset the data, year that the conflict began or the length\nof time it lasted are available. Aspects like each side’s religion and\nthe area where the conflict took place could be used to subset the data\nitself.\nPart 2:\nBrokerage and Betweeness\ncentrality\nWhat are betweeness and brokerage cenrrality\nCalculate brokerage and betweenneess centrality measures for one or more\nsubsets of your network data, and write up the results and your\ninterpretation of them.\nAnswer: I will be calculating these measures for\nwars in 1000-1099, 1100-1199, and 1200-1399.\n\n\n\nBrokerage scores in the\n1000s\n\n\n\n\n\n(wars_in_1000s.nodes.stat_2%>%\n  arrange(desc(broker.tot))%>%\n  slice(1:10))[,c(1,11:15)] %>%kable()\n\n\nname\nbroker.tot\nbroker.coord\nbroker.itin\nbroker.rep\nbroker.gate\nByzantine Empire\n22.7376579\nNaN\n3.1654785\nNaN\nNaN\nHoly Roman Empire\n9.2813605\nNaN\n2.2468427\nNaN\nNaN\nSultanate of Rum\n9.2813605\nNaN\n-0.5090648\nNaN\nNaN\nEngland\n6.9745666\nNaN\n5.0036896\n-0.0853606\n-0.0853606\nKingdom of Sicily\n5.0522384\n-0.0176111\n4.0866123\n-0.1201631\n-0.1201631\nSeljuk Empire\n1.9765133\n-0.0176111\n-0.5084146\n3.4677529\n-0.1201631\nKingdom of France\n1.9765133\nNaN\n-0.5090648\nNaN\nNaN\nKingdom of Georgia\n0.8231164\n-0.0176111\n-0.5084146\n-0.1201631\n-0.1201631\nPapal States\n0.4386507\n-0.0176111\n-0.5084146\n-0.1201631\n10.6435850\nGhaznavids\n0.0541851\n-0.1380791\n-0.4907567\n-0.2903366\n-0.2903366\n\nBrokerage scores in the\n1100s\n\n\n\n\n\n\n\n\n(wars_in_1100s.nodes.stat_2%>%\n  arrange(desc(broker.tot))%>%\n  slice(1:10))[,c(1,10:14)] %>%kable()\n\n\nname\nbroker.tot\nbroker.coord\nbroker.itin\nbroker.rep\nbroker.gate\nKingdom of Jerusalem\n17.1050061\nNaN\n2.8705599\n24.5610650\n-0.1357675\nFatimid Caliphate\n10.2415178\nNaN\n-0.6472506\nNaN\nNaN\nAyyubid Dynasty\n9.3615834\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nZengid Dynasty\n7.4257278\nNaN\n0.7591543\nNaN\nNaN\nByzantine Empire\n6.8977671\nNaN\n0.7602887\n-0.1357675\n-0.1357675\nEngland\n5.8418459\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nHoly Roman Empire\n3.0260558\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nKingdom of France\n1.6181608\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nKingdom of Sicily\n0.5622395\n-0.1467125\n-0.6293842\n-0.3476788\n-0.3476788\nPapal States\n0.0342789\n-0.1264908\n-0.6336748\n-0.3236913\n2.5014147\n\nBrokerage scores in the\n1200s\n\n\n\n\n\n\n\n\n\n\nname\nbroker.tot\nbroker.coord\nbroker.itin\nbroker.rep\nbroker.gate\nMongol Empire\n47.964825\nNaN\n-0.5966483\nNaN\nNaN\nKingdom of France\n28.663539\nNaN\n-0.5966483\nNaN\nNaN\nAyyubid Dynasty\n26.995527\nNaN\n2.3528915\nNaN\nNaN\nKingdom of England\n21.991489\nNaN\n8.9893561\nNaN\nNaN\nRepublic of Genoa\n11.983415\nNaN\n-0.5966483\nNaN\nNaN\nKnights Templar\n10.077115\nNaN\n1.6155066\nNaN\nNaN\nHoly Roman Empire\n4.834790\n-0.0170801\n-0.5961482\n10.865523\n10.865523\nPrincipality of Antioch\n4.834790\n-0.0170801\n2.3541101\n13.613565\n-0.126648\nKingdom of Cyprus\n4.596503\n58.5391124\n0.1414163\n13.613565\n10.865523\nArmenian Kingdom of Cilicia\n3.881640\n-0.0170801\n-0.5961482\n-0.126648\n-0.126648\n\n\nname\nbroker.gate\nPapal States\n10.6435850\nCounty of Aversa\n-0.0853606\nCounty of Sicily\n-0.0853606\nEngland\n-0.0853606\nChola Empire\n-0.0853606\nCounty of Apulia\n-0.1201631\nKingdom of Sicily\n-0.1201631\nKingdom of Georgia\n-0.1201631\nGreat Seljuq Empire\n-0.1201631\nSeljuk Empire\n-0.1201631\nname\nbroker.tot\nByzantine Empire\n22.7376579\nHoly Roman Empire\n9.2813605\nSultanate of Rum\n9.2813605\nEngland\n6.9745666\nKingdom of Sicily\n5.0522384\nSeljuk Empire\n1.9765133\nKingdom of France\n1.9765133\nKingdom of Georgia\n0.8231164\nPapal States\n0.4386507\nGhaznavids\n0.0541851\n\nOption 2.A\nFor a Specific Research Question: If you have a\nspecific research question, please feel free to use that to guide your\nanalysis. Otherwise, you may want to orient your analysis as follows in\norder to identify a compelling question or noteworthy pattern in the\ndata that can be interpreted.\nAnswer: Since I am interested in the relative power\nof nations by their relative position ad centrality in the worldwide\nconflict, network brokerage can be used to illustrate significant\npositions in global conflict. Below I wanted to look at 4 kinds of\nbrokerage, these are broker.gate or gatekeeper, coordinator, liason, and\nitinerant. I am interested to see if these specific coordination types\nare primarily done by specific nations.\n\n\n\n\n\n\n\n\n\nTotal Brokerage\nExplanation: Looking at total brokerage in this\ndataset gives a sense of which factions were responsible for highest\nconnection of unconnected actors through conflict. Given the crusades\nigniting conflict between Europe and the middle east it is sensible that\nthe Byzantine Empire in the center of both connects the most unconnected\nactors through conflict closely followed by the Sultanate of Rum, a\nmajor Muslim faction that fought against the crusades and third being\nthe Holy Roman Empire who participated in many conflicts including the\ncrusades. These are followed by England who centered the wars in the\nBritish isles and the Kingdom of Sicily who were also in a position of\nconflict.\n\nname\nbroker.tot\nByzantine Empire\n22.737658\nHoly Roman Empire\n9.281360\nSultanate of Rum\n9.281360\nEngland\n6.974567\nKingdom of Sicily\n5.052238\n\nCoordinator Brokerage\nExplanation: In this case no particular country is\nvery high above any other in terms of their coordinator brokerage,\nmeaning that within groups no particular nations appear to be brokering\nmore within the groups.\n\nname\nbroker.coord\nCounty of Apulia\n-0.0176111\nKingdom of Sicily\n-0.0176111\nKingdom of Georgia\n-0.0176111\nGreat Seljuq Empire\n-0.0176111\nPapal States\n-0.0176111\n\nItinerant Brokerage\nExplanation: Itinerant brokerage represents when a\nnon-group actor connects 2 actors in a group it is no in to each other,\nin this case England has the highest score. Looking at the network graph\nthey do appear to connect 2 actors in a group together.\n\nname\nbroker.itin\nEngland\n5.0036896\nKingdom of Sicily\n4.0866123\nByzantine Empire\n3.1654785\nHoly Roman Empire\n2.2468427\nPrincipality of Kiev\n0.4812412\n\nRepresentative Brokerage\nExplanation: Representative brokerage indicates that\nthe broker, or nation in question loses a war to another in their group,\nbut wins another against a faction outside of their group. This can be\nthough of as their directed connections to them. In this case the Seljuk\nEmpire and Kingdom of Aragon have instances in which they lose to\nfactions within their group before beating those outside of it.\n\nname\nbroker.rep\nSeljuk Empire\n3.4677529\nKingdom of Aragon\n0.9281821\nCounty of Aversa\n-0.0853606\nCounty of Sicily\n-0.0853606\nEngland\n-0.0853606\n\nGatekeeper Brokerage\nExplanation: The Papal states being ranked highest\nin gatekeeper brokerage is an interesting observation as no other nation\nin the dataset appears to be close to their level as most are negative\nin this category. In this cae being a gatekeeper means that they are in\nat conflict in a group with another while the nation in a different\ngroup of conflicts is only at war with them from the group. This is an\ninteresting observation given the Papal states role as a coordinator of\nthe war, but not a participant in the conflcit as directly as other\nbelligerents. (This being the crusade given the period)\n\nname\nbroker.gate\nPapal States\n10.6435850\nCounty of Aversa\n-0.0853606\nCounty of Sicily\n-0.0853606\nEngland\n-0.0853606\nChola Empire\n-0.0853606\n\nLiaison Brokerage\nExplanation: A liaison broker, in this case, is a\nfaction that loses a war to a group they do not belong to and wins a war\nagainst a different group than the first that they also do not belong\nto. The Byzantine Empire, Sultanate of Rum, and Holy Roman Empire are\nhighest in this category likely owing to their frequent states of\nconflict beyond the crusades against a variety of groups.\n\nname\nbroker.lia\nByzantine Empire\n28.140866\nSultanate of Rum\n12.477603\nHoly Roman Empire\n10.961803\nEngland\n6.548214\nKingdom of Sicily\n4.589419\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork\n1000s Plot Grouping Determined with No Cluster Method\n\n\n\nNetwork\n1000s Plot Grouping Determined with the Average Cluster Method\n\n\n\nNetwork\n1000s Plot Grouping Determined with the Single Cluster Method\n\n\n\nNetwork\n1000s Plot Grouping Determined with the Ward.D Cluster Method\n\n\n\nNetwork 1000s Plot igraph\n\n\n\nNetwork Graphing 1100s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork\n1100s Plot Grouping Determined with No Cluster Method\n\n\n\nNetwork\n1100s Plot Grouping Determined with the Average Cluster Method\n\n\n\nNetwork\n1100s Plot Grouping Determined with the Single Cluster Method\n\n\n\nNetwork\n1100s Plot Grouping Determined with the Ward.D Cluster Method\n\n\n\nNetwork 1100s Plot igraph\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwars_in_1000s_edgelist <- as.matrix(wars_in_1000s)\n\nwars_in_1000s_edgelist_network_edgelist <- graph.edgelist(wars_in_1000s_edgelist, directed=TRUE)\n\nwars_in_1000s.ig<-graph_from_data_frame(wars_in_1000s)\n\nwars_in_1000s_network <- asNetwork(wars_in_1000s.ig)\n\n\n\n\n\naspects_of_1000s_states <- read_excel(\"~/Desktop/Spring 2022/Networks/aspects_of_1000s_states.xlsx\")\n\ntotal_1000s <- merge(aspects_of_1000s_states, wars_in_1000s.nodes.stat_2, by=\"name\")\n\n\n\n\n\ntotal_1000s_brokerag_reg<-total_1000s\n\ntotal_1000s_brokerag_reg$win_rate <- (total_1000s_brokerag_reg$outdegree/total_1000s_brokerag_reg$totdegree)\n\ntotal_1000s_brokerag_reg$loss_rate <- (total_1000s_brokerag_reg$indegree/total_1000s_brokerag_reg$totdegree)\n\ntotal_1000s_brokerag_reg_binom <- total_1000s_brokerag_reg %>% mutate(more_win_or_loss = case_when(\n  win_rate < 0.5 ~ 0,\n    win_rate >= 0.5 ~ 1))\n\nFirst_1000s_regression <- glm(more_win_or_loss~.-name-totdegree-indegree-outdegree-dc-eigen.dc-win_rate-loss_rate, total_1000s_brokerag_reg_binom, family=binomial)\n\nFirst_1000s_regression\n\n\n\nCall:  glm(formula = more_win_or_loss ~ . - name - totdegree - indegree - \n    outdegree - dc - eigen.dc - win_rate - loss_rate, family = binomial, \n    data = total_1000s_brokerag_reg_binom)\n\nCoefficients:\n (Intercept)      Catholic         Islam      Orthodox      Buddhist  \n  -2.090e+01     1.446e-01    -7.108e-02    -4.043e-01    -8.572e-02  \n       Pagan      Tengrism        Shinto         Hindu     Shamanism  \n   5.506e-01    -5.656e+01     1.820e+00    -2.142e+00    -1.506e+00  \n       eigen         close            rc      eigen.rc    broker.tot  \n  -1.877e+03     5.146e+03    -3.979e+00     1.574e+03     2.378e+02  \nbroker.coord   broker.itin    broker.rep   broker.gate    broker.lia  \n  -9.610e+01    -9.449e+01    -7.164e+01    -2.810e+01    -1.298e+02  \n\nDegrees of Freedom: 101 Total (i.e. Null);  82 Residual\n  (8 observations deleted due to missingness)\nNull Deviance:      140.8 \nResidual Deviance: 4.53e-09     AIC: 40\n\n\n\nset.seed(292)\n\ntotal_1000s_for_regression <- total_1000s[,-c(1, 20:25)]\n\ntotal_1000s_for_regression$win_rate <- (total_1000s_for_regression$outdegree/total_1000s_for_regression$totdegree)\n\ntotal_1000s_for_regression$loss_rate <- (total_1000s_for_regression$indegree/total_1000s_for_regression$totdegree)\n\ntotal_1000s_for_regression <- total_1000s_for_regression %>% mutate(more_win_or_loss = case_when(\n  win_rate < 0.5 ~ 0,\n    win_rate >= 0.5 ~ 1))\n\nFirst_1000s_regression <- glm(more_win_or_loss~.-loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, total_1000s_for_regression, family=binomial)\n\nFirst_1000s_regression\n\n\n\nCall:  glm(formula = more_win_or_loss ~ . - loss_rate - win_rate - totdegree - \n    indegree - outdegree - dc - eigen.dc, family = binomial, \n    data = total_1000s_for_regression)\n\nCoefficients:\n(Intercept)     Catholic        Islam     Orthodox     Buddhist  \n   -15.1948      13.9008      12.7531      14.6893      15.0858  \n      Pagan     Tengrism       Shinto        Hindu    Shamanism  \n     0.9610      11.6691      16.0623       9.1358      -0.1497  \n      eigen        close           rc     eigen.rc  \n   -82.1100     256.5294      -3.3322     -17.3152  \n\nDegrees of Freedom: 109 Total (i.e. Null);  96 Residual\nNull Deviance:      152.3 \nResidual Deviance: 58.4     AIC: 86.4\n\n\n\nset.seed(6738)\n\nin_training<- sample(1:nrow(total_1000s_for_regression),  nrow(total_1000s_for_regression) * 0.7 )\n\ntraining_1000s <- total_1000s_for_regression[in_training,]\n\ntest_1000s <- total_1000s_for_regression[-in_training,]\n\nlm_1000s_binom_subset_1 <- glm(more_win_or_loss~.-loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, total_1000s_for_regression, family=binomial, subset = in_training )\n\nlogsitic_1_1000s_prob <- predict(lm_1000s_binom_subset_1, test_1000s,\ntype = \"response\")\n\nlog_preds_1<-ifelse(logsitic_1_1000s_prob >= 0.5, 1, 0)\n\nprediction_1_logs <-mean(log_preds_1 == test_1000s$more_win_or_loss)\n\nprediction_1_logs %>% kable()\n\n\nx\n0.9090909\n\n\n\nlibrary(glmnet)\nlibrary(MASS)\n\n\n\n\n\nset.seed(246)\n\nx_ridge <- model.matrix(more_win_or_loss ~ .-loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, total_1000s_for_regression)[, -1] \n\ny_ridge <- total_1000s_for_regression$more_win_or_loss\n\ngrid <- 10^seq(10, -2, length = 100)\n\nridge.mod <- glmnet(x_ridge, y_ridge, alpha = 0, lambda = grid)\n\ndim(coef(ridge.mod))\n\n\n[1]  14 100\n\n\n\nset.seed(729)\ntrain_ridge <- sample(1:nrow(x_ridge), nrow(x_ridge)*0.8 ) \n\ntest_ridge <- (-train_ridge)\n\ny.test_ridge <- y_ridge[test_ridge]\n\n\n\n\n\nset.seed(9292)\n\nridge.mod <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], \n                    alpha = 0, lambda = grid, thresh = 1e-12)\n\nridge.pred <- predict(ridge.mod, s = 4, newx = x_ridge[test_ridge,])\n\nmean((ridge.pred - y.test_ridge)^2) %>% kable()\n\n\nx\n0.2416376\n\n\n\nset.seed(231)\nridge.pred <- predict(ridge.mod, s = 0, newx = x_ridge[test_ridge, ], \n                      exact = T, x = x_ridge[train_ridge, ], y = y_ridge[train_ridge])\n\npredict(ridge.mod, s = 0, exact = T, type = \"coefficients\", \n        x = x_ridge[train_ridge, ], y = y_ridge[train_ridge])[1:14, ]\n\n\n(Intercept)    Catholic       Islam    Orthodox    Buddhist \n 0.21024033  0.21827317 -0.01160454  0.21312966  0.35601806 \n      Pagan    Tengrism      Shinto       Hindu   Shamanism \n 0.08955257  0.14069809  0.38278477 -0.07034364 -0.01038790 \n      eigen       close          rc    eigen.rc \n-4.61480591 12.51011844 -0.29977861  4.64835194 \n\n\n\nset.seed(9292)\n\ncv.out <- cv.glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], alpha = 0) \n\nplot(cv.out)\n\n\n\n\n\n\nset.seed(9292)\n\nbestlam <- cv.out$lambda.min\n\nbestlam\n\n\n[1] 0.415338\n\n\n\nset.seed(9292)\n\nridge.pred <- predict(cv.out, s = bestlam, newx = x_ridge[test_ridge,])\n\nmean((ridge.pred - y.test_ridge)^2) %>% kable()\n\n\nx\n0.174632\n\n\n\nset.seed(2897)\n\nx_lasso <- model.matrix(more_win_or_loss ~ .-loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, total_1000s_for_regression)[, -1] \n\ny_lasso <- total_1000s_for_regression$more_win_or_loss\n\ngrid <- 10^seq(10, -2, length = 100)\n\nlasso.mod <- glmnet(x_lasso, y_lasso, alpha = 0, lambda = grid)\n\ndim(coef(lasso.mod))\n\n\n[1]  14 100\n\n\n\nset.seed(729)\n\ntrain_lasso <- sample(1:nrow(x_ridge), nrow(x_ridge)*0.8 ) \n\ntest_lasso <- (-train_lasso)\n\ny.test_lasso <- y_lasso[test_lasso]\n\n\n\n\n\nset.seed(9292)\n\nlasso.mod <- glmnet(x_lasso[train_lasso, ], y_lasso[train_lasso], \n                    alpha = 1, lambda = grid)\n\nplot(lasso.mod)\n\n\n\n\n\n\nset.seed(1029)\n\ncv.out_2 <- cv.glmnet(x_lasso[train_lasso, ], y_lasso[train_lasso], alpha = 1) \n\nplot(cv.out_2)\n\n\n\n\n\n\nset.seed(1920)\n\nbestlam_2 <- cv.out_2$lambda.min\n\nlasso.pred <- predict(cv.out_2, s = bestlam_2, newx = x_ridge[test_ridge,])\n\nmean((lasso.pred - y.test_ridge)^2) %>% kable()\n\n\nx\n0.1749583\n\n\n\nset.seed(2739)\n\nout <- glmnet(x_lasso[train_lasso, ], y_lasso[train_lasso], \n              alpha = 1, lambda = grid)\n\nlasso.coef <- predict(out, type = \"coefficients\", s = bestlam_2)[1:14, ]\n\nlasso.coef\n\n\n(Intercept)    Catholic       Islam    Orthodox    Buddhist \n 0.42561685  0.05577020 -0.09275344  0.00000000  0.00000000 \n      Pagan    Tengrism      Shinto       Hindu   Shamanism \n 0.00000000  0.00000000  0.00000000  0.00000000  0.00000000 \n      eigen       close          rc    eigen.rc \n 0.00000000  3.22570629 -0.21240622  0.00000000 \n\n\n\naspects_of_1100s_states <- read_excel(\"~/Desktop/Spring 2022/Networks/aspects_of_1100s_states.xlsx\")\n\ntotal_1100s <- merge(aspects_of_1100s_states, wars_in_1100s.nodes.stat_2, by=\"name\")\n\n\n\n\n\naspects_of_1200s_states <- read_excel(\"~/Desktop/Spring 2022/Networks/aspects_of_1200s_states.xlsx\")\n\ntotal_1200s <- merge(aspects_of_1200s_states, wars_in_1200s.nodes.stat_2, by=\"name\")\n\n\n\nCommunity Grouping\nLabel Propagation 1000s:\nThe first community cluster below is done using label propagation.\nThis results in 39 groups\n\n\nset.seed(23)\ncomm.lab<-label.propagation.community(wars_in_1000s.ig)\n#Inspect clustering object\n# igraph::groups(comm.lab)\n\n\n\n\n\n\nWalktrap 1000s:\nWalktrap classification as seen below results in 19 distinct\ncommunities.\n\n\nset.seed(238)\n#Run clustering algorithm: fast_greedy\nwars_in_1000s.wt<-walktrap.community(wars_in_1000s.ig)\n\n#igraph::groups(wars_in_1000s.wt)\n\n\n\nAdding more steps resulted in 19 groups for both 10 and 20 steps.\n\n\n#Run & inspect clustering algorithm: 10 steps\n#igraph::groups(walktrap.community(wars_in_1000s.ig, steps=10)) \n#Run & inspect clustering algorithm: 20 steps\n#igraph::groups(walktrap.community(wars_in_1000s.ig ,steps=20))\n#Run & inspect clustering algorithm\n\n\n\n\n\n\nMachine\nLearning, Regression and Principle Components:\n\n\ntotal_1000s_for_PCA <- total_1000s_brokerag_reg_binom[-c(20:27)]\n\napply(total_1000s_for_PCA[-1], 2, mean)\n\n\n        Catholic            Islam         Orthodox         Buddhist \n     0.454545455      0.181818182      0.154545455      0.063636364 \n           Pagan         Tengrism           Shinto            Hindu \n     0.036363636      0.018181818      0.054545455      0.045454545 \n       Shamanism        totdegree         indegree        outdegree \n     0.009090909      2.754545455      1.336363636      1.418181818 \n           eigen            close               rc         eigen.rc \n     0.028058711      0.023546832      0.287358773      0.003637773 \n              dc         eigen.dc more_win_or_loss \n     0.712641227      0.024420939      0.481818182 \n\n\n\napply(total_1000s_for_PCA[-1], 2, var)\n\n\n        Catholic            Islam         Orthodox         Buddhist \n    0.2502085071     0.1501251043     0.1318598832     0.0601334445 \n           Pagan         Tengrism           Shinto            Hindu \n    0.0353628023     0.0180150125     0.0520433695     0.0437864887 \n       Shamanism        totdegree         indegree        outdegree \n    0.0090909091     8.9208507089     2.6656380317     6.3189324437 \n           eigen            close               rc         eigen.rc \n    0.0076304265     0.0019575460     0.1260782284     0.0004728954 \n              dc         eigen.dc more_win_or_loss \n    0.1260782284     0.0056490031     0.2519599666 \n\n\n\npr.out <- prcomp(total_1000s_for_PCA[-1], scale = TRUE)\n\n\n\n\n\nnames(pr.out)\n\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\n\npr.out$center\n\n\n        Catholic            Islam         Orthodox         Buddhist \n     0.454545455      0.181818182      0.154545455      0.063636364 \n           Pagan         Tengrism           Shinto            Hindu \n     0.036363636      0.018181818      0.054545455      0.045454545 \n       Shamanism        totdegree         indegree        outdegree \n     0.009090909      2.754545455      1.336363636      1.418181818 \n           eigen            close               rc         eigen.rc \n     0.028058711      0.023546832      0.287358773      0.003637773 \n              dc         eigen.dc more_win_or_loss \n     0.712641227      0.024420939      0.481818182 \n\n\n\npr.out$scale\n\n\n        Catholic            Islam         Orthodox         Buddhist \n      0.50020846       0.38745981       0.36312516       0.24522122 \n           Pagan         Tengrism           Shinto            Hindu \n      0.18805000       0.13422002       0.22813016       0.20925221 \n       Shamanism        totdegree         indegree        outdegree \n      0.09534626       2.98677932       1.63267818       2.51374868 \n           eigen            close               rc         eigen.rc \n      0.08735231       0.04424416       0.35507496       0.02174616 \n              dc         eigen.dc more_win_or_loss \n      0.35507496       0.07515985       0.50195614 \n\n\n\n\n\n\nbiplot(pr.out, scale = 0)\n\n\n\n\n\n\npr.out$rotation = -pr.out$rotation \n\npr.out$x = -pr.out$x\n\nbiplot(pr.out, scale = 0)\n\n\n\n\n\n\npr.out$sdev\n\n\n [1] 2.217501e+00 1.681548e+00 1.239242e+00 1.211199e+00 1.065982e+00\n [6] 1.037692e+00 1.029507e+00 1.011117e+00 1.005425e+00 9.514802e-01\n[11] 8.848499e-01 7.782431e-01 6.162540e-01 4.426224e-01 2.541422e-01\n[16] 1.091189e-01 7.597269e-16 6.258811e-16 2.174635e-16\n\n\n\npr.var <- pr.out$sdev^2\n\npr.var\n\n\n [1] 4.917311e+00 2.827605e+00 1.535720e+00 1.467004e+00 1.136318e+00\n [6] 1.076804e+00 1.059884e+00 1.022359e+00 1.010879e+00 9.053146e-01\n[11] 7.829594e-01 6.056623e-01 3.797690e-01 1.959146e-01 6.458828e-02\n[16] 1.190694e-02 5.771849e-31 3.917271e-31 4.729037e-32\n\n\n\npve <- pr.var / sum(pr.var)\n\npve\n\n\n [1] 2.588059e-01 1.488213e-01 8.082739e-02 7.721075e-02 5.980622e-02\n [6] 5.667390e-02 5.578337e-02 5.380835e-02 5.320417e-02 4.764814e-02\n[11] 4.120839e-02 3.187696e-02 1.998784e-02 1.031129e-02 3.399383e-03\n[16] 6.266808e-04 3.037815e-32 2.061722e-32 2.488967e-33\n\n\n\npar(mfrow = c(1, 2))\nplot(pve, xlab = \"Principal Component\",\nylab = \"Proportion of Variance Explained\", ylim = c(0, 1),\ntype = \"b\")\n\nplot(cumsum(pve), xlab = \"Principal Component\",\nylab = \"Cumulative Proportion of Variance Explained\", ylim = c(0, 1), type = \"b\")\n\n\n\n\n\n\nnames(total_1200s)\n\n\n [1] \"name\"         \"Catholic\"     \"Islam\"        \"Orthodox\"    \n [5] \"Buddhist\"     \"Pagan\"        \"Tengrism\"     \"Shinto\"      \n [9] \"Hindu\"        \"Shamanism\"    \"totdegree\"    \"indegree\"    \n[13] \"outdegree\"    \"eigen\"        \"rc\"           \"eigen.rc\"    \n[17] \"dc\"           \"eigen.dc\"     \"broker.tot\"   \"broker.coord\"\n[21] \"broker.itin\"  \"broker.rep\"   \"broker.gate\"  \"broker.lia\"  \n\n\n\ntotal_1200s_brokerag_reg<-total_1200s\n\n\n\n\n\ntotal_1200s_brokerag_reg$win_rate <- (total_1200s_brokerag_reg$outdegree/total_1200s_brokerag_reg$totdegree)\n\n\n\n\n\ntotal_1200s_brokerag_reg$loss_rate <- (total_1200s_brokerag_reg$indegree/total_1200s_brokerag_reg$totdegree)\n\n\n\n\n\ntotal_1200s_brokerag_reg_binom <- total_1200s_brokerag_reg %>% mutate(more_win_or_loss = case_when(\n  win_rate < 0.5 ~ 0,\n    win_rate >= 0.5 ~ 1))\n\n\n\n\n\ntotal_1200s_for_PCA <- total_1200s_brokerag_reg_binom[-c(20:27)]\n\n\napply(total_1200s_for_PCA[-1], 2, mean)\n\n\n   Catholic       Islam    Orthodox    Buddhist       Pagan \n0.712500000 0.068750000 0.087500000 0.087500000 0.012500000 \n   Tengrism      Shinto       Hindu   Shamanism   totdegree \n0.025000000 0.000000000 0.006250000 0.000000000 3.918750000 \n   indegree   outdegree       eigen          rc    eigen.rc \n1.962500000 1.956250000 0.025567955 0.158754617 0.002192746 \n         dc    eigen.dc  broker.tot \n0.841245383 0.023375209 0.341581810 \n\n\n\napply(total_1200s_for_PCA[-1], 2, var)\n\n\n    Catholic        Islam     Orthodox     Buddhist        Pagan \n2.061321e-01 6.442610e-02 8.034591e-02 8.034591e-02 1.242138e-02 \n    Tengrism       Shinto        Hindu    Shamanism    totdegree \n2.452830e-02 0.000000e+00 6.250000e-03 0.000000e+00 2.666631e+01 \n    indegree    outdegree        eigen           rc     eigen.rc \n6.237579e+00 1.595405e+01 5.631476e-03 7.141295e-02 7.316162e-05 \n          dc     eigen.dc   broker.tot \n7.141295e-02 4.574350e-03 3.001236e+01 \n\n\n\n# I cannot scale variables with \n\ntotal_1200s_for_PCA<-total_1200s_for_PCA[-c(8,10)]\n\n\n\n\n\npr.out_2 <- prcomp(total_1200s_for_PCA[-1], scale = TRUE)\n\n\n\n\n\nnames(pr.out_2)\n\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\n\npr.out_2$center\n\n\n   Catholic       Islam    Orthodox    Buddhist       Pagan \n0.712500000 0.068750000 0.087500000 0.087500000 0.012500000 \n   Tengrism       Hindu   totdegree    indegree   outdegree \n0.025000000 0.006250000 3.918750000 1.962500000 1.956250000 \n      eigen          rc    eigen.rc          dc    eigen.dc \n0.025567955 0.158754617 0.002192746 0.841245383 0.023375209 \n broker.tot \n0.341581810 \n\n\n\npr.out_2$scale\n\n\n   Catholic       Islam    Orthodox    Buddhist       Pagan \n0.454017704 0.253822971 0.283453545 0.283453545 0.111451261 \n   Tengrism       Hindu   totdegree    indegree   outdegree \n0.156615139 0.079056942 5.163943541 2.497514488 3.994251963 \n      eigen          rc    eigen.rc          dc    eigen.dc \n0.075043164 0.267232010 0.008553457 0.267232010 0.067633938 \n broker.tot \n5.478353760 \n\n\n\n\n\n\nbiplot(pr.out_2, scale = 0)\n\n\n\n\n\n\npr.out_2$rotation = -pr.out_2$rotation \n\npr.out_2$x = -pr.out_2$x\n\nbiplot(pr.out_2, scale = 0)\n\n\n\n\n\n\npr.out$sdev\n\n\n [1] 2.217501e+00 1.681548e+00 1.239242e+00 1.211199e+00 1.065982e+00\n [6] 1.037692e+00 1.029507e+00 1.011117e+00 1.005425e+00 9.514802e-01\n[11] 8.848499e-01 7.782431e-01 6.162540e-01 4.426224e-01 2.541422e-01\n[16] 1.091189e-01 7.597269e-16 6.258811e-16 2.174635e-16\n\n\n\npr.var_2 <- pr.out_2$sdev^2\n\npr.var_2\n\n\n [1] 4.903737e+00 2.344663e+00 1.670548e+00 1.250176e+00 1.132904e+00\n [6] 1.097802e+00 1.011326e+00 9.460639e-01 8.661454e-01 5.139677e-01\n[11] 1.659928e-01 9.667541e-02 2.916516e-30 4.832251e-31 2.292490e-31\n[16] 1.889562e-32\n\n\n\npve_2 <- pr.var_2 / sum(pr.var_2)\n\npve_2\n\n\n [1] 3.064835e-01 1.465414e-01 1.044092e-01 7.813602e-02 7.080651e-02\n [6] 6.861260e-02 6.320785e-02 5.912899e-02 5.413409e-02 3.212298e-02\n[11] 1.037455e-02 6.042213e-03 1.822822e-31 3.020157e-32 1.432806e-32\n[16] 1.180977e-33\n\n\n\npar(mfrow = c(1, 2))\nplot(pve_2, xlab = \"Principal Component\",\nylab = \"Proportion of Variance Explained\", ylim = c(0, 1),\ntype = \"b\")\n\nplot(cumsum(pve_2), xlab = \"Principal Component\",\nylab = \"Cumulative Proportion of Variance Explained\", ylim = c(0, 1), type = \"b\")\n\n\n\n\n(information regarding the meaning of each type of brokerage was\nacquired from https://edis.ifas.ufl.edu/publication/WC197)\n\n\n\n",
    "preview": "posts/2022-03-29-blog-post-7-networks/blog-post-7-networks_files/figure-html5/unnamed-chunk-18-1.png",
    "last_modified": "2022-03-29T12:51:28-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-29-into-the-20th-century/",
    "title": "Into the 20th Century",
    "description": "A Network analysis of conflct in the 20th Century.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-03-29",
    "categories": [],
    "contents": "\n\n\n\nLoading and Summarizing Data\nLoading: Similarly to the datasets I used for the\nprior assignments, this one will involve the use of a conflict dataset.\nGiven the relative success of the analysis techniques on dyadic conflict\nin the 9th, 10th, and 11th centuries I decided to try it on a much more\nrecent period, that being the height of the cold war beginning in 1945\nand officially ending with the dissolution of the Soviet Union in 1992,\nhowever the dataset in question ends in 1989.\n\n\n\n\n\n\n\n\n\nInterpretation and Inital\nAnalysis\nInitial Interpretation: After briefly cleaning the\ndataset to remove excess spaces resulting in nations, or factions being\ncounted twice, I then converted my data in 4 different kinds of network\nobjects, being a matrix, edgelist graph, igraph, and network. A ggplot\nobject of the initial network can be seen below.\n\n\n\nContinued Visualization\nVisuals Though not necessarily practical for\nunderstanding the nature of the network the two visualization below are\nused to exhibit the increased complexity of this dataset relative to\nprior ones, as in previous models of conflict there were under 200 nodes\n(or factions) being visualized, as will be illustrated in subsequent\nanalysis there were about 600 nations, rebel groups, factions, and\nwarring parties considered direct belligerents in this dataset. It is\nalso important to note that this network does not include direct\nmilitary support that did did not constitute direct\ninvolvement, this will later be included as a grouping dummy variable,\nbut in its current form military support that is not directly\nintervention will not be considered.\n\n\nset.seed(2)\n\nggraph(Wars_in_latter_half_of_20th_network, 'dendrogram', circular = TRUE) + \n    geom_edge_elbow() + \n    coord_fixed() +\n    geom_edge_link0(edge_alpha = 0.001)+\n    geom_node_text(aes(label = name), size=1, repel=FALSE)\n\n\n\n\n\n\nggraph(Wars_in_latter_half_of_20th_network, layout = \"treemap\") + \n  geom_node_tile(aes(fill = depth))+    \n  geom_node_text(aes(label = name), size=1, repel=FALSE)\n\n\n\n\ninterpret the data, identifying at least two results of interest.\nQuestions you may want to consider include (but are not limited to) the\nfollowing. Calculate structural equivalence models for your network\ndata, and use clustering and blockmodeling to identify nodes in similar\npositions. Can you find any patterns in the data, and do the blocks\n“make sense.” What types of behavior would we expect to see (or do we\nsee) on the basis of equivalence and block assignment? Do different\nclustering methods and/or the use of weights make a difference? How much\ninsight can you get from plotting the block role assignments. You may\nalso want to see if nodes that are equivalent (and/or belong to same\nblock) are similar on measures of centrality introduced in earlier\nweeks.\nInital Analysis and\nInterpretation\nNetwork Summary: As can be seen by the network print\nbelow the conflict dataset comprises 599 nodes, factions, or\nbelligerents that are direct combatants in each\nconflict, these are directed with an in-degree being a lost war, and an\nout-degree being a won war. There are loops as many factions fight each\nother multiple times and amongst different sets of allies losing some\nwars and winning others.\n\n Network attributes:\n  vertices = 600 \n  directed = TRUE \n  hyper = FALSE \n  loops = TRUE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 1090 \n    missing edges= 0 \n    non-missing edges= 1090 \n\n Vertex attribute names: \n    vertex.names \n\n Edge attribute names not shown \n\n\n\n\n\n\ntemp<-data.frame(brokerage(Wars_in_latter_half_of_20th_network, cl = Wars_in_latter_half_of_20th_network.nodes.stat$totdegree)$z.nli)\n\nWars_in_latter_half_of_20th_network.nodes.stat_2 <- Wars_in_latter_half_of_20th_network.nodes.stat %>%\n  mutate(broker.tot = temp$t,\n         broker.coord = temp$w_I,\n         broker.itin = temp$w_O,\n         broker.rep = temp$b_IO,\n         broker.gate = temp$b_OI,\n         broker.lia = temp$b_O)\n\n\n\nBrokerage and Network\nAttributes\nBrokerage Scores: In this case brokerage scores are\ncalculated using the function brokerage() and appended to the data frame\nwith the these measures\nTotal Degree\n\n\n(Wars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  arrange(desc(totdegree))%>%\n  slice(1:10))[,c(1,2)] %>%kable()\n\n\nname\ntotdegree\nUnited States\n63\nIsrael\n47\nChina\n47\nSoviet Union\n39\nFrance\n38\nPhilippines\n35\nIndia\n34\nUnited Kingdom\n31\nNorth Korea\n30\nThailand\n27\n\nIn-Degree\n\n\n(Wars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  arrange(desc(indegree))%>%\n  slice(1:10))[,c(1,3)] %>%kable()\n\n\nname\nindegree\nSoviet Union\n25\nUnited States\n23\nNorth Korea\n21\nChina\n20\nSyria\n17\nCuba\n16\nSouth Vietnam\n15\nSLA\n13\nIsreal\n13\nLebanese Front\n13\n\nOut-Degree\n\n\n(Wars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  arrange(desc(outdegree))%>%\n  slice(1:10))[,c(1,4)] %>%kable()\n\n\nname\noutdegree\nIsrael\n47\nUnited States\n40\nIndia\n32\nFrance\n31\nChina\n27\nPhilippines\n27\nItaly\n26\nUnited Kingdom\n20\nIran\n19\nNorth Vietnam\n19\n\n\n\n(Wars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  arrange(desc(broker.coord))%>%\n  slice(0:5))[,c(1,12)] %>%kable()\n\n\nname\nbroker.coord\nKhmer Issarak\n-0.0042826\nSouth Africa\n-0.0042826\nKhmer Rouge\n-0.0042826\nPLO\n-0.0042826\nKhmer Republic\n-0.0042826\n\n\n\n(Wars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  arrange(desc(broker.itin))%>%\n  slice(0:5))[,c(1,13)] %>%kable()\n\n\nname\nbroker.itin\nUnited States\n30.874105\nEthiopia\n20.382727\nChina\n13.169813\nFrance\n11.202598\nUnited Kingdom\n7.268297\n\n\nname\nbroker.rep\nPKK\n6.3786629\nSouth Yemen\n3.2995076\nIran\n-0.0442578\nIsrael\n-0.0442578\nAustralia\n-0.0442578\n\n\nname\nbroker.gate\nMorocco\n9.0810516\nPUK\n4.1978461\nArab Socialist Ba’ath Party\n0.7128172\nSenegal\n0.7128172\nPapua New Guinea\n0.5288009\n\n\nname\nbroker.lia\nUnited States\n287.35034\nChina\n168.65374\nSoviet Union\n110.80239\nPhilippines\n70.16306\nUnited Kingdom\n67.49818\n\n\n\n\n\n\n\n\n\n\nExamining Centrality\n\n\nsna::dyad.census(Wars_in_latter_half_of_20th_network)\n\n\n     Mut Asym   Null\n[1,] 115  859 178726\n\nsna::triad.census(Wars_in_latter_half_of_20th_network)\n\n\n          003    012   102 021D 021U 021C 111D 111U 030T 030C 201\n[1,] 35246656 546903 17792 3817 1903 1822  526  639   46    0  83\n     120D 120U 120C 210 300\n[1,]    0    3   10   0   0\n\n\n\n(Wars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  arrange(desc(eigen))%>%\n  slice(0:5))[,c(1,5)] %>%kable()\n\n\nname\neigen\nChina\n0.3435636\nNorth Vietnam\n0.2973257\nPathet Lao\n0.2836275\nKhmer Rouge\n0.2223014\nKhmer Issarak\n0.2153606\n\n\n\n(Wars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  arrange(desc(close))%>%\n  slice(0:5))[,c(1,6)] %>%kable()\n\n\nname\nclose\nIsrael\n0.1662215\nUnited States\n0.1533667\nChina\n0.1494992\nFrance\n0.1447412\nPathet Lao\n0.1378130\n\n\n\n(Wars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  arrange(desc(eigen.rc))%>%\n  slice(0:5))[,c(1,8)] %>%kable()\n\n\nname\neigen.rc\nEthiopia\n0.0801873\nFrance\n0.0627097\nSouth Africa\n0.0451530\nUnited Kingdom\n0.0303952\nUnited States\n0.0294664\n\n\n\n(Wars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  arrange(desc(eigen.dc))%>%\n  slice(0:5))[,c(1,10)] %>%kable()\n\n\nname\neigen.dc\nChina\n0.3166654\nNorth Vietnam\n0.2973257\nPathet Lao\n0.2836275\nKhmer Rouge\n0.2223014\nKhmer Issarak\n0.2153606\n\n\n\nWars_in_latter_half_of_20th_network.nodes.stat_2%>%\n  select(-name) %>% \n  gather() %>% \n  ggplot(aes(value)) +\n    geom_histogram() +\n    facet_wrap(~key, scales = \"free\")\n\n\n\n\n\n\nwars_correlation_latter_half<-Wars_in_latter_half_of_20th_network.nodes.stat_2 %>% \n  select(totdegree,indegree,outdegree,eigen,eigen.rc,eigen.dc)%>%\n  correlate() \nfashion(wars_correlation_latter_half)\n\n\n       term totdegree indegree outdegree eigen eigen.rc eigen.dc\n1 totdegree                .71       .87   .64      .55      .60\n2  indegree       .71                .28   .34      .45      .30\n3 outdegree       .87      .28             .64      .45      .62\n4     eigen       .64      .34       .64            .53      .99\n5  eigen.rc       .55      .45       .45   .53               .42\n6  eigen.dc       .60      .30       .62   .99      .42         \n\n\n\nrplot(wars_correlation_latter_half)\n\n\n\n\n\n\nlibrary(threejs)\nlibrary(htmlwidgets)\nlibrary(igraph)\n\n\n\n\n\n#net.js <- Wars_in_latter_half_of_20th.ig\n#graph_attr(net.js, \"layout\") <- NULL \n\n\n\n\n\n#gjs <- graphjs(net.js, main=\"Cold War Interactive Network\", bg=\"gray10\", vertex.size=0.5, showLabels=T, vertex.label = V(net.js)$name, stroke=F, curvature=0.1, attraction=0.9, repulsion=0.7, opacity=0.9)\n#print(gjs)\n#saveWidget(gjs, file=\"Media-Network-gjs.html\")\n#browseURL(\"Media-Network-gjs.html\")\n\n\n\n\n\n\n",
    "preview": "posts/2022-03-29-into-the-20th-century/into-the-20th-century_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-03-29T13:36:58-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-29-ml-homework-1/",
    "title": "ML Homework 1",
    "description": "Machine learning homework 1.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-03-29",
    "categories": [],
    "contents": "\nISLR Ch. 3, exercise 3\nSome collaboration with (Jason, Dana, Ben, and Larri)\n\n\n\nCh. 2 exercise 1\n(a)\nQuestion: The sample size n is extremely large, and\nthe number of predictors p is small.\nAnswer: You would expect a flexible statistical\nlearning method to perform better when the sample size is larger, this\nis because a flexible model will fit the data more closely, in addition\nestimating more parameters could risk overfitting, but because the\nnumber of predictors is small this could not be an issue so a flexible\nmodel will simply the the larger number of available point more\neffectively without as much risk of facing dimensionality constraints as\nit is likely that a small number of araeters would not make the\ncalculation of distance between points too difficult.\n(b)\nQuestion: The number of predictors p is extremely\nlarge, and the number of observations n is small.\nAnswer: If the number of predictors is extremely\nlarge then a less flexible model will perform better than a more\nflexible one. This is because a larger number of predictors for small\nnumber of observations can lead to overfitting and an excess sensitivity\ntowards points that could be outliers. In the case of a large number\npredictors a more flexible model would include more and thus because of\nthe small number of actual observations it would be difficult to\ndetermine what the smallest distance is in any dimension as described in\nthe curse of dimensionslity. In this case a less flexible model will use\nless predictors and thus wont face the difficulties in calculating the\nclosest point in a large number of dimensions.\n(c)\nQuestion: The relationship between the predictors\nand response is highly non-linear.\nAnswer: If the relationship between predictors and\nresponse is highly non-linear than a more flexible model will likely\nmake predictions in a way that is more accurate than extremely\ninflexible models like linear regression. Flexible models are generally\nnonlinear and account for more predictors than extremely inflexible\nmodels by nature. As a result a correctly defined\nflexible model could be effective at describing a non-linear\nrelationship, especially if its predictors are used and defined in a way\nthat does not overfit certain predictors over others.\n(d)\nQuestion: The variance of the error terms,\ni.e.sigma2 = Var(error), is extremely high.\nAnswer: As in previous cases, extremely flexible\nmodels risks overfitting data too closely. If the variance in error is\nquite large then there is a greater potential that in making\npredictions, a more flexible model will overfit points that vary a great\namount from the true relationship. In essence fit patterns in the\ntraining data that may not be replicated in the test data that are the\nresult of variance in the error of predictions. A dataset that has a\nlarge amount of variance will likely have large errors that flexible\nmethods will overfit to. As a result inflexible models will likely make\nbetter predictions when variance is large because the model will not fit\nerrors closely.\nCh. 2 exercise 2\nProblem: Explain whether each scenario is a\nclassification or regression problem, and indicate whether we are most\ninterested in inference or prediction. Finally, provide n and p.\n(a)\nQuestion: We collect a set of data on the top 500\nfirms in the US. For each firm we record profit, number of employees,\nindustry and the CEO salary. We are interested in understanding which\nfactors affect CEO salary.\nAnswer: This problem is a regression problem, we are\nattempting to predict variations in CEO salary given a number of inputs.\nSalary itself is a continuous measurement in dollars and does not belong\nto a particular classification group. Here we are interested in\ninference, as the problem state we want to know “which factors” affect\nCEO salary rather than predicting what a CEO’s salary is. n = 500\n(number of firms)\np = 3 (profit, number of employees, industry)\n(b)\nQuestion: We are considering launching a new product\nand wish to know whether it will be a success or a failure. We collect\ndata on 20 similar products that were previously launched. For each\nproduct we have recorded whether it was a success or failure, price\ncharged for the product, marketing budget, competition price, and ten\nother variables.\nAnswer: classification, here we are trying to\ndetermine success or failure given a number of inputs and not measuring\nany kind of continuous variable, here we want to categorize outcomes as\n1 of 2 binary outcomes either failure, or success.\nn = 20 (Similar Products)\np = 13 (price, marketing budget, competition price, 10 more)\n(c)\nQuestion: We are interested in predicting the %\nchange in the USD/Euro exchange rate in relation to the weekly changes\nin the world stock markets. Hence we collect weekly data for all of\n2012. For each week we record the % change in the USD/Euro, the % change\nin the US market, the % change in the British market, and the % change\nin the German market.\nAnswer: This is a regression problem, the exchange\nrate is observed in relation to other inputs but the rate itself is a\ncontinuous numeric value that is being measured in relation to other\nnon-categorical, continous variables.\nn = 52 (Weekly data, 52 weeks in a year) p = 3 (the % change in the\nUS market, the % change in the British market, and the % change in the\nGerman market)\nCh. 2 exercise 7\nQuestion: Describe the differences between a parametric and a\nnon-parametric statistical learning approach. What are the advantages of\na para- metric approach to regression or classification (as opposed to a\nnon- parametric approach)? What are its disadvantages? The table below\nprovides a training data set containing six observations, three\npredictors, and one qualitative response variable.\n(a)\nQuestion: Compute the Euclidean distance between\neach observation and the test point,X1 =X2 =X3 =0.\nAnswer: The test point here is X1=X2=X3=0 or (0, 0,\n0) Euclidean distance is a difference between magnitude vectors\nSqrt(sum(qi-pi)ˆ2)\nThe first is Red with distance:\n\n\nsqrt((0-0)^2+(3-0)^2+(0-0)^2) %>% kable()\n\n\nx\n3\n\nThe Second is Red with distance:\n\n\nsqrt((2-0)^2+(0-0)^2+(0-0)^2) %>% kable()\n\n\nx\n2\n\nThe Third is Red with distance:\n\n\nsqrt((0-0)^2+(1-0)^2+(3-0)^2) %>% kable()\n\n\nx\n3.162278\n\nThe fourth is Green with distance:\n\n\nsqrt((0-0)^2+(1-0)^2+(2-0)^2) %>% kable()\n\n\nx\n2.236068\n\nThe Fifth is Green with distance:\n\n\nsqrt( (-1-0)^2+(0-0)^2+(1-0)^2 ) %>% kable()\n\n\nx\n1.414214\n\nThe sixth is Red with distance:\n\n\nsqrt((1-0)^2+(1-0)^2+(1-0)^2) %>% kable()\n\n\nx\n1.732051\n\n(b)\nQuestion: What is our prediction with K = 1?\nWhy?\nAnswer: The nearest neighbor classifier is Pr( y = j\n| x = x0 ) = 1/K * sum ( yi= j ) and i in N0 so we have 1/1 * (yi = j)\nhere we look to 1 nearest neighbor in this case it is from (0, 0, 0) to\nthe nearest single point, this is when we have observation 5 as it has\nthe smallest euclidean distance at 1.414214. Because of this for 1\nnearest neighbor we would predict the point to be Green.\n(c)\nAnswer: What is our prediction with K = 3?\nWhy? Pr( y = j | x = x0 ) = 1/K * sum ( yi= j ) and i in N0 Pr( y = j\n| x = x0 ) = 1/3 * sum ( yi=j )\nand i in N0 our 3 nearest neighbors here are\n1.414214 = Green\n1.732051 = Red\n2 = Red\nso in this case we have a 2/3 probability of the point being red as\n2/3 of its nearest points are red. Finally it has a 1/3 probability of\nbeing green as 1/3 of the nearest points to (x1=0, x2=0, x3=0) is\ngreen.\n(d)\nQuestion: If the Bayes decision boundary in this\nproblem is highly nonlinear, then would we expect the best value for K\nto be large or small? Why?\nAnswer: If the decision boundary is highly nonlinear\nthan we want a more fleixable K value as an inflexible K-value will\nresult in a boundary that is more linear in appearance. Instead it would\nbe better to use a smaller K-value which determines points by a smaller\nnumber of neighbors. In this case if a boundary is further from linear\nwe would expect a more nonlinear decision making boundary to predict our\npoints better. This is because the K-value determines the number of\nlocal points used to determine the classification of a point so if the\ntrue relationship is determinant on a large number of parameters then is\nbeneficial to allow the algoithm to account for a larger amount of\nparamteres and thus variation itself.\nCh. 2 exercise 10\n(a)\nQuestion: How many rows are in this data set? How\nmany columns? What do the rows and columns represent?\n\n\nnrow(Boston) %>% kable()\n\n\nx\n506\n\nAnswer: The number of rows in the dataset is 506\n\n\nncol(Boston) %>% kable()\n\n\nx\n13\n\nAnswer:The number of columns in the dataset is\n13\nThe rows in the Boston dataset represent census tracts in the city of\nBoston. Each column is an observation for an individual census tract for\n13 different categories including crime rate, nitrus oxide\nconcentration, average age, and a number of other indicators describing\neach census tract.\n(b)\nQuestion: Make some pairwise scatterplots of the\npredictors (columns) in this data set. Describe your findings.\nAnswer: Plot 1. my first graph represents the\nrelationship between non-retail acres of industry in the census tract\nand crime rate. In this case it does not appear as if there is any\nnoticable association between the two variables as around a indus level\nof 18 there is a large variation in crime rate.\n\n\nplot( Boston$indus, Boston$crim)\n\n\n\n\nAnswer: Plot 2. The second plot below graphs crime\nrate as a function of nitrus oxide concentration in each census tract.\nHere there does appear to be some association between the two as\nincreasing the level of nox past 0.5 appears to be associated with a\nlarger variation in crime rate.\n\n\nplot(Boston$nox, Boston$crim)\n\n\n\n\nAnswer: Plot 3. The next plot compares average age\nin a census tract to the tax rate there. Here there does no appear to be\na strong association between the two variables, at least linearly,\nlooking at taxes above and below 500 does indicate that people that are,\non average, older than others tend to be associated with areas with\nhigher taxes.\n\n\nplot(Boston$age, Boston$tax)\n\n\n\n\nAnswer: Plot 4. The plot below looks at the\nassociation between nitrus oxicide concentration and average number of\nrooms per dwelling, there does not appear to be any direct association\nas there is a great deal of variation with most points centering around\n6 rooms without too much association though there are some areas with\nlow nox and many rooms and some with high nox and few rooms.\n\n\nplot(Boston$rm, Boston$nox)\n\n\n\n\nAnswer: Plot 5. The graph below plots the\nassociation between the tax rate of a town, or census tract, and the\npupil-teacher ratio there. In this case there does not appear to be a\nstrong association between the two as the points are distributed fairly\nevenly between 14-20 on the ptratio y-axis and 200-400 on the tax-rate\nx-axis.\n\n\nplot(Boston$tax, Boston$ptratio)\n\n\n\n\nAnswer: Plot 6. The final plot below looks at the\nassociation between median value of occupied homes in the 1000’s and\nlstat, or proportion of people that are lower status in the community.\nIn this case there does appear to be some negative association between\nincreasing levels or lower status and lower house prices.\n\n\nplot(Boston$lstat, Boston$medv)\n\n\n\n\n(c)\nQuestion: Are any of the predictors associated with\nper capita crime rate? If so, explain the relationship.\nAnswer: Below I have began by using the pairs\nfunction to look for association between per-capita crime rate and each\nof the other predictors. This is represented by the graphs in the first\ncolumn under “crim”\n\n\npairs(\nBoston[c(1, 2:4)], lower.panel = NULL\n)\n\n\n\n\nIt appears as if none of the variables in this fist pairs() plot are\nassociated with crime rate.\n\n\npairs(\nBoston[c(1, 5:7)], lower.panel = NULL\n)\n\n\n\n\nIn this second set of graphs there does appear to be an association\nin some way with nox, but it is difficult to distinguish a relationship\nfor the other variables.\n\n\npairs(\nBoston[c(1, 8:10)], lower.panel = NULL\n)\n\n\n\n\nIn the 3rd set of graphs it only appears that dis has an association\nwith crime rate.\n\n\npairs(\nBoston[c(1, 11:13)], lower.panel = NULL\n)\n\n\n\n\nIn the final graph indicates an association between lstat and medv\nand crime. From the appearance of these graphs it would seem that: nox\nin the second panel’s graphs dis in the third panel’s graphs medv, stat\nin the fourth panel’s graphs\n\n\nplot(Boston$nox, Boston$crim)\n\n\n\n\n\n\nplot(Boston$dis, Boston$crim)\n\n\n\n\n\n\nplot(Boston$medv, Boston$crim)\n\n\n\n\n\n\nplot(Boston$lstat, Boston$crim)\n\n\n\n\nAnswer: The relationship between nox and crime\nappears to indicate that there begins to be an association between\nincreased nox and crime above 0.5 of nox. The relationship between dis\nand crim appears to be that crime is much higher, on average closer to\nemployment centers and gradually falls as the town is further away.\nThe relationship between medv and crim appears to be that as medv, or\nmedian value of occupied homes increases there appears to be an\nassociation with decreasing crime rates.\nFinally the association between lstat and crim indicates that as\nlstat or percent of lower status people increaes the crime rate\nincreases as well.\n(d)\nQuestion: Do any of the census tracts of Boston\nappear to have particularly high crime rates? Tax rates? Pupil- teacher\nratios? Comment on the range of each predictor.\n\n\nhist(Boston$crim, breaks=75, probability = TRUE)\n\n\n\n\n\n\nquantile(Boston$crim)\n\n\n       0%       25%       50%       75%      100% \n 0.006320  0.082045  0.256510  3.677083 88.976200 \n\n\n\ncount(Boston[Boston$crim > 3.677083,])\n\n\n    n\n1 127\n\nBased on the information in the first code chunk there do appear to\nbe some tracts with particularly high crime rates. In this case the\nhistogram indicates that most area have low crime but that there are\nsome areas with far higher rates. To find out how many area had quite\nhigh crime rates I looked at observations in crime over the 75th\nquantile and found 127 census tracts with larger amounts of crime than\n75 quaniles of the data.\n\n\nhist(Boston$tax, breaks=50, probability = TRUE)\n\n\n\n\n\n\nquantile(Boston$tax)\n\n\n  0%  25%  50%  75% 100% \n 187  279  330  666  711 \n\n\n\ncount(Boston[Boston$tax > 666,])\n\n\n  n\n1 5\n\nBased on the information in the above code chunk there do appear to\nbe some census tracts with an exceptionally high tax rate. As seen in\nthe histogram property taxes for many towns are between 200 and 550,\nhowever a larger number of census tracts have higher property taxes.\nAgain I looked for how many areas were above the 75th quantile of the\ndata and found that only 5 town had a propoerty tax over 666.\n\n\nhist(Boston$ptratio, breaks=75, probability = TRUE)\n\n\n\n\n\n\nquantile(Boston$ptratio) %>% kable()\n\n\n\nx\n0%\n12.60\n25%\n17.40\n50%\n19.05\n75%\n20.20\n100%\n22.00\n\n\n\ncount(Boston[Boston$ptratio > 20.20,]) %>% kable()\n\n\nn\n56\n\nFinally I looked at the number of census tracts with exceptionally\nhigh pupil to student ratios, there appear to be a number of towns with\nhigher ratios, in this case there were 56 census tracts with over the\n75th quantile of students per-teacher.\n(e)\nQuestion:How many of the census tracts in this data\nset bound the Charles river?\nAnswer: Based on the code below it appears that 35\ncensus tracts in the dataset bound the charles river as the “chas”\ncolumn indicates with ‘1’ being that the area does bound the charles and\n‘0’ being that it does not.\n\n\ncount(Boston[Boston$chas == 1,]) %>% kable()\n\n\nn\n35\n\n(f)\nQuestion: What is the median pupil-teacher ratio\namong the towns in this data set?\nAnswer: The median pupul-teacher ratio, indicated by\nthe “ptratio” column of the dataset is calculated below as 19.05.\n\n\nmedian(Boston$ptratio) %>% kable()\n\n\nx\n19.05\n\n(g)\nQuestion Which census tract of Boston has lowest\nmedian value of owner occupied homes? What are the values of the other\npredictors for that census tract, and how do those values compare to the\noverall ranges for those predictors? Comment on your findings.\n\n\ncolrange <-function(x){ sapply(x,range)\n} \ncolrange(Boston) %>% kable()\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nlstat\nmedv\n0.00632\n0\n0.46\n0\n0.385\n3.561\n2.9\n1.1296\n1\n187\n12.6\n1.73\n5\n88.97620\n100\n27.74\n1\n0.871\n8.780\n100.0\n12.1265\n24\n711\n22.0\n37.97\n50\n\n\n\n(Boston[order(Boston$medv),])[1,] %>% kable()\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nlstat\nmedv\n399\n38.3518\n0\n18.1\n0\n0.693\n5.453\n100\n1.4896\n24\n666\n20.2\n30.59\n5\n\n\n\nsummary(Boston[399,]) %>% kable()\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nlstat\nmedv\n\nMin. :38.35\nMin. :0\nMin. :18.1\nMin. :0\nMin. :0.693\nMin. :5.453\nMin. :100\nMin. :1.49\nMin. :24\nMin. :666\nMin. :20.2\nMin. :30.59\nMin. :5\n\n1st Qu.:38.35\n1st Qu.:0\n1st Qu.:18.1\n1st Qu.:0\n1st Qu.:0.693\n1st Qu.:5.453\n1st Qu.:100\n1st Qu.:1.49\n1st Qu.:24\n1st Qu.:666\n1st Qu.:20.2\n1st Qu.:30.59\n1st Qu.:5\n\nMedian :38.35\nMedian :0\nMedian :18.1\nMedian :0\nMedian :0.693\nMedian :5.453\nMedian :100\nMedian :1.49\nMedian :24\nMedian :666\nMedian :20.2\nMedian :30.59\nMedian :5\n\nMean :38.35\nMean :0\nMean :18.1\nMean :0\nMean :0.693\nMean :5.453\nMean :100\nMean :1.49\nMean :24\nMean :666\nMean :20.2\nMean :30.59\nMean :5\n\n3rd Qu.:38.35\n3rd Qu.:0\n3rd Qu.:18.1\n3rd Qu.:0\n3rd Qu.:0.693\n3rd Qu.:5.453\n3rd Qu.:100\n3rd Qu.:1.49\n3rd Qu.:24\n3rd Qu.:666\n3rd Qu.:20.2\n3rd Qu.:30.59\n3rd Qu.:5\n\nMax. :38.35\nMax. :0\nMax. :18.1\nMax. :0\nMax. :0.693\nMax. :5.453\nMax. :100\nMax. :1.49\nMax. :24\nMax. :666\nMax. :20.2\nMax. :30.59\nMax. :5\n\nAnswer: Row 399, or census tract 399 has the lowest\nmedian household price.\nAnswer continued: For each predictor we compare the\n5th rows observation with the average from the boston dataset,\nThe tract has a crim of 38.3518 which is much larger than the average\nof 3.61352 which indicates a higher than average crime rate.\nthe tract has a zn of 0 which is less than the average of 11.36\nindicating a lower than average proportion of land zoned for lots over\n25,000 feet with there being none at all.\nThe tract has an indus of 18.1 which is more than the average of\n11.14, which indicates that the town has a higher than average\nproportion of non-retail business.\nThe tract has a chas of 0 which is less than the mean of 0.06917,\nthis indicates that the town does not abut the Charles river.\nThe tract has a nox 0.693 which is higher than the mean of 0.5547.\nThis indicates that the tract has a higher than average concentration of\nnitrogen oxides.\nThe tract has a rm of 5.453 which is less than the average of 6.285.\nThis means the tract has a lower than average number of rooms.\nThe tract has an age of 100 which is less than the average of 68.57,\nwhich would indicate that the average in census tract 399 is miuch\nhigher than the average.\nThe tract has a dis of 1.4896 which is less than the average of 3.795\nwhich indicates a lower than average distance to 5 Boston employment\ncenters.\nThe tract has a rad of 24 which is more than the average of 9.549, so\nhigher highway accessibility.\nThe tract has a tax of 666 which is higher than the average 408.2\nindicating higher property tax rates.\nThe ptratio is 20.2 which is very slightly larger than the average of\n18.46 which indicates a larger than average number of pupils per teacher\ncompared to the average.\nThe tract has an lstat of 30.59 which is far more than the average of\n12.65 indicating a larger proportion of the population of the tract\nbeing lower status.\nFinally the tract has a medv of 5 which is higher than the average of\n22.53 which indicates that this is indeed an area with lower median home\nvalues.\n(h)\nQuestion: In this data set, how many of the census\ntracts average more than seven rooms per dwelling? More than eight rooms\nper dwelling? Comment on the census tracts that average more than eight\nrooms per dwelling.\n\n\nnrow(Boston[Boston$rm > 7, ]) %>% kable()\n\n\nx\n64\n\n64 census tracts have over 7 rooms per dwelling.\n\n\nnrow(Boston[Boston$rm > 8, ]) %>% kable()\n\n\nx\n13\n\nc(as.numeric(rownames(Boston[Boston$rm > 8, ]))) %>% kable()\n\n\nx\n98\n164\n205\n225\n226\n227\n233\n234\n254\n258\n263\n268\n365\n\n13 census tracts have over 8 rooms per dwelling\n\n\nBoston_over_8_rooms <- subset(Boston, rm>8) \n   \nsummary(Boston_over_8_rooms) %>% kable()\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nlstat\nmedv\n\nMin. :0.02009\nMin. : 0.00\nMin. : 2.680\nMin. :0.0000\nMin. :0.4161\nMin. :8.034\nMin. : 8.40\nMin. :1.801\nMin. : 2.000\nMin. :224.0\nMin. :13.00\nMin. :2.47\nMin. :21.9\n\n1st Qu.:0.33147\n1st Qu.: 0.00\n1st Qu.: 3.970\n1st Qu.:0.0000\n1st Qu.:0.5040\n1st Qu.:8.247\n1st Qu.:70.40\n1st Qu.:2.288\n1st Qu.: 5.000\n1st Qu.:264.0\n1st Qu.:14.70\n1st Qu.:3.32\n1st Qu.:41.7\n\nMedian :0.52014\nMedian : 0.00\nMedian : 6.200\nMedian :0.0000\nMedian :0.5070\nMedian :8.297\nMedian :78.30\nMedian :2.894\nMedian : 7.000\nMedian :307.0\nMedian :17.40\nMedian :4.14\nMedian :48.3\n\nMean :0.71879\nMean :13.62\nMean : 7.078\nMean :0.1538\nMean :0.5392\nMean :8.349\nMean :71.54\nMean :3.430\nMean : 7.462\nMean :325.1\nMean :16.36\nMean :4.31\nMean :44.2\n\n3rd Qu.:0.57834\n3rd Qu.:20.00\n3rd Qu.: 6.200\n3rd Qu.:0.0000\n3rd Qu.:0.6050\n3rd Qu.:8.398\n3rd Qu.:86.50\n3rd Qu.:3.652\n3rd Qu.: 8.000\n3rd Qu.:307.0\n3rd Qu.:17.40\n3rd Qu.:5.12\n3rd Qu.:50.0\n\nMax. :3.47428\nMax. :95.00\nMax. :19.580\nMax. :1.0000\nMax. :0.7180\nMax. :8.780\nMax. :93.90\nMax. :8.907\nMax. :24.000\nMax. :666.0\nMax. :20.20\nMax. :7.44\nMax. :50.0\n\n\n\nsummary(Boston) %>% kable()\n\n\n\ncrim\nzn\nindus\nchas\nnox\nrm\nage\ndis\nrad\ntax\nptratio\nlstat\nmedv\n\nMin. : 0.00632\nMin. : 0.00\nMin. : 0.46\nMin. :0.00000\nMin. :0.3850\nMin. :3.561\nMin. : 2.90\nMin. : 1.130\nMin. : 1.000\nMin. :187.0\nMin. :12.60\nMin. : 1.73\nMin. : 5.00\n\n1st Qu.: 0.08205\n1st Qu.: 0.00\n1st Qu.: 5.19\n1st Qu.:0.00000\n1st Qu.:0.4490\n1st Qu.:5.886\n1st Qu.: 45.02\n1st Qu.: 2.100\n1st Qu.: 4.000\n1st Qu.:279.0\n1st Qu.:17.40\n1st Qu.: 6.95\n1st Qu.:17.02\n\nMedian : 0.25651\nMedian : 0.00\nMedian : 9.69\nMedian :0.00000\nMedian :0.5380\nMedian :6.208\nMedian : 77.50\nMedian : 3.207\nMedian : 5.000\nMedian :330.0\nMedian :19.05\nMedian :11.36\nMedian :21.20\n\nMean : 3.61352\nMean : 11.36\nMean :11.14\nMean :0.06917\nMean :0.5547\nMean :6.285\nMean : 68.57\nMean : 3.795\nMean : 9.549\nMean :408.2\nMean :18.46\nMean :12.65\nMean :22.53\n\n3rd Qu.: 3.67708\n3rd Qu.: 12.50\n3rd Qu.:18.10\n3rd Qu.:0.00000\n3rd Qu.:0.6240\n3rd Qu.:6.623\n3rd Qu.: 94.08\n3rd Qu.: 5.188\n3rd Qu.:24.000\n3rd Qu.:666.0\n3rd Qu.:20.20\n3rd Qu.:16.95\n3rd Qu.:25.00\n\nMax. :88.97620\nMax. :100.00\nMax. :27.74\nMax. :1.00000\nMax. :0.8710\nMax. :8.780\nMax. :100.00\nMax. :12.127\nMax. :24.000\nMax. :711.0\nMax. :22.00\nMax. :37.97\nMax. :50.00\n\nComparing the means from each category we have crim has an average of\n3.61352 while with over 8 dwellings it is 0.71879 indicating that these\ncensus tracts have a lower crime rate on average.\nzn, 13.62 for 8 compared to 11.36 on average so slightly more than\naverage amount of land zoned for lots over 25,000 feet.\nindus, 7.078 for 8 bedroom compared to an overall average of 11.14\nindicating that tracts with over 8 bedrooms have a lower proportion of\nretail business than the average.\nchas, 0.1538 for 8 bedrooms compared to an overall average of 0.06917\nwhich indicates that the tracts are, on average, less likely to abut the\ncharles river.\nnox, 0.5392 for the tract with 8 bedrooms and 0.5547 on average\nindicating a similar to average level of nitrogen oxides.\nage, is 71.54 on average in these tracts compared to 68.57 elsewhere\nwhich indicates a slightly higher than average age.\ndis of 3.430 for 8 bedrooms compared to 3.795 indicating a slightly\nlower than average distance to employment centers.\nrad, 7.462 for 8 bedrooms compared to 9.549 on average indicating\nthat highways are more accessible than average.\ntax, 325.1 for 8 bedrooms compared to the 408.2 of the average which\nwould suggest that the tracts have lower average property taces\nptratio, The pupil teacher ratio is 16.36 which is lower than the\naverage of 18.46.\nlstat, For over 8 bedrooms the proportion of lower status population\nis much lower than average being 4.31 as compared to 12.65.\nmedv, or the median value of occupied homes is essentially double\nthat of the average suggesting a much higher than average median home\nprice of 44.2 compared to 22.53.\nCh. 3 exercise 1\nQuestion: Describe the null hypotheses to which the\np-values given in Table 3.4 correspond. Explain what conclusions you can\ndraw based on these p-values. Your explanation should be phrased in\nterms of sales, TV, radio, and newspaper, rather than in terms of the\ncoefficients of the linear model.\nAnswer: The null hypothesis in this case would be\nthat TV, radio, and newspaper advertisements do not have an effects on\nsales for the product. This null hypothesis would be that for our\nequation: Yhat = Beta0 + Beta1 * x1 + Beta2 * x2 + Beta3 * x3, Here our\nnull hypothesis would be that Beta1 + Beta2 + Beta3 are all zero so that\nthey do not have an effect on our dependent variable.\nCh. 3 exercise 3\nQuestion: Suppose we have a data set with five\npredictors, X1 = GPA, X2 = IQ, X3 = Level (1 for College and 0 for High\nSchool), X4 = Interaction between GPA and IQ, and X5 = Interaction\nbetween GPA and Level. The response is starting salary after graduation\n(in thousands of dollars). Suppose we use least squares to fit the\nmodel, and get B^0 = 50,B^1 = 20,B^2 = 0.07,B^3 = 35,B^4 = 0.01,B^5 =\n-10. Salary\n(a)\nQuestion: Which answer is correct, and why?\n(i)For a fixed value of IQ and GPA, high school\ngraduates earn more, on average, than college graduates.\n(ii)For a fixed value of IQ and GPA, college graduates\nearn more, on average, than high school graduates.\n(iii)For a fixed value of IQ and GPA, high school\ngraduates earn more, on average, than college graduates provided that\nthe GPA is high enough. This is (iv)For a fixed value\nof IQ and GPA, college graduates earn more, on average, than high school\ngraduates provided that the GPA is high enough.\nAnswer: Using the Equation we see that iii is\ncorrect..\nSalary = 50 + 20 * GPA + 0.07 * IQ + 35 * level + 0.01 * GPA * IQ -\n10 * GPA * level\nAnswer: This is false, as can be seen from the equation above,\nholding all values fixed at their base level except for IQ and GPA our\nequation, and thus our salary prediction depends on 2 terms\nHere I will hold hold non-graduates at their value of 0 and graduates\nat their value of 1.\nHS Salary = 50 + 20 * GPA + 0.07 * IQ + 35 * 0 + 0.01 * GPA * IQ - 10\n* GPA * 0\nthis becomes 50 + 20 * GPA + 0.07 * IQ + 0.01 * GPA * IQ\nCollege Salary = 50 + 20 * GPA + 0.07 * IQ + 35 * 1 + 0.01 * GPA * IQ\n- 10 * GPA * 1\nthis becomes 50 + 10 * GPA + 0.07 * IQ + 35\nor 85 + 10 * GPA + 0.07 * IQ\nEquating the two we see\n50 + 20 * GPA + 0.07 * IQ + 0.01 * GPA * IQ = 85 + 10 * GPA + 0.07 *\nIQ + 0.01 * GPA *IQ\nhigh School( 50 + 20 * GPA ) = college( 85 + 10 * GPA )\nSo in this case if we fix GPA and IQ that high students earn more\nthan college students provided they have a high enough GPA.\n(b)\nQuestion:Predict the salary of a college graduate\nwith IQ of 110 and a GPA of 4.0.\nAnswer:My answer is as follows based on the original\nequation:\n50 + 20 * GPA + 0.07 * IQ + 35 * 1 + 0.01 * GPA * IQ - 10 * GPA *\n1\n\n\nGPA<-4\n\nIQ<-110\n\n(50 + 20 * GPA + 0.07 * IQ + 35 * 1 + 0.01 * GPA * IQ - 10 * GPA * 1) %>% kable()\n\n\nx\n137.1\n\nAs can be seen above we would predict a salary of 137.1 or $137,100\ndollars for a college graduate with a 4.0 and IQ of 110.\n(c)\nQuestion: True or false: Since the coefficient for\nthe GPA/IQ interaction term is very small, there is very little evidence\nof an interaction effect. Justify your answer.\nFalse, in this case there could very well be evidence of an\ninteraction effect between the two variables, though we are not provided\nwith the information the standard errors of the interaction term could\nbe quite small and the p-value could suggest that it is non-zero, it may\nbe correctly estimated but just be quite a small number. In addition it\ntakes the product of IQ which averages near 100 and GPA which (in some\nschools) could be expected to be around 3.0\nas a result the interaction can still have a non-negligible effect on\nsalary estimates simply due to the magnitude of the observations it\nrepresents the interaction between.\n\n\n(0.01 * 100 * 3) %>% kable()\n\n\nx\n3\n\nAs can be seen above the interaction adds an addition predicted $3000\ndollars of salary.\nCh. 3 exercise 15\nQuestion: This problem involves the Boston data set,\nwhich we saw in the lab for this chapter. We will now try to predict\nper-capita crime rate using the other variables in this data set. In\nother words, per-capita crime rate is the response, and the other\nvariables are the predictors.\n(a)\nQuestion: For each predictor, fit a simple linear\nregression model to predict the response. Describe your results. In\nwhich of the models is there a statistically significant association\nbetween the predictor and the response? Create some plots to back up\nyour assertions.\n\n\nfit.zn <- lm(crim ~ zn, Boston) \n\nsummary(fit.zn)\n\n\n\nCall:\nlm(formula = crim ~ zn, data = Boston)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.429 -4.222 -2.620  1.250 84.523 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.45369    0.41722  10.675  < 2e-16 ***\nzn          -0.07393    0.01609  -4.594 5.51e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.435 on 504 degrees of freedom\nMultiple R-squared:  0.04019,   Adjusted R-squared:  0.03828 \nF-statistic:  21.1 on 1 and 504 DF,  p-value: 5.506e-06\n\nplot(Boston$zn, Boston$crim) \n\nabline(coef=coef(fit.zn), col=\"red\")\n\n\n\n\n\n\nfit.indus <- lm(crim ~ indus, Boston) \n\nsummary(fit.indus)\n\n\n\nCall:\nlm(formula = crim ~ indus, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.972  -2.698  -0.736   0.712  81.813 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.06374    0.66723  -3.093  0.00209 ** \nindus        0.50978    0.05102   9.991  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.866 on 504 degrees of freedom\nMultiple R-squared:  0.1653,    Adjusted R-squared:  0.1637 \nF-statistic: 99.82 on 1 and 504 DF,  p-value: < 2.2e-16\n\nplot(Boston$indus, Boston$crim)\n\nabline(coef=coef(fit.indus), col=\"red\")\n\n\n\n\n\n\nfit.chas <- lm(crim ~ chas, Boston) \n\nsummary(fit.chas)\n\n\n\nCall:\nlm(formula = crim ~ chas, data = Boston)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.738 -3.661 -3.435  0.018 85.232 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.7444     0.3961   9.453   <2e-16 ***\nchas         -1.8928     1.5061  -1.257    0.209    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.597 on 504 degrees of freedom\nMultiple R-squared:  0.003124,  Adjusted R-squared:  0.001146 \nF-statistic: 1.579 on 1 and 504 DF,  p-value: 0.2094\n\nplot(Boston$chas, Boston$crim) \n\nabline(coef=coef(fit.chas), col=\"red\")\n\n\n\n\n\n\nfit.nox <- lm(crim ~ nox, Boston) \n\nsummary(fit.nox)\n\n\n\nCall:\nlm(formula = crim ~ nox, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.371  -2.738  -0.974   0.559  81.728 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -13.720      1.699  -8.073 5.08e-15 ***\nnox           31.249      2.999  10.419  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.81 on 504 degrees of freedom\nMultiple R-squared:  0.1772,    Adjusted R-squared:  0.1756 \nF-statistic: 108.6 on 1 and 504 DF,  p-value: < 2.2e-16\n\nplot(Boston$nox, Boston$crim) \n\nabline(coef=coef(fit.nox), col=\"red\")\n\n\n\n\n\n\nfit.rm <- lm(crim ~ rm, Boston) \n\nsummary(fit.rm)\n\n\n\nCall:\nlm(formula = crim ~ rm, data = Boston)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.604 -3.952 -2.654  0.989 87.197 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   20.482      3.365   6.088 2.27e-09 ***\nrm            -2.684      0.532  -5.045 6.35e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.401 on 504 degrees of freedom\nMultiple R-squared:  0.04807,   Adjusted R-squared:  0.04618 \nF-statistic: 25.45 on 1 and 504 DF,  p-value: 6.347e-07\n\nplot(Boston$rm, Boston$crim) \n\nabline(coef=coef(fit.rm), col=\"red\")\n\n\n\n\n\n\nfit.age <- lm(crim ~ age, Boston) \n\nsummary(fit.age)\n\n\n\nCall:\nlm(formula = crim ~ age, data = Boston)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.789 -4.257 -1.230  1.527 82.849 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.77791    0.94398  -4.002 7.22e-05 ***\nage          0.10779    0.01274   8.463 2.85e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.057 on 504 degrees of freedom\nMultiple R-squared:  0.1244,    Adjusted R-squared:  0.1227 \nF-statistic: 71.62 on 1 and 504 DF,  p-value: 2.855e-16\n\nplot(Boston$age, Boston$crim) \n\nabline(coef=coef(fit.age), col=\"red\")\n\n\n\n\n\n\nfit.dis <- lm(crim ~ dis, Boston) \n\nsummary(fit.dis)\n\n\n\nCall:\nlm(formula = crim ~ dis, data = Boston)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.708 -4.134 -1.527  1.516 81.674 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   9.4993     0.7304  13.006   <2e-16 ***\ndis          -1.5509     0.1683  -9.213   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.965 on 504 degrees of freedom\nMultiple R-squared:  0.1441,    Adjusted R-squared:  0.1425 \nF-statistic: 84.89 on 1 and 504 DF,  p-value: < 2.2e-16\n\nplot(Boston$dis, Boston$crim) \n\nabline(coef=coef(fit.dis), col=\"red\")\n\n\n\n\n\n\nfit.rad <- lm(crim ~ rad, Boston) \n\nsummary(fit.rad)\n\n\n\nCall:\nlm(formula = crim ~ rad, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.164  -1.381  -0.141   0.660  76.433 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -2.28716    0.44348  -5.157 3.61e-07 ***\nrad          0.61791    0.03433  17.998  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.718 on 504 degrees of freedom\nMultiple R-squared:  0.3913,    Adjusted R-squared:   0.39 \nF-statistic: 323.9 on 1 and 504 DF,  p-value: < 2.2e-16\n\nplot(Boston$rad, Boston$crim) \n\nabline(coef=coef(fit.rad), col=\"red\")\n\n\n\n\n\n\nfit.tax <- lm(crim ~ tax, Boston) \n\nsummary(fit.tax)\n\n\n\nCall:\nlm(formula = crim ~ tax, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.513  -2.738  -0.194   1.065  77.696 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -8.528369   0.815809  -10.45   <2e-16 ***\ntax          0.029742   0.001847   16.10   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.997 on 504 degrees of freedom\nMultiple R-squared:  0.3396,    Adjusted R-squared:  0.3383 \nF-statistic: 259.2 on 1 and 504 DF,  p-value: < 2.2e-16\n\nplot(Boston$tax, Boston$crim) \n\nabline(coef=coef(fit.tax), col=\"red\")\n\n\n\n\n\n\nfit.ptratio <- lm(crim ~ ptratio, Boston) \n\nsummary(fit.ptratio)\n\n\n\nCall:\nlm(formula = crim ~ ptratio, data = Boston)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.654 -3.985 -1.912  1.825 83.353 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -17.6469     3.1473  -5.607 3.40e-08 ***\nptratio       1.1520     0.1694   6.801 2.94e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.24 on 504 degrees of freedom\nMultiple R-squared:  0.08407,   Adjusted R-squared:  0.08225 \nF-statistic: 46.26 on 1 and 504 DF,  p-value: 2.943e-11\n\nplot(Boston$ptratio, Boston$crim) \n\nabline(coef=coef(fit.ptratio), col=\"red\")\n\n\n\n\n\n\nfit.lstat <- lm(crim ~ lstat, Boston) \n\nsummary(fit.lstat)\n\n\n\nCall:\nlm(formula = crim ~ lstat, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-13.925  -2.822  -0.664   1.079  82.862 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.33054    0.69376  -4.801 2.09e-06 ***\nlstat        0.54880    0.04776  11.491  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.664 on 504 degrees of freedom\nMultiple R-squared:  0.2076,    Adjusted R-squared:  0.206 \nF-statistic:   132 on 1 and 504 DF,  p-value: < 2.2e-16\n\nplot(Boston$lstat, Boston$crim) \n\nabline(coef=coef(fit.lstat), col=\"red\")\n\n\n\n\n\n\nfit.medv <- lm(crim ~ medv, Boston)\n\nsummary(fit.medv)\n\n\n\nCall:\nlm(formula = crim ~ medv, data = Boston)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.071 -4.022 -2.343  1.298 80.957 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 11.79654    0.93419   12.63   <2e-16 ***\nmedv        -0.36316    0.03839   -9.46   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.934 on 504 degrees of freedom\nMultiple R-squared:  0.1508,    Adjusted R-squared:  0.1491 \nF-statistic: 89.49 on 1 and 504 DF,  p-value: < 2.2e-16\n\nplot(Boston$medv, Boston$crim) \n\nabline(coef=coef(fit.medv), col=\"red\")\n\n\n\n\nEvery predictor has a statistically significant association with\ncrime rate, except chas according to each’s summaries, however, the\ngraphs indicate that there are fewer associations in the data. The\ngraphs that appear to have some association are: nox, rm, age, dis,\nlstat, and medv\n(b)\nQuestion: Fit a multiple regression model to predict\nthe response using all of the predictors. Describe your results. For\nwhich predictors can we reject the null hypothesis H0 : Bj = 0\n\n\nfit.all <- lm(crim ~ zn + indus + chas + nox+ rm +age + dis + rad + tax + ptratio + lstat, Boston)\n\nsummary(fit.all) \n\n\n\nCall:\nlm(formula = crim ~ zn + indus + chas + nox + rm + age + dis + \n    rad + tax + ptratio + lstat, data = Boston)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.186 -2.048 -0.286  1.019 76.368 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.747054   6.726314   0.706   0.4807    \nzn           0.036346   0.018852   1.928   0.0544 .  \nindus       -0.062997   0.084680  -0.744   0.4573    \nchas        -1.490141   1.184255  -1.258   0.2089    \nnox         -5.989765   5.243945  -1.142   0.2539    \nrm          -0.180914   0.572897  -0.316   0.7523    \nage         -0.001688   0.018173  -0.093   0.9260    \ndis         -0.702975   0.273063  -2.574   0.0103 *  \nrad          0.563842   0.087623   6.435 2.93e-10 ***\ntax         -0.001012   0.005182  -0.195   0.8453    \nptratio     -0.100446   0.180191  -0.557   0.5775    \nlstat        0.267419   0.068011   3.932 9.63e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.542 on 494 degrees of freedom\nMultiple R-squared:  0.4342,    Adjusted R-squared:  0.4216 \nF-statistic: 34.47 on 11 and 494 DF,  p-value: < 2.2e-16\n\nAnswer: In this case we cannot reject the null\nhypothesis that H0 : Bj = 0 as the p-values for all coefficients but\nmedv,dis, rad, and zn are within the rejection region for our p-value so\nwe cannot conhcldue that any of our coefficients are nonzero.\n(c)\nQuestion: How do your results from (a) compare to\nyour results from (b)? Create a plot displaying the univariate\nregression coefficients from (a) on the x-axis, and the multiple\nregression coefficients from (b) on the y-axis. That is, each predictor\nis displayed as a single point in the plot. Its coefficient in a simple\nlinear regression model is shown on the x-axis, and its coefficient\nestimate in the multiple linear regression model is shown on the\ny-axis.\n\n\ncoefficients_for_single_variate <- vector(\"numeric\",0)\n\ncoefficients_for_single_variate <- c(as.numeric(fit.zn$coef[2]), as.numeric(fit.tax$coef[2]), as.numeric(fit.rm$coef[2]), as.numeric(fit.rad$coef[2]), as.numeric(fit.ptratio$coef[2]), as.numeric(fit.nox$coef[2]), as.numeric(fit.medv$coef[2]), as.numeric(fit.lstat$coef[2]), as.numeric(fit.indus$coef[2]), as.numeric(fit.dis$coef[2]), as.numeric(fit.chas$coef[2]), as.numeric(fit.age$coef[2]))\n\nall_coefficients <- vector(\"numeric\",0)\n\nall_coefficients <- c(as.numeric(fit.all$coef))\n\nplot(coefficients_for_single_variate, all_coefficients, col= \"blue\")\n\n\n\n\n(d)\nQuestion: Is there evidence of non-linear\nassociation between any of the predictors and the response? To answer\nthis question, for each predictor X, fit a model of the form Y =B0\n+B1X+B2X2 +B3X3 +error.\nAnswer: There does appear to be evidence for a\nnonlinear relationship between the variables, this is because the\ncoefficient for the average score of the multivariate appears to be less\nvariable and in some cases, less erroneous than each factor\nindividually. Based on the output from the previous fit with all\npredictors 4 had p-values that suggested they would likely be\nnonzero,\n\n\nfit.multiple_d <- lm(crim ~ zn + dis + rad + lstat, Boston)\nsummary(fit.multiple_d)\n\n\n\nCall:\nlm(formula = crim ~ zn + dis + rad + lstat, data = Boston)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.656 -1.922 -0.253  1.040 76.445 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -3.22126    1.16334  -2.769  0.00583 ** \nzn           0.03954    0.01686   2.345  0.01939 *  \ndis         -0.39847    0.20579  -1.936  0.05340 .  \nrad          0.50627    0.04068  12.445  < 2e-16 ***\nlstat        0.24208    0.05007   4.834 1.78e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.533 on 501 degrees of freedom\nMultiple R-squared:  0.4277,    Adjusted R-squared:  0.4231 \nF-statistic:  93.6 on 4 and 501 DF,  p-value: < 2.2e-16\n\nAnswer continued:This regression indicates that the\ncoefficients from the regression with all predictors that were unlikely\nto be zero are still likely non-zero in this equation with only 4 of\nthem. Here the form Y =B0 +B1X+B2X2 +B3X3 +error results in a number of\nlikely nonzero coefficients whose standard errors are smaller than their\ncoefficients but sufficient to change the sign of the coefficient. As a\nresult there is some evidence to suggest that a multivariate approach\ncould exhibit a stronger association that fits the form above.\nCh. 4, exercise 4\nQuestion: When the number of features p is large,\nthere tends to be a deterioration in the performance of KNN and other\nlocal approaches that\n(a)\nQuestion: Suppose that we have a set of\nobservations, each with measurements on p = 1 feature, X. We assume that\nX is uniformly (evenly) distributed on [0, 1]. Associated with each\nobservation is a response value. Suppose that we wish to predict a test\nobservation’s response using only observations that are within 10 % of\nthe range of X closest to that test observation. For instance, in order\nto predict the response for a test observation with X = 0.6, we will use\nobservations in the range [0.55,0.65]. On average, what fraction of the\navailable observations will we use to make the prediction?\nAnswer: The answer for this problem is 10% since we\nhave a even distribution from 0 to 1. This would result in a graph\nsimilar to the one below, as can be seen taking a range from 0.55 to\n0.65 or 0.1 around an observation of x=0.6 makes 10% of our data\navailible from our original uniform distribution from 0 to 1.\n\n\n#define x-axis\nx <- seq(-0.5, 1.5, length=100)\n\n#calculate uniform distribution probabilities\ny <- dunif(x, min = 0, max = 1)\ny_2 <- dunif(x, min = 0.55, max = 0.65)\n\n#plot uniform distribution\nplot(x, y, type = 'l')\npar(new=TRUE)\nplot(x, y_2, type = 'l', col=\"red\")\n\n\n\n\n(b)\nQuestion: Now suppose that we have a set of\nobservations, each with measurements on p = 2 features, X1 and X2. We\nassume that (X1, X2) are uniformly distributed on [0, 1] × [0, 1]. We\nwish to predict a test observation’s response using only observations\nthat are within 10 % of the range of X1 and within 10 % of the range of\nX2 closest to that test observation. For instance, in order to predict\nthe response for a test observation with X1 = 0.6 and X2 = 0.35, we will\nuse observations in the range [0.55, 0.65] for X1 and in the range\n[0.3,0.4] for X2. On average, what fraction of the available\nobservations will we use to make the prediction?\nAnswer: The answer for this problem is 1%. This is\nbecause we have data that is uniformly distributed in 3 dimensions so\nall the possible measurements are contained in a 1x1 cube. As a result\nof this taking a range of [0.55, 0.65] in the x1 dimension, which is 10%\nof the range in that direction and [0.3,0.4] in the x2 dimension only\nallows for the use 10% of the observations in either dimension and\naccounting for both 1% cumulatively as we will have 10% of 10% of the\nobservations available.\n(c)\nQuestion: Now suppose that we have a set of\nobservations on p = 100 features. Again the observations are uniformly\ndistributed on each feature, and again each feature ranges in value from\n0 to 1. We wish to predict a test observation’s response using\nobservations within the 10 % of each feature’s range that is closest to\nthat test observation. What fraction of the available observations will\nwe use to make the prediction?\nAnswer: The answer for this problem is 0.1^100. The\nsolution to this problem follows from the prior 2 (parts a and b) in\nthis case we have 100 features we are considering each with a uniform\ndistribution from 0 to 1. Since we can only use 10% of the observation\nin each of the 100 features we must consider our fraction of possible\nobservations as 0.1 x 0.1 x 0.1, 100 times thus we have:\n\n\n0.1^100\n\n\n[1] 1e-100\n\n(d)\nQuestion: Using your answers to parts (a)–(c), argue\nthat a drawback of KNN when p is large is that there are very few\ntraining observations “near” any given test observation.\nAnswer: The prior 3 questions increasingly small\nnumbers of available observations indicate the problem of the “curse of\ndimensionality.” The k-nearest-neighbors method faces this “curse” when\nthe number of k-dimensions of parameters is too large. In larger\ndimensions being “near” another observation becomes difficult to\ndetermine as the closeness of points is determined by their distance in\nlarge numbers of dimensions. This would indicate that observations could\nbe very close in some dimensions and farther in others. This is also a\nproblem for prediction as available data will be sparse compared to the\nnumber of dimensions being considered which requires exponential\nincreases in the number of observations for each additional dimension.\nAs a result prediction from training observations becomes more it is\ndifficult as few observations in each dimension can result in\noverfitting towards certain ones nd difficulty in finding points that\nare truly close enough in a sufficient number of dimensions to be\nreliable neighbors.\n(e)\nQuestion: Now suppose that we wish to make a\nprediction for a test observation by creating a p-dimensional hypercube\ncentered around the test observation that contains, on average, 10 % of\nthe training observations. For p = 1,2, and 100, what is the length of\neach side of the hypercube? Comment on your answer. dimensional Note: A\nhypercube is a generalization of a cube to an arbitrary number of\ndimensions. When p = 1, a hypercube is simply a line segment, when p = 2\nit is a square, and when p = 100 it is a 100-dimensional cube.\nAnswer: In this instance we are attempting to create\na p-dimensional hypercube that centers around an observation that\ncontains 10% of the data. In this case we need the volume provided by\nthe hypercube to maintain its 10% of the total observations in order to\nfulfill the requirements of the problem. As we add dimensions in part d\nwe saw that adding 100 made our available observations 0.1^-100 so for\neach dimension we have the counteract the tendency for exponential\ndecreases in available test data. In the prior instances we have:\nV=0.1^p\nIn this instance we need to keep our values of volume created by our\nedge lengths p^p.\nSo in this case to maintain our volume we need to equate our rates of\nchange for our dimension p and our volume p^p. Because of this we would\nhave to use the inverse of our previous formula in terms of growth\nrates, maintain 0.1^p we now have to consider 0.1^ (1/p) calculating\neach side length with this inverse of the rate at which our availible\nvolume is decreasing.\nCh. 4, exercise 6\nQuestion: Suppose we collect data for a group of\nstudents in a statistics class with variables X1 = hours studied, X2 =\nundergrad GPA, and Y = receive an A. We fit a logistic regression and\nproduce estimated coefficient, B^0 = -6, B^1 = 0.05, B^2 = 1.\n(a)\nQuestion: Estimate the probability that a student who studies for 40\nh and has an undergrad GPA of 3.5 gets an A in the class.\n\n\np_x_for_a <- ( exp(-6 + 0.05 * 40+ 1* 3.5)) / ( 1 +exp(-6 + 0.05 * 40+ 1* 3.5))\n\np_x_for_a %>% kable()\n\n\nx\n0.3775407\n\n(b)\nQuestion: How many hours would the student in part (a) need to study\nto have a 50 % chance of getting an A in the class?\nWe have the equation p(x) / (1-p(x)) = e^B0 + B1 * x\nso 0.5/(1-0.5) = exp(-6 + 0.05 * 40+ 1* 3.5))\n1 = exp(-6 + 0.05 * 40+ 1* 3.5))\nln(1) = ln(exp(-6 + 0.05 * 40+ 1* 3.5)))\nln(1)= -6 + 0.05 * x + 1* 3.5\n\n\n(log(1) + 6 -  3.5) %>% kable()\n\n\nx\n2.5\n\n2.5 = 0.05 * x\n\n\n(2.5/0.05) %>% kable()\n\n\nx\n50\n\nSo it must be 50\nCh. 4, exercise 9\n(a)\nQuestion: On average, what fraction of people with\nan odds of 0.37 of defaulting on their credit card payment will in fact\ndefault?\nThe logistic function for prediction is which gives us the log-odds\nof x occurring:\np(x) = ( e^B0 + B1 * x) / ( 1 + e^B0 + B1 * x)\nSince we have odds already we must use the transformed formula\n(4.3)\np(x) / (1-p(x)) = e^B0 + B1 * x\nSince we have odds here we have:\n0.37 = p(x) / (1-p(x))\nwe need p(x) in this case so to solve for it:\n0.37 - 0.37 * p(x) = p(x)\n0.37 = p(x) + 0.37 * p(x)\n0.37 = p(x) (1 + 0.37) dividing both sides 1 + 0.37 we have\n0.37/(1+0.37) = p(x)\n\n\n(0.37/(1+0.37)) %>% kable()\n\n\nx\n0.270073\n\n(b)\nQuestion: Suppose that an individual has a 16%\nchance of defaulting on her credit card payment. What are the odds that\nshe will default?\np(x) = ( e^B0 + B1 * x) / ( 1 + e^B0 + B1 * x)\nSince we have odds already we must use the transformed formula\n(4.3)\np(x) / (1-p(x)) = e^B0 + B1 * x\nso here\n\n\nodds_with_16 <- .16/(1-.16)\n\nodds_with_16 %>% kable()\n\n\nx\n0.1904762\n\n\n\n\n",
    "preview": "posts/2022-03-29-ml-homework-1/ml-homework-1_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2022-03-29T13:32:57-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-29-ml-homework-2/",
    "title": "ML Homework 2",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-03-29",
    "categories": [],
    "contents": "\n\n\n\nCh. 4, Exercise 16\nQuestion: Using the Boston data set, fit\nclassification models in order to predict whether a given census tract\nhas a crime rate above or below the median. Explore logistic regression,\nLDA, naive Bayes, and KNN models using various subsets of the\npredictors. Describe your findings.\nLooking at the Boston Data\nData Exploration and\nRevue of Homework 1\nAnswer: In homework 1 we looked at predictors for\nlinear regression individual and with all in the same equation however\nwe did not look at logistic regression. As can be seen from the\ncorrelation graph above and prior correlation calculations a handful of\npredictors seem to exhibit some correlation with crime that I will be\nusing, these will be rad, tax dis medv, and lstat.\nHere we want to predict if crime is above the median so first we will\nmake a variable that reflects this.\n\n\nmedian_crime_rate <- median(Boston$crim)\n\nmedian_crime_rate %>% kable()\n\n\nx\n0.25651\n\nAnswer Median Crime: In order to create the median\ncrime rate dummy variable I calculate the median crime rate in the\nBoston dataset and use the mutate function to create a new column with\nthis data.\n\n\nBoston_over_median <- Boston %>% mutate(median_crime_in_tract = case_when(\n  crim < 0.25651 ~ 0,\n    crim >= 0.25651 ~ 1))\n\n\n\nAnswer Pairs After this I look at pair to attempt to\ndistinguish if some variables seem particularly correlated with median\ncrime rate visually.\n\n\n\nBuilding a training and test set: Though it is\nmentioned more in later chapters the authors do use a split in their\ndata in Smarket to act as their training. In this case I will do\nsomething similar, however, I will use a sample of 70% of the data.\n\n\nset.seed(222)\n\nin_training<- sample(1:nrow(Boston_over_median),  nrow(Boston) * 0.7 )\n\ntraining_boston <- Boston_over_median[in_training,]\n\ntest_boston <- Boston_over_median[-in_training,]\n\n\n\nModel Choice Logistic 1: Firstly I will be using all\nof the variables in my logistic regression and using both p-values and\nstandard errors along with the predict function to evaluate the\nmodel\n\n\nset.seed(222)\n\nlogsitic_Boston_1<-  glm(median_crime_in_tract ~ zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + lstat + medv, data = Boston_over_median, family = binomial, subset=in_training)\n\nsummary(logsitic_Boston_1)\n\n\n\nCall:\nglm(formula = median_crime_in_tract ~ zn + indus + chas + nox + \n    rm + age + dis + rad + tax + ptratio + lstat + medv, family = binomial, \n    data = Boston_over_median, subset = in_training)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0510  -0.1337  -0.0003   0.0035   3.4664  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -48.641048   7.865389  -6.184 6.24e-10 ***\nzn           -0.092755   0.042744  -2.170  0.03000 *  \nindus        -0.074307   0.050366  -1.475  0.14012    \nchas          0.468421   0.798887   0.586  0.55765    \nnox          51.914808   9.097957   5.706 1.16e-08 ***\nrm            0.444596   0.867999   0.512  0.60851    \nage           0.014141   0.013691   1.033  0.30167    \ndis           0.784864   0.256550   3.059  0.00222 ** \nrad           0.631667   0.196382   3.217  0.00130 ** \ntax          -0.005575   0.003062  -1.821  0.06861 .  \nptratio       0.442991   0.153263   2.890  0.00385 ** \nlstat         0.141092   0.058371   2.417  0.01564 *  \nmedv          0.166757   0.086194   1.935  0.05303 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 490.75  on 353  degrees of freedom\nResidual deviance: 146.76  on 341  degrees of freedom\nAIC: 172.76\n\nNumber of Fisher Scoring iterations: 9\n\n\n\nset.seed(222)\n\nlogsitic_1_Boston_probs <- predict(logsitic_Boston_1, test_boston,\ntype = \"response\")\n\nlog_preds_1<-ifelse(logsitic_1_Boston_probs >= 0.5, 1, 0)\n\nprediction_1_logs <-mean(log_preds_1 == test_boston$median_crime_in_tract)\n\nprediction_1_logs %>% kable()\n\n\nx\n0.8618421\n\nResults Logistic 1: As can be seen above the first\nlogistic model appears to correctly evaluate whether or not a census\ntract in our test set is above or below the median correctly 0.8618421\nor 86.2% of the time.\nModel Choice Logistic 2: For my second function I\nwill be using only the variables that my glm output as statistically\nsignificant based on the glm p-values. This would suggests that most of\nthese coefficients are unlikely to be zero.\n\n\nset.seed(222)\n\nlogsitic_Boston_2 <-  glm(median_crime_in_tract ~ zn + nox + dis + rad + \n                            \nptratio + medv , data = Boston_over_median, family = binomial, \n\nsubset=in_training)\n\nsummary(logsitic_Boston_2)\n\n\n\nCall:\nglm(formula = median_crime_in_tract ~ zn + nox + dis + rad + \n    ptratio + medv, family = binomial, data = Boston_over_median, \n    subset = in_training)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.07337  -0.24571  -0.00187   0.00753   3.13797  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -35.02931    5.78928  -6.051 1.44e-09 ***\nzn           -0.07337    0.03319  -2.210 0.027089 *  \nnox          41.29154    6.64906   6.210 5.29e-10 ***\ndis           0.60332    0.21526   2.803 0.005068 ** \nrad           0.48104    0.13704   3.510 0.000448 ***\nptratio       0.32090    0.12317   2.605 0.009175 ** \nmedv          0.11077    0.03732   2.968 0.002993 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 490.75  on 353  degrees of freedom\nResidual deviance: 168.50  on 347  degrees of freedom\nAIC: 182.5\n\nNumber of Fisher Scoring iterations: 8\n\nResults Logistic 2: As can be seen above this has\nlead to a very slightly reduction in the error rate from about 86% to\nabout 89% of tracts being correctly identified.\nModel Choice Logistic 3: Next, I will remove\nvariables that changed in their significance, I will remove ones that\nwent down in significance in my third iteration and include those that\nstayed the same or improved in their “statistical significance so here\nwe just remove zn.\n\n\nset.seed(727)\n\nlogsitic_Boston_3 <-  glm(median_crime_in_tract ~ nox + dis + rad + \n                            \nmedv , data = Boston_over_median, family = binomial, subset=in_training)\n\nsummary(logsitic_Boston_3)\n\n\n\nCall:\nglm(formula = median_crime_in_tract ~ nox + dis + rad + medv, \n    family = binomial, data = Boston_over_median, subset = in_training)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.96251  -0.34660  -0.03326   0.01130   2.61492  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -23.82357    4.00139  -5.954 2.62e-09 ***\nnox          36.77822    6.01008   6.119 9.39e-10 ***\ndis           0.30796    0.16164   1.905 0.056747 .  \nrad           0.43698    0.12121   3.605 0.000312 ***\nmedv          0.03308    0.02884   1.147 0.251420    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 490.75  on 353  degrees of freedom\nResidual deviance: 187.91  on 349  degrees of freedom\nAIC: 197.91\n\nNumber of Fisher Scoring iterations: 8\n\nResults Logistic 3: As can be seen by the error rate\nabove, removing the single variable did not serve to improve the fit as\nit is still guessed about 89% of the tracts correctly.\nModel Choice LDA 1: My first fit for LDA will again\nstart by using all of the variables in the dataset.\n\n\nset.seed(292)\n\nlda.fit_1 <- lda(median_crime_in_tract ~ zn + indus + chas + nox + rm + \nage + dis + rad + tax + ptratio + lstat + medv, \ndata = Boston_over_median, subset=in_training)\n\nlda.fit_1\n\n\nCall:\nlda(median_crime_in_tract ~ zn + indus + chas + nox + rm + age + \n    dis + rad + tax + ptratio + lstat + medv, data = Boston_over_median, \n    subset = in_training)\n\nPrior probabilities of groups:\n  0   1 \n0.5 0.5 \n\nGroup means:\n         zn     indus       chas       nox       rm      age      dis\n0 23.161017  6.824407 0.06214689 0.4684520 6.407825 49.71808 5.085899\n1  1.265537 15.204181 0.10169492 0.6392147 6.174492 86.04802 2.537347\n       rad      tax  ptratio     lstat     medv\n0  4.19209 307.8192 17.85706  9.061356 25.25424\n1 13.92655 496.0226 18.95932 16.165819 20.23842\n\nCoefficients of linear discriminants:\n                  LD1\nzn      -0.0075446955\nindus    0.0111437527\nchas    -0.2075615996\nnox      9.1062150722\nrm       0.2401656098\nage      0.0131358309\ndis      0.1330540845\nrad      0.0584337654\ntax     -0.0006712969\nptratio  0.1157332806\nlstat    0.0409740539\nmedv     0.0509953338\n\nResults LDA 1: As can be seen above the LDA output\nsuggests that pihat1 and pihat2 are both 0.5 so half of the tracts are\nbelow and half are above the median as should be expected.\n\n\nset.seed(292)\n\nlda.pred <- predict(lda.fit_1, test_boston)\n\nlda.class <- lda.pred$class\n\ntable(lda.class, test_boston$median_crime_in_tract) %>% kable()\n\n\n\n0\n1\n0\n67\n17\n1\n9\n59\n\nResults LDA 1 Continued: As can be seen above the\nfirst LDA model with all of the predictors included predicts the test\nmedian crime rate dummy variable correctly about 82.9% of the time, in\ntotal it predicts the splits above correctly and the total percent as\nindicated below.\n\n\nmean(lda.class == test_boston$median_crime_in_tract) %>% kable()\n\n\nx\n0.8289474\n\nResults LDA 1 Continued: Below we see that with a\n50% threshold we acquire the two predictions below.\n\n\nsum(lda.pred$posterior[, 1] >= .5) %>% kable()\n\n\nx\n84\n\nsum(lda.pred$posterior[, 1] < .5) %>% kable()\n\n\nx\n68\n\nResults LDA 1 Continued: The plot of the fit is as\nplotted below.\n\n\nplot(lda.fit_1)\n\n\n\n\nModel Choice LDA 2: Since LDA does not give an\nindication of the performance of individual predictors I will model my\nsecond one using the predictors that logistic regression indicated as\nstatistically significant, or likely to be non-zero.\n\n\nset.seed(292)\n\nlda.fit_2 <- lda(median_crime_in_tract ~  zn + nox + dis + rad + ptratio \n                 + medv , data = Boston_over_median, subset=in_training)\n\nlda.fit_2 \n\n\nCall:\nlda(median_crime_in_tract ~ zn + nox + dis + rad + ptratio + \n    medv, data = Boston_over_median, subset = in_training)\n\nPrior probabilities of groups:\n  0   1 \n0.5 0.5 \n\nGroup means:\n         zn       nox      dis      rad  ptratio     medv\n0 23.161017 0.4684520 5.085899  4.19209 17.85706 25.25424\n1  1.265537 0.6392147 2.537347 13.92655 18.95932 20.23842\n\nCoefficients of linear discriminants:\n                 LD1\nzn      -0.007133417\nnox     10.155028179\ndis     -0.006666860\nrad      0.051446121\nptratio  0.116810250\nmedv     0.036270047\n\nResults LDA 2: As can be seen above the LDA output\nsuggests that pihat1 and pihat2 are both 0.5 so half of the tracts are\nbelow and half are above the median as should be expected as in the\nfirst example.\n\n\nset.seed(292)\n\nlda.pred_2 <- predict(lda.fit_2, test_boston)\n\nlda_2.class <- lda.pred_2$class\n\ntable(lda_2.class, test_boston$median_crime_in_tract) %>% kable()\n\n\n\n0\n1\n0\n65\n15\n1\n11\n61\n\nResults LDA 2 continued: As can be seen below this\nLDA performs incredibly similarly to the first 0.8289474 with both\nhaving identical correct classification rates.\n\n\nmean(lda_2.class == test_boston$median_crime_in_tract) %>% kable()\n\n\nx\n0.8289474\n\n\n\nsum(lda.pred_2$posterior[, 1] >= .5) %>% kable()\n\n\nx\n80\n\nsum(lda.pred_2$posterior[, 1] < .5) %>% kable()\n\n\nx\n72\n\n\n\nplot(lda.fit_2)\n\n\n\n\nModel Choice LDA 3: For my third LDA model choice I\nused all of the dummy variables and nox to see how using a number of\ncategorical variables effects the outcome of LDA.\n\n\nset.seed(7209)\n\nlda.fit_3<- lda(median_crime_in_tract ~ age + rad + chas\n + nox , data = Boston_over_median, subset=in_training)\n\nlda.fit_3\n\n\nCall:\nlda(median_crime_in_tract ~ age + rad + chas + nox, data = Boston_over_median, \n    subset = in_training)\n\nPrior probabilities of groups:\n  0   1 \n0.5 0.5 \n\nGroup means:\n       age      rad       chas       nox\n0 49.71808  4.19209 0.06214689 0.4684520\n1 86.04802 13.92655 0.10169492 0.6392147\n\nCoefficients of linear discriminants:\n            LD1\nage  0.01539699\nrad  0.06022015\nchas 0.01643176\nnox  6.98239859\n\n\n\nset.seed(7209)\n\nlda.pred_3 <- predict(lda.fit_3, test_boston)\n\nlda.class_3 <- lda.pred_3$class\n\ntable(lda.class_3, test_boston$median_crime_in_tract)\n\n\n           \nlda.class_3  0  1\n          0 70 16\n          1  6 60\n\nResults Model Choice LDA 3: As seen above this\nsubset of predictors has decreased the misclassification error rate as\ncompared to the previous models using just the most “statistically\nsignificant” predictors in LDA. The correct classification rate is\naround 85% here which is a slight increase from previous models.\n\n\nmean(lda.class_3 == test_boston$median_crime_in_tract) %>% kable()\n\n\nx\n0.8552632\n\n\n\nsum(lda.pred_3$posterior[, 1] >= .5)%>% kable()\n\n\nx\n86\n\nsum(lda.pred_3$posterior[, 1] < .5) %>% kable()\n\n\nx\n66\n\n\n\nplot(lda.fit_3)\n\n\n\n\nModel Choice Naive Bayes: My first model for naive\nbayes uses an, nox, rad, and taxes to predict median crime rate.\n\n\nset.seed(2680)\n\nnb.fit_1 <- naiveBayes(median_crime_in_tract ~ zn + nox +rad +\n                         tax , data = Boston_over_median, in_training)\n\nnb.fit_1\n\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n  0   1 \n0.5 0.5 \n\nConditional probabilities:\n   zn\nY        [,1]      [,2]\n  0 21.525692 29.319808\n  1  1.201581  4.798611\n\n   nox\nY        [,1]       [,2]\n  0 0.4709711 0.05559789\n  1 0.6384190 0.09870365\n\n   rad\nY        [,1]     [,2]\n  0  4.158103 1.659121\n  1 14.940711 9.529843\n\n   tax\nY       [,1]     [,2]\n  0 305.7431  87.4837\n  1 510.7312 167.8553\n\nResults 1 Naive Bayes: As can be seen from the error\nrates below the first naive bayes model correctly classifies the\npredictions 83% of the time whihc is similar to the results of the\nprevious LDA models.\n\n\nset.seed(2680)\n\nnb.class_1 <- predict(nb.fit_1, test_boston)\n\ntable(nb.class_1, test_boston$median_crime_in_tract) %>% kable()\n\n\n\n0\n1\n0\n65\n15\n1\n11\n61\n\n\n\nmean(nb.class_1 ==  test_boston$median_crime_in_tract) %>% kable()\n\n\nx\n0.8289474\n\nModel Choice 2 Naive Bayes: As can be seen below the\nnext model choice uses the same statistically significant predictors\naccording to their p-values in logsitic regression to see if this will\nimprove the prediction of Naive Bayes.\n\n\nset.seed(382)\n\nnb.fit_2 <- naiveBayes(median_crime_in_tract ~ zn + nox + dis + rad + \n              ptratio + medv , data = Boston_over_median, subset = in_training)\n\nnb.fit_2\n\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n  0   1 \n0.5 0.5 \n\nConditional probabilities:\n   zn\nY        [,1]     [,2]\n  0 23.161017 30.10405\n  1  1.265537  4.93395\n\n   nox\nY        [,1]       [,2]\n  0 0.4684520 0.05410754\n  1 0.6392147 0.10448031\n\n   dis\nY       [,1]     [,2]\n  0 5.085899 2.084295\n  1 2.537347 1.180086\n\n   rad\nY       [,1]     [,2]\n  0  4.19209 1.626256\n  1 13.92655 9.545955\n\n   ptratio\nY       [,1]     [,2]\n  0 17.85706 1.836210\n  1 18.95932 2.414571\n\n   medv\nY       [,1]      [,2]\n  0 25.25424  6.828791\n  1 20.23842 10.349862\n\nNaive Bayes model 2 Results: The inclusion of 6\nvariables, as opposed to 4 slightly improves the correct classification\nrate of our model, in this case, about a percent.\n\n\nset.seed(2680)\n\nnb.class_2 <- predict(nb.fit_2, test_boston)\n\ntable(nb.class_2, test_boston$median_crime_in_tract) %>% kable()\n\n\n\n0\n1\n0\n61\n9\n1\n15\n67\n\n\n\nmean(nb.class_2 ==  test_boston$median_crime_in_tract) %>% kable()\n\n\nx\n0.8421053\n\ntraining_boston <- Boston_over_median[in_training,]\ntest_boston <- Boston_over_median[-in_training,]\nModel Choice 3 naive Bayes: For this naive bayes I\nwill be using the categorical variables in addition to nox and number of\nrooms to see the impact of these two categorical variables with the two\nindicators of density and pollution.\n\n\nset.seed(282)\n\nnb.fit_3 <- naiveBayes(median_crime_in_tract ~ nox + chas +rad + \n                         rm , data = Boston_over_median, subset = in_training)\n\nnb.fit_3\n\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n  0   1 \n0.5 0.5 \n\nConditional probabilities:\n   nox\nY        [,1]       [,2]\n  0 0.4684520 0.05410754\n  1 0.6392147 0.10448031\n\n   chas\nY         [,1]      [,2]\n  0 0.06214689 0.2421070\n  1 0.10169492 0.3031041\n\n   rad\nY       [,1]     [,2]\n  0  4.19209 1.626256\n  1 13.92655 9.545955\n\n   rm\nY       [,1]      [,2]\n  0 6.407825 0.5381020\n  1 6.174492 0.8282674\n\nModel Evaluation Naive Bayes 3: As can be seen from\nthe results below the naive Bayes with these variables included performs\nvery well improving the number of correctly identified cross validated\npoints to about 88%.\n\n\nset.seed(235)\n\nnb.class_3 <- predict(nb.fit_3, test_boston)\n\ntable(nb.class_3, test_boston$median_crime_in_tract) %>% kable()\n\n\n\n0\n1\n0\n73\n14\n1\n3\n62\n\n\n\nmean(nb.class_3 ==  test_boston$median_crime_in_tract) %>% kable()\n\n\nx\n0.8881579\n\nModel Choice 1 KNN: Using nox and rad as our\nvariables below we see that 2 nearest neighbors results in a 98% rate of\ncorrect identifications.\n\n\nset.seed(777)\n\ntrain.X <- cbind(Boston_over_median$nox, Boston_over_median$rad)[in_training, ]\n\ntest.X <- cbind(Boston_over_median$nox, Boston_over_median$rad)[-in_training, ]\n\ntrain.median <- Boston_over_median$median_crime_in_tract[in_training]\n\nknn_pred_1 <- knn(train.X, test.X, train.median, k=2)\n\ntable(knn_pred_1, test_boston$median_crime_in_tract) %>% kable()\n\n\n\n0\n1\n0\n74\n1\n1\n2\n75\n\n\n\nmean(knn_pred_1 == test_boston$median_crime_in_tract) %>% kable()\n\n\nx\n0.9802632\n\nModel Choice 2 KNN: As can be seen below using these\ntwo variables and 3 nearest neighbors decreases the number of correctly\nidentified points in the test set slightly. Continuing to 4 nearest\nneighbors afterwards we see this trend continue.\n\n\nset.seed(377)\n\nknn_pred_2 <- knn(train.X, test.X, train.median, k=3)\n\ntable(knn_pred_2, test_boston$median_crime_in_tract)\n\n\n          \nknn_pred_2  0  1\n         0 72  1\n         1  4 75\n\n\n\nmean(knn_pred_2 == test_boston$median_crime_in_tract) %>% kable()\n\n\nx\n0.9671053\n\nModel Choice 3 KNN:\n\n\nset.seed(377)\n\nknn_pred_3 <- knn(train.X, test.X, train.median, k=4)\n\ntable(knn_pred_3, test_boston$median_crime_in_tract)\n\n\n          \nknn_pred_3  0  1\n         0 70  1\n         1  6 75\n\n\n\nmean(knn_pred_3 == test_boston$median_crime_in_tract) %>% kable()\n\n\nx\n0.9539474\n\nKNN-Results: According to the KNN’s above the\nresults of suggests that 2 nearest neighbors have the lowest amount of\nmisclassifications for the nox and rad variables.\nCh. 5, Exercise 2\nbasis: We will now derive the probability that a\ngiven observation is part of a bootstrap sample. Suppose that we obtain\na bootstrap sample from a set of n observations.\n(a)\nQuestion: What is the probability that the first\nbootstrap observation is not the jth observation from the original\nsample? Justify your answer.\nAnswer: The probability that the first bootstrap\nobservation is the jth observation from the original sample of n\nobservations is 1/n. This means the probability of the converse, that it\nis not the jth observation is the total probability 1 -\n1/n\n(b)\nQuestion: What is the probability that the second\nbootstrap observation is not the jth observation from the original\nsample?\nAnswer: The probability the second bootstrap\nobservation is not the jth observation is the same as the first since\nbootstrapping uses replacement so the probability of each observation\nbeing drawn does not change from bootstrap to bootstrap even if the\nnumber of interest is drawn. As the authors state that since there is\nreplacement the “same observation can occur more than once in the\nbootstrap data set” (211) so the second observation has the same\nprobability as the first of being the jth observation,\n(c)\nQuestion: Argue that the probability that the jth\nobservation is not in the bootstrap sample is (1 - 1/n)^n.\nAnswer: The probability that the jth observation is\nnot in the boot is (1 - 1/n)^n. This is because a the probability for a\nsingle observation is (1-1/n), as there is replacement the probability\nof drawing the jth observation does not change from bootstrap to\nbootstrap or draw to draw. As a result the probability of jth\nobservation in an entire bootstrap sample is the product of every single\nbootstrap observation not being j, so because we have n observations to\nchoose from our probability of not having any one in particular\nobservation is the probability of not drawing jth to the power of the\nnumber of observations.\n(d)\nQuestion: When n = 5, what is the probability that\nthe jth observation is in the bootstrap sample?\nAnswer: As seen below, the probability that the jth\nobservation is in the bootstrap is 0.67232, this is because the\nprobability of not drawing an observation is (1-1/n)^n\nso the probability of drawing one must be one minus this value as seen\nbelow.\n\n\n(1-(1 - 1/5)^5) %>% kable()\n\n\nx\n0.67232\n\n(e)\nQuestion: When n = 100, what is the probability that\nthe jth observation is in the bootstrap sample?\nAnswer: In this case when n = 100 the probability\nthat the jth observation is in the bootstrap sample is 0.6339677 or\n63.39677%\n\n\n(1-(1 - 1/100)^100) %>% kable()\n\n\nx\n0.6339677\n\n(f)\nQuestion: When n = 10, 000, what is the probability\nthat the jth observation is in the bootstrap sample?\nAnswer: In this case when n = 10,000 the probability\nthat the jth observation is in the bootstrap sample is 0.632139 or\n63.2139%\n\n\n(1-(1 - 1/10000)^10000) %>% kable()\n\n\nx\n0.632139\n\n(g)\nQuestion: Create a plot that displays, for each\ninteger value of n from 1 to 100,000, the probability that the jth\nobservation is in the bootstrap sample. Comment on what you observe.\nAnswer: Below I have plotted an integer value from 0\nto 100000 using the colon command. After this I calculate the\nprobability that the observation is in the dataset using 1 - the\nprobability that it is not in the dataset which is (1-(1 -\n1/possible_integers)^possible_integers), the inverse of the prior\nproblems. For clarity I have also used the prior n-sizes of 5, 100, and\n10,000. As can be seen the data appears to have a negative exponential\nrelationship that eventually reaches a minimum probability near 0.63 no\nmatter how larger the number of bootstraps we take is.\n\n\n\n\n\n\n\n\n\n(h)\nQuestion: We will now investigate numerically the\nprobability that a bootstrap sample of size n = 100 contains the jth\nobservation. Here j = 4. We repeatedly create bootstrap samples, and\neach time we record whether or not the fourth observation is contained\nin the bootstrap sample.\nAnswer: For the loop below the authors use a\nreplication function with an empty vector, NA, this is then done 10,000\ntimes. Next this goes into a loop with i in 1 through 10000. Next this\nfunction added a sample with replacement from 1:00, sees if it equals 4,\nsums the number of samples equal to 4 are greater than zero, this is\nthen stored in the “store” which is then expressed by taking its\nmean.\n\n\nset.seed(222222)\n\nstore <- rep(NA, 10000) \n\nfor(i in 1:10000){\n   \nstore[i] <- sum(sample(1:100, rep=TRUE) == 4) > 0 \n\n}\n\nmean(store)\n\n\n[1] 0.6362\n\nQuestion: Comment on the results obtained.\nAnswer continued: The results of the function are\n0.6362, this suggests that as the number of samples increases the\nlikelihood of observing a specific jth observation in an n out of 100 in\nan increasingly large number of bootstraps with replacement approaches\n0.636.\nCh. 5, Exercise 3\nBasis: We now review k-fold cross-validation.\nPart (a)\nQuestion: Explain how k-fold cross-validation is\nimplemented.\nAnswer: A k-folds cross validation method is\nimplemented by randomly diving sets of observations in our data into\ngroups, in this case they are known as folds. The data is divided k\ntimes and the first fold, which we refer to as the validation set is\nthen removed and the k-1 folds leftover are then fit using an analysis\nmethod, we then validate the model that we got from the model with the\ndata not in the fold to see how accurately the model predicts the left\nout data. This process is then repeated for all k-folds that were not\nthe first and were used to fit the original model. Iterating through\nthis process results in a cross validation estimate in the form of\nerrors between the predicted values from the model using k-1 and the\ntrue values in the left out k-fold.\nPart (b)\nQuestion: What are the advantages and disadvantages\nof k-fold cross-validation relative to:\nPart i.\nQuestion: The validation set approach?\nAnswer: The validation set approach faces the issue\nof insufficient data being used to construct its model. Since the method\ncan only divide the data in half its estimates can do a poor job\npredicting the left out set because it contains so much data that could\nbe reflective of the relationship between predictors and responses. This\ncan result in variance in comparing the predictions from the modeling\nset to the validation set. As the authors also point out the validation\nset approach is fitting on fewer observations it is likely to\noverestimate test error rate. The disadvantage of k-folds compared to\nvalidation set approach.\nPart ii.\nQuestion: LOOCV?\nAnswer: Leave one out cross validation is\nessentially a special case of k-folds cross validation where the number\nof “k” folds is the total number of observations in the dataset, minus\none. This approach is n-1 k-folds on the dataset and as a result it has\nlittle randomness in its selection thus it has an extremely small amount\nof variance between its predictions since a vast majority of data is\nused to make predictions that can only vary significantly from one point\nthat is not used. Depending on the size of the dataset cross validating\none for every point could take a great deal of computational power.\nUnlike the validation set leave-one-out-cross-validation will greatly\nunderestimate the test error rates since its prediction only excludes a\nsingle point. However in k-folds a number of folds that is not\ncomputationally intensive or accounts for a larger difference between\nfolds that allow for randomness in the subsets that can give a better\nsense of how errors are being made as they will be validated on many\nmore points. As a result a number of k-folds can be chosen that is not 2\nor n-1, this k can be determined in such a way that it likely has more\nbias than loocv, but balances the excessively high variance from 2 folds\nor validation set, and the excessively low variance of loocv.\nCh. 5, Exercise 5\nBasis: In Chapter 4, we used logistic regression to\npredict the probability of default using income and balance on the\nDefault data set. We will now estimate the test error of this logistic\nregression model using the validation set approach. Do not forget to set\na random seed before beginning your analysis.\n(a)\nQuestion: Fit a logistic regression model that uses\nincome and balance to predict default.\nAnswer: Below is my first equation labelled\ninc_bal_logit which uses the default data and a binomial family within a\nglm to model a logistic regression predicting whether of not an\nindividual defaults based on the income and their balance of debt.\n\n\nset.seed(777)\n\n\ninc_bal_logit <- glm(default ~ income + balance,  data = Default, family = \"binomial\")\n\ninc_bal_logit\n\n\n\nCall:  glm(formula = default ~ income + balance, family = \"binomial\", \n    data = Default)\n\nCoefficients:\n(Intercept)       income      balance  \n -1.154e+01    2.081e-05    5.647e-03  \n\nDegrees of Freedom: 9999 Total (i.e. Null);  9997 Residual\nNull Deviance:      2921 \nResidual Deviance: 1579     AIC: 1585\n\n(b)\nBasis: Using the validation set approach, estimate\nthe test error of this model. In order to do this, you must perform the\nfollowing steps:\ni.\nQuestion: Split the sample set into a training set\nand a validation set.\nAnswer: First I split the sample set into training\nand validation by sampling the data using the total number of rows in\nthe dataset nrow(Default) and diving it in half by sampling the\nnrow(Default), nrow(Default)/2 times. This amounts to sampling from\n10000 rows 5000 times randomly. Next I split the data into a validation\nand training set by choosing rows equivalent to those in my training set\nand not in my training set.\n\n\nset.seed(777)\n\ntrain5_5_1 <- sample(nrow(Default), nrow(Default)/2)\n\ndefault_training <- Default[train5_5_1,]\n\ndefault_validation <- Default[-train5_5_1,]\n\n\n\nii.\nQuestion: Fit a multiple logistic regression model\nusing only the training observations.\nAnswer: Next I fit a logistic regression using only\nmy training observations which I defined above as default_training\n\n\nset.seed(777)\n\ninc_bal_logit_ii <- glm(default ~ income + balance,  family = \"binomial\", data = default_training)\n\n\n\niii.\nQuestion: Obtain a prediction of default status for\neach individual in the validation set by computing the posterior\nprobability of default for that individual, and classifying the\nindividual to the default category if the posterior probability is\ngreater than 0.5.\nAnswer: I then predict whether or not an individual\nin the validation set defaults by using the predict() function which\ntakes my function inc_bal_logit_ii and my dataset, in this case the\nvalidation set default_validation, I then choose response as this is the\ntype of prediction we are interested in rather than terms.\n\n\nset.seed(777)\n\ndefault_estimates <- predict(inc_bal_logit_ii, default_validation,  type=\"response\")\n\nclassification <- ifelse(default_estimates>0.5,\"Yes\",\"No\")\n\n\n\niv.\nQuestion: Compute the validation set error, which is\nthe fraction of the observations in the validation set that are\nmisclassified.\nAnswer: I then compute the validation set error by\ntaking the mean of the number of classification outcomes that are not\nequal to their true values in the default_validation dataset’s default\ncolumn.\n\n\nmean(classification != default_validation$default)\n\n\n[1] 0.0256\n\n(a) (Split 2)\nQuestion: Fit a logistic regression model that uses\nincome and balance to predict default.\nAnswer: In the subsequent question I repeat my model\nusing different splits acquired by changing the seed which dictates the\nrandom generation of all “random” generation in the model through the\nsampling split.\n\n\nset.seed(908)\n\ninc_bal_logit <- glm(default ~ income + balance,  data = Default, \n                     family = \"binomial\")\n\ninc_bal_logit\n\n\n\nCall:  glm(formula = default ~ income + balance, family = \"binomial\", \n    data = Default)\n\nCoefficients:\n(Intercept)       income      balance  \n -1.154e+01    2.081e-05    5.647e-03  \n\nDegrees of Freedom: 9999 Total (i.e. Null);  9997 Residual\nNull Deviance:      2921 \nResidual Deviance: 1579     AIC: 1585\n\n(b) (Split 2)\nBasis: Using the validation set approach, estimate\nthe test error of this model. In order to do this, you must perform the\nfollowing steps:\ni.\nQuestion: Split the sample set into a training set\nand a validation set.\n\n\nset.seed(908)\n\ntrain5_5_1 <- sample(nrow(Default), nrow(Default)/2)\n\ndefault_training<-Default[train5_5_1,]\n\ndefault_validation<-Default[-train5_5_1,]\n\n\n\nii.\nQuestion: Fit a multiple logistic regression model\nusing only the training observations.\n\n\nset.seed(908)\n\ninc_bal_logit_ii <- glm(default ~ income + balance,  family = \"binomial\", \n                        data = default_training)\n\n\n\niii.\nQuestion: Obtain a prediction of default status for\neach individual in the validation set by computing the posterior\nprobability of default for that individual, and classifying the\nindividual to the default category if the posterior probability is\ngreater than 0.5.\n\n\nset.seed(908)\n\ndefault_estimates<-predict(inc_bal_logit_ii, default_validation, \n                           type=\"response\")\n\nclassification<-ifelse(default_estimates>0.5,\"Yes\",\"No\")\n\n\n\niv.\nQuestion: Compute the validation set error, which is\nthe fraction of the observations in the validation set that are\nmisclassified.\n\n\nmean(classification != default_validation$default)\n\n\n[1] 0.0286\n\n(a) (Split 3)\nQuestion: Fit a logistic regression model that uses\nincome and balance to predict default.\n\n\nset.seed(2000)\n\ninc_bal_logit <- glm(default ~ income + balance,  data = Default,\n                     family = \"binomial\")\n\ninc_bal_logit\n\n\n\nCall:  glm(formula = default ~ income + balance, family = \"binomial\", \n    data = Default)\n\nCoefficients:\n(Intercept)       income      balance  \n -1.154e+01    2.081e-05    5.647e-03  \n\nDegrees of Freedom: 9999 Total (i.e. Null);  9997 Residual\nNull Deviance:      2921 \nResidual Deviance: 1579     AIC: 1585\n\n(b) (Split 3)\nBasis: Using the validation set approach, estimate\nthe test error of this model. In order to do this, you must perform the\nfollowing steps:\ni.\nQuestion: Split the sample set into a training set\nand a validation set.\n\n\nset.seed(2000)\n\ntrain5_5_1 <- sample(nrow(Default), nrow(Default)/2)\n\ndefault_training<-Default[train5_5_1,]\n\ndefault_validation<-Default[-train5_5_1,]\n\n\n\nii.\nQuestion: Fit a multiple logistic regression model\nusing only the training observations.\n\n\nset.seed(2000)\n\ninc_bal_logit_ii <- glm(default ~ income + balance,  family = \"binomial\", \n                        data = default_training)\n\n\n\niii.\nQuestion: Obtain a prediction of default status for\neach individual in the validation set by computing the posterior\nprobability of default for that individual, and classifying the\nindividual to the default category if the posterior probability is\ngreater than 0.5.\n\n\nset.seed(2000)\n\ndefault_estimates<-predict(inc_bal_logit_ii, default_validation,  \n                           type=\"response\")\n\nclassification<-ifelse(default_estimates>0.5,\"Yes\",\"No\")\n\n\n\niv.\nQuestion: Compute the validation set error, which is\nthe fraction of the observations in the validation set that are\nmisclassified.\n\n\nmean(classification != default_validation$default)\n\n\n[1] 0.0294\n\n(a) (Split 4)\nQuestion: Fit a logistic regression model that uses\nincome and balance to predict default.\n\n\nset.seed(2)\n\ninc_bal_logit <- glm(default ~ income + balance,  data = Default,\n                     family = \"binomial\")\n\ninc_bal_logit\n\n\n\nCall:  glm(formula = default ~ income + balance, family = \"binomial\", \n    data = Default)\n\nCoefficients:\n(Intercept)       income      balance  \n -1.154e+01    2.081e-05    5.647e-03  \n\nDegrees of Freedom: 9999 Total (i.e. Null);  9997 Residual\nNull Deviance:      2921 \nResidual Deviance: 1579     AIC: 1585\n\n(b) (Split 4)\nBasis: Using the validation set approach, estimate\nthe test error of this model. In order to do this, you must perform the\nfollowing steps:\ni.\nSplit the sample set into a training set and a validation set.\n\n\nset.seed(2)\n\ntrain5_5_1 <- sample(nrow(Default), nrow(Default)/2)\n\ndefault_training<-Default[train5_5_1,]\n\ndefault_validation<-Default[-train5_5_1,]\n\n\n\nii.\nFit a multiple logistic regression model using only the training\nobservations.\n\n\nset.seed(2)\n\ninc_bal_logit_ii <- glm(default ~ income + balance, \n                        family = \"binomial\", data = default_training)\n\n\n\niii.\nObtain a prediction of default status for each individual in\nthe validation set by computing the posterior probability of default\nfor that individual, and classifying the individual to the default\ncategory if the posterior probability is greater than 0.5.\n\n\nset.seed(2)\n\ndefault_estimates<-predict(inc_bal_logit_ii, default_validation,  \n                           type=\"response\")\n\nclassification<-ifelse(default_estimates>0.5,\"Yes\",\"No\")\n\n\n\niv.\nCompute the validation set error, which is the fraction of the\nobservations in the validation set that are misclassified.\n\n\nmean(classification != default_validation$default)\n\n\n[1] 0.0238\n\n(d)\nQuestion: Now consider a logistic regression model\nthat predicts the probability of default using income, balance, and a\ndummy variable for student. Estimate the test error for this model using\nthe validation set approach. Comment on whether or not including a dummy\nvariable for student leads to a reduction in the test error rate.\n\n\nset.seed(222)\n\n\nstudent_bal_logit <- glm(default ~ income + balance + student,  \n                         data = Default, family = \"binomial\")\n\nstudent_bal_logit\n\n\n\nCall:  glm(formula = default ~ income + balance + student, family = \"binomial\", \n    data = Default)\n\nCoefficients:\n(Intercept)       income      balance   studentYes  \n -1.087e+01    3.033e-06    5.737e-03   -6.468e-01  \n\nDegrees of Freedom: 9999 Total (i.e. Null);  9996 Residual\nNull Deviance:      2921 \nResidual Deviance: 1572     AIC: 1580\n\n(b)\nBasis: Using the validation set approach, estimate\nthe test error of this model. In order to do this, you must perform the\nfollowing steps:\ni.\nSplit the sample set into a training set and a validation set.\n\n\nset.seed(222)\n\ntrain5_5_1 <- sample(nrow(Default), nrow(Default)/2)\n\ndefault_training<-Default[train5_5_1,]\n\ndefault_validation<-Default[-train5_5_1,]\n\n\n\nii.\nFit a multiple logistic regression model using only the training\nobservations.\n\n\nset.seed(222)\n\nstudent_bal_logit_ii <- glm(default ~ income + balance,  \n                            family = \"binomial\", data = default_training)\n\n\n\niii.\nObtain a prediction of default status for each individual in\nthe validation set by computing the posterior probability of default\nfor that individual, and classifying the individual to the default\ncategory if the posterior probability is greater than 0.5.\n\n\nset.seed(222)\n\ndefault_estimates_student<-predict(student_bal_logit_ii, default_validation,  \n                                   type=\"response\")\n\nclassification_students<-ifelse(default_estimates_student>0.5,\"Yes\",\"No\")\n\n\n\niv.\nCompute the validation set error, which is the fraction of the\nobservations in the validation set that are misclassified.\n\n\nmean(classification != default_validation$default)\n\n\n[1] 0.0492\n\n(d)\nAnswer: It appears that including the dummy variable\nfor student increases the missclassification error rate in the dataset,\nin this way the inclusion of the variable is not beneficial in reducing\nthe error rate of the function as it increases from under 0.03, or less\nthan 3% to 0.0468 using my seed which is 4.68% which thus decreases the\naccuracy of the model. This could suggest that student variable leads to\nsome calculations in the model that are more likely to mis-classify.\nStudents take on a great volume of debt at a young age, however they do\nso as an investment for later earnings, as a result students with high\ndebt may be misclassified by the regression due to their likely high\ndebt.\nCh. 5, Exercise 9\nBasis: We will now consider the Boston housing data\nset, from the ISLR2 library.\n(a)\nQuestion: Based on this data set, provide an\nestimate for the population mean of medv. Call this estimate mu.\nAnswer: My calculation for the population mean is\nfound my taking the mean of the entire column of medv from the Boston\ndataset which is stored as mu\n\n\nmu <- mean(Boston$medv)\n\nmu %>% kable()\n\n\nx\n22.53281\n\n(b)\nQuestion: Provide an estimate of the standard error\nof mu. Interpret this result.\nHint: We can compute the standard error of the sample mean by\ndividing the sample standard deviation by the square root of the number\nof observations.\nAnswer: Using the hint I label the standard error of\nmu as the standard deviation of the Boston$medv column and diving it by\nthe number of observations nrow in the Boston dataset which has its\nsquare root taken before deiving the standard deviation, this is then\nstored in se_mu\n\n\nse_mu<- sd(Boston$medv)/sqrt(nrow(Boston))\n\nse_mu %>% kable()\n\n\nx\n0.4088611\n\n(c)\nQuestion: Now estimate the standard error of mu\nusing the bootstrap. How does this compare to your answer from (b)?\nAnswer: In order to estimate the standard error of\nmu using a bootstrap I must first create a function that can be used by\nboot, which will return our statistic of interest. In this case the data\nis Boston and its column medv Boston$medv which are placed in the first\nobservation of the bootstrap. Next I need a statistic writing the\nfunction I needed it to specify the dataframe and column index as\ndescribed in the chapter 5 lab. Next I need this function to return the\nmean of this dataframe’s specified index. Following the chapter 5 lab I\nuse 1000 replicates of the bootstrap.\n\n\n?boot\nset.seed(547)\n\nmu_function <- function(data, index) {\n mu_for_function <- mean(data[index])\n return(mu_for_function)\n}\n\n\nboot(Boston$medv, mu_function , R = 1000)\n\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Boston$medv, statistic = mu_function, R = 1000)\n\n\nBootstrap Statistics :\n    original     bias    std. error\nt1* 22.53281 0.01013577   0.4090362\n\n(d)\nQuestion: Based on your bootstrap estimate from (c),\nprovide a 95 % confidence interval for the mean of medv. Compare it to\nthe results obtained using t.test(Boston$medv).\nHint: You can approximate a 95 % confidence interval\nusing the formula [mu - 2SE(mu), mu + 2SEmu].\nAnswer: I create the 95% confidence interval from my\nbootstrap using my mean estimate plus and minus 2 standard deviations,\nthis estimate is 22.53281 as the mean I then use the hint formula addin\nor subtracting 2 standard errors.\n\n\n(sd(Boston$medv)/sqrt(nrow(Boston)) )%>% kable()\n\n\nx\n0.4088611\n\nci_95_boot <- c(\n (22.53281-2*0.4132074),(22.53281+2*0.4132074)\n   )\nci_95_boot %>% kable()\n\n\nx\n21.70640\n23.35922\n\n\n\nt.test(Boston$medv)\n\n\n\n    One Sample t-test\n\ndata:  Boston$medv\nt = 55.111, df = 505, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.72953 23.33608\nsample estimates:\nmean of x \n 22.53281 \n\nAnswer Continued: The t-test estimates (21.72953\n23.33608) and our bootstrap estimates (21.70640 23.35922). The\ndifferences in the lower tail are below:\n\n\n21.72953-21.70640\n\n\n[1] 0.02313\n\nAnswer Continued: The differences in the upper tail\nare below:\n\n\n23.33608 - 23.35922\n\n\n[1] -0.02314\n\nAnswer Continued: The difference is similar in both\ndirections on the lower tail the t-test estimates a slightly higher\nupper tail and a slightly higher on the lower tail.\n(e)\nQuestion: Based on this data set, provide an\nestimate, mu_med, for the median value of medv in the population.\nAnswer: I calculate this median value as the median\nof the Boston dataset and medv column using the median function which is\nsaved as mu_med.\n\n\nmu_med <- median(Boston$medv)\n\nmu_med %>% kable()\n\n\nx\n21.2\n\n(f)\nQuestion: We now would like to estimate the standard\nerror of mu_med.Unfortunately, there is no simple formula for computing\nthe standard error of the median. Instead, estimate the standard error\nof the median using the bootstrap. Comment on your findings.\nAnswer: Below I create a function to take the median\nof a dataset and its column index that works the same way as the prior\nmean function.\n\n\nset.seed(8282)\n\nmedian_function <- function(data, index) {\n median_for_function <- median(data[index])\n return(median_for_function)\n}\n\nboot(Boston$medv, median_function, R = 1000)\n\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Boston$medv, statistic = median_function, R = 1000)\n\n\nBootstrap Statistics :\n    original   bias    std. error\nt1*     21.2 -0.01325    0.381356\n\nAnswer Continued: The resulting standard error from\nthis bootstrap is 0.381356.\n(g)\nQuestion: Based on this data set, provide an\nestimate for the tenth percentile of medv in Boston census tracts. Call\nthis quantity mu_0.1. (You can use the quantile() function.)\nAnswer: Below I calculate the 10th percentile using\nthe Boston dataset and the medv column and the quantile function, then\nspecifying the quantile between [0 and 1] I use 0.1 as this is the 10th\npercentile\n\n\nmu_0.1 <- quantile(Boston$medv, 0.1)\n\nmu_0.1 %>% kable()\n\n\n\nx\n10%\n12.75\n\n(h)\nQuestion: Use the bootstrap to estimate the standard\nerror of mu_0.1. Comment on your findings.\n\n\nset.seed(8282)\n\n\ntenth_quantile_function <- function(data, index) {\n tenth_for_function <- quantile(data[index], 0.1)\n return(tenth_for_function)\n}\n\nboot(Boston$medv, tenth_quantile_function, R = 1000)\n\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Boston$medv, statistic = tenth_quantile_function, \n    R = 1000)\n\n\nBootstrap Statistics :\n    original  bias    std. error\nt1*    12.75  -0.004   0.5122122\n\nAnswer: In this case I estimate the average quantile\nestimate of medv in the 10th percentile as 12.75 in the bootstrap as\ncompared to 12.75 in the dataset. It appears that both function capture\nthe same exact medv point as the 10th percentile in the dataset.\nCh. 6, Exercise 2\nBasis: For parts (a) through (c), indicate which of\ni. through iv. is correct. Justify your answer.\n(a)\nQuestion: The lasso, relative to least squares,\nis:\ni.\nMore flexible and hence will give improved prediction accuracy when its\nincrease in bias is less than its decrease in variance.\nAnswer: Compared to least squares the lasso\nregression is less flexible since it will perform variable selection,\nthis is meant to reduce overfitting which least squares will often do.\nIn this case the answer is correct because lasso’s are not more flexible\nto least squares since they do not fit as many variables and thus will\nnot fit the data as closely.\nii.\nMore flexible and hence will give improved prediction accuracy when its\nincrease in variance is less than its decrease in bias.\nAnswer: This answer is is incorrect as in the case\nof the first one because a\niii.\nLess flexible and hence will give improved prediction accuracy when its\nincrease in bias is less than its decrease in variance.\nAnswer: This answer is correct, as established lasso\nis less flexible as it will reduce non-influential parameters to zero\ndepending on its tuning parameters but will always be less flexible than\nthe regression it is based on if the tuning parameter is non-zero.\nHowever as the authors in ISLR explain “the lasso solution can yield a\nreduction in variance at the expense of a small increase in bias,” in\ncases when least squares is over fit and results in inaccurate\npredictions,” which this case follows.\niv.\nLess flexible and hence will give improved prediction accuracy when its\nincrease in variance is less than its decrease in bias.\nAnswer: This answer is incorrect as lasso regression\nis formulated in order to reduce overfitting and thus variance on fitted\ndata. As a result, and relative to least squares, lasso is not used to\nreduce bias, but instead variance in estimates.\n(b)\nQuestion: ridge regression relative to least\nsquares.\ni.\nMore flexible and hence will give improved prediction accuracy when its\nincrease in bias is less than its decrease in variance.\nAnswer: As in the first response the ridge\nregression also is less flexible than least squares because it removes\nvariables, or at least most their coefficients closer to zero, however,\nunlike lasso, it keeps the p-varaibles within the model. In this case\nmaking variables smaller does not allow the ridge regression to follow\nthe data as closely as least squares as increasing the shrinkage\ncoefficient leads to a decrease in flexibility by making the fit closer,\nthe second part is correct however as the ridge regression will decrease\nvariance as overfitting is also reduced..\nii.\nMore flexible and hence will give improved prediction accuracy when its\nincrease in variance is less than its decrease in bias.\nAnswer: This response is also incorrect as if the\nshrinkage coefficient is sufficiently small the function is less\nflexible, the second part is also incorrect as the function is valuable\nwhen it gives improved prediction accuracy when its increase in\nbias is sufficiently less than its decrease in\nvariance.\niii.\nLess flexible and hence will give improved prediction accuracy when its\nincrease in bias is less than its decrease in variance.\nAnswer: This response is again correct, here like\nlasso, we reduce the influence of non-influential parameters by moving\nthem towards, or to zero, as a result these variables will lose\ninfluence thus not being as likely to overfit as a result, the function\nmay not be able to avoid bias as much as linear regression but can\ngreatly reduce the variance due to overfitting.\niv.\nLess flexible and hence will give improved prediction accuracy when its\nincrease in variance is less than its decrease in bias.\nAnswer: This is again incorrect as it will decrease\nvariance rather than bias in its efforts not to overfit.\n(c)\nQuestion: non-linear methods relative to least\nsquares.\ni.\nMore flexible and hence will give improved prediction accuracy when its\nincrease in bias is less than its decrease in variance.\nAnswer: Non-linear methods are more flexible than\nleast squares, however, in most cases it is useful when its increase in\nbias is less than its decrease in\nii.\nMore flexible and hence will give improved prediction accuracy when its\nincrease in variance is less than its decrease in bias.\nAnswer: Non-linear methods are more flexible as they\nwill fit predictions more closely, they are also able to afford more\npredictive accuracy when their increase in variance is less than their\nincrease in bias, since bias is the difference between a parameters and\nthe expected value of a statistic the non-linear model will give better\npredictions when its increase in variance is less than its increase in\nbias. This is due to the bias variance trade-off, here out non-linear\nmodel fits data more closely thus it has the potential to have higher\nvariance if it is overfit than linear regression, as a result if that\nincrease in variance is sufficiency less than its decrease in bias due\nto its closer fit than we consider it a stronger model for the scenario\nthus this answer is correct.\niii.\nLess flexible and hence will give improved prediction accuracy when its\nincrease in bias is less than its decrease in variance.\nAnswer: This answer is incorrect as non-linear\nmodels are more flexible than linear ones.\niv.\nLess flexible and hence will give improved prediction accuracy when its\nincrease in variance is less than its decrease in bias.\nAnswer: This answer is incorrect as non-linear\nmodels are more flexible than linear ones.\nISLR Ch. 6, Exercise 9\nBasis: In this exercise, we will predict the number\nof applications received using the other variables in the College data\nset.\n(a)\nQuestion: Split the data set into a training set and\na test set.\nAnswer: I split the data into training and test set\nusing 1 through the number of rows in the dataset and sampling it\nnrow(College)*0.8 times which works out to taking 621.6 row samples with\nreplacement equals FALSE, this equates to and 80%, 20% split in the\ntraining and test sets. I specify the college_training as the 80% of\nCollege data I specified as my training. Next I find the test set using\nthe converse of the training rows.\n\n\nset.seed(9292)\n\ntrain_College_numbers <- sample(1:nrow(College), nrow(College)*0.8,\n                                replace = FALSE)\n\ncollege_training <- College[train_College_numbers,]\n\ncollege_test <- College[-train_College_numbers,]\n\n\n\n(b)\nQuestion: Fit a linear model using least squares on\nthe training set, and report the test error obtained.\nAnswer: I fit a least squares on my 80% training set\nbelow using all variables since we will be using a ridge regression that\nlater account for the number of variables we should use as the ridge\nmoves their coefficients towards zero.\n\n\nset.seed(9292)\n\nleast_squares_college <- glm(Grad.Rate ~. , data=college_training)\n\n\n\nAnswer: I then use the predict function using my\nleast squares function and my test data to make prediction for my error\nrate. I express the test error as the mean square error, which is\ncalculated as 142.0359\n\n\nset.seed(9292)\n\npred_college_for_mse <- predict(least_squares_college, newdata=college_test)\n\nMSE_college_test <- mean((college_test$Grad.Rate-pred_college_for_mse)^2)\n\nMSE_college_test %>% kable()\n\n\nx\n190.8233\n\n(c)\nQuestion: Fit a ridge regression model on the\ntraining set, with lambda chosen by cross-validation. Report the test\nerror obtained.\nAnswer: Below I make a grid of values that is a\nsequence of 10 to -2 of length 100 which goes from 10^10 to 10^(-2)\ncreates a set of selected values. In order to fit a ridge model using\ncross validation I split the model into training and test set. Using the\nglmnet() function I then plug in my x-ridge model matrix and the y-ridge\nbeing the true values from the graduation rate column of the college\ndataset. This results in a dimension of 18 by 100.\n\n\nset.seed(9292)\n\nx_ridge <- model.matrix(Grad.Rate ~ ., College)[, -1] \n\ny_ridge <- College$Grad.Rate\n\ngrid <- 10^seq(10, -2, length = 100)\n\nridge.mod <- glmnet(x_ridge, y_ridge, alpha = 0, lambda = grid)\n\ndim(coef(ridge.mod))\n\n\n[1]  18 100\n\nAnswer: Below I then split my data into an 80% 20%\nsplit between training and test sets. These values are stored in the\ntrain_ridge and test_ridge variables.\n\n\nset.seed(9292)\n\ntrain_ridge <- sample(1:nrow(x_ridge), nrow(x_ridge)*0.8 ) \n\ntest_ridge <- (-train_ridge)\n\ny.test_ridge <- y_ridge[test_ridge]\n\n\n\nAnswer: I then use the glmnet() function with an\nalpha of 0 which is a ridge penalty in this case. In this scenario the\nfunction is named ridge.mod, this is then put into predict() where it\npredicts on the test section of the data.\n\n\nset.seed(9292)\n\nridge.mod <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], \n                    alpha = 0, lambda = grid, thresh = 1e-12)\n\nridge.pred <- predict(ridge.mod, s = 4, newx = x_ridge[test_ridge,])\n\nmean((ridge.pred - y.test_ridge)^2) %>% kable()\n\n\nx\n191.9619\n\nAnswer: Afterwards we fit the model again using the\nglmnet() function ridge.mod using a new x from x_ridge and our x and y\nfrom training. We then access the 18 coefficients below, as can be seen\nthe ridge moves the coefficients it decides have little impact towns\nzero.\n\n\nset.seed(9292)\n\nridge.pred <- predict(ridge.mod, s = 0, newx = x_ridge[test_ridge, ], \n                      exact = T, x = x_ridge[train_ridge, ], y = y_ridge[train_ridge])\n\npredict(ridge.mod, s = 0, exact = T, type = \"coefficients\", \n        x = x_ridge[train_ridge, ], y = y_ridge[train_ridge])[1:18, ]\n\n\n  (Intercept)    PrivateYes          Apps        Accept        Enroll \n28.8970053130  4.6799076483  0.0012568714 -0.0008668106  0.0023593568 \n    Top10perc     Top25perc   F.Undergrad   P.Undergrad      Outstate \n 0.0468337629  0.1437342449 -0.0004323463 -0.0013350746  0.0008084910 \n   Room.Board         Books      Personal           PhD      Terminal \n 0.0021795777  0.0004206020 -0.0017882831  0.1379432456 -0.0702662338 \n    S.F.Ratio   perc.alumni        Expend \n 0.0406692785  0.2997380403 -0.0004995716 \n\nAnswer continued: Next after beginning these tests\nwe use cross validation to get the optimal tuning parameters from a\nnumber of repetitions using cv.glmnet() in this case it takes our\ntraining values and an alpha of zero to indicate it is a ridge and uses\na number of values of lambda to determine which minimizes mean squared\nerrors. This is plotted below\n\n\nset.seed(9292)\n\ncv.out <- cv.glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], alpha = 0) \n\nplot(cv.out)\n\n\n\n\nAnswer continued: The cv.out we calculated above\noutputs a 1se and min lambda for its mean square errors which I access\nbelow to determine the cross validated tuning parameter.\n\n\nset.seed(9292)\n\nbestlam <- cv.out$lambda.min\n\nbestlam\n\n\n[1] 1.876765\n\nAnswer continued: Finally we use out predict\nfunction to see what our mean squared error will be for our tuning\nparameters than minimizes MSE, in this case it is 191.0022 as seen below\ncompared to the grid values which yielded a 191.9619 at best.\n\n\nset.seed(9292)\n\nridge.pred <- predict(cv.out, s = bestlam, newx = x_ridge[test_ridge,])\n\nmean((ridge.pred - y.test_ridge)^2) %>% kable()\n\n\nx\n191.0022\n\nAnswer continued: The coefficient outputs for the\nlambda that minimizes MSE are then outputted below. Here Private school,\nper.alumni, Top10perc, and top25perc, along with Phd having coefficients\nover 0.05 in predicting the graduation rate.\n\n\nset.seed(9292)\n\nout <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], alpha = 0)\n\npredict(out, type = \"coefficients\", s = bestlam)[1:18, ]\n\n\n  (Intercept)    PrivateYes          Apps        Accept        Enroll \n30.4028107340  4.3857666072  0.0005579653  0.0002846755  0.0002715066 \n    Top10perc     Top25perc   F.Undergrad   P.Undergrad      Outstate \n 0.0898592282  0.1170937551 -0.0001524760 -0.0012353250  0.0006590921 \n   Room.Board         Books      Personal           PhD      Terminal \n 0.0020270611  0.0001479914 -0.0018704177  0.0918979855 -0.0206644006 \n    S.F.Ratio   perc.alumni        Expend \n 0.0319300362  0.2726551009 -0.0002998044 \n\n(d)\nQuestion: Fit a lasso model on the training set,\nwith lambda chosen by cross validation. Report the test error obtained,\nalong with the number of non-zero coefficient estimates.\nAnswer: The process of creating the lasso is largely\nsimilar to that of the ridge though here our inital glmnet() takes an\nalpha of 1. The plot below this indicates that depending on our choise\nof tuning parameter lambda, certain coefficients will become zero.\n\n\nset.seed(9292)\n\nlasso.mod <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], \n                    alpha = 1, lambda = grid)\n\nplot(lasso.mod)\n\n\n\n\nAnswer continued: We continue this analysis by cross\nvalidation using cv.glmnet() with an alpha of 1 to find the coefficients\nthat again, minimize the MSE.\n\n\nset.seed(9292)\n\ncv.out_2 <- cv.glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], alpha = 1) \n\nplot(cv.out_2)\n\n\n\n\nAnswer continued: After this cross validation has\nbeen calculated we then access the lambda that minimizes MSE. We then\nuse the predict function to calculate predictions for our lasso using\nthe lambda that minimizes MSE this gets a MSE of 190.8808.\n\n\nset.seed(9292)\n\nbestlam <- cv.out_2$lambda.min\n\nlasso.pred <- predict(cv.out_2, s = bestlam, newx = x_ridge[test_ridge,])\n\nmean((lasso.pred - y.test_ridge)^2) %>% kable()\n\n\nx\n190.8808\n\nAnswer continued: Finally we calculate and show the\ncoefficients for our equation with the lasso reducing our number of\nparameters by moving coefficients towards zero that have low impact.\nHere we remove, Accept, Enroll, F.Undergrad, Books, Terminal, and\nS.F.Ratio with other values still being reflected in the equation\ncoefficients.\n\n\nset.seed(9292)\n\nout <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], \n              alpha = 1, lambda = grid)\n\nlasso.coef <- predict(out, type = \"coefficients\", s = bestlam)[1:18, ]\n\nlasso.coef\n\n\n  (Intercept)    PrivateYes          Apps        Accept        Enroll \n29.8146208578  4.2178703808  0.0006714817  0.0000000000  0.0000000000 \n    Top10perc     Top25perc   F.Undergrad   P.Undergrad      Outstate \n 0.0508305488  0.1371884865  0.0000000000 -0.0013060619  0.0007140270 \n   Room.Board         Books      Personal           PhD      Terminal \n 0.0020119407  0.0000000000 -0.0016927269  0.0683671684  0.0000000000 \n    S.F.Ratio   perc.alumni        Expend \n 0.0000000000  0.2909171513 -0.0002954747 \n\nCh. 8, Exercise 4\nUses Plots in Figure 8.14.\nPart (a)\nQuestion: Sketch the tree corresponding to the\npartition of the predictor space illustrated in the left-hand panel of\nFigure 8.14. The numbers inside the boxes indicate the mean of Y within\neach region.\nAnswer: Possible Partitions in 8.14 Parent\n(x1<1), if not below return 5. All else if x1 is below 1, next\n(x2<1, or x2>1) if x2 greater than 1 return 15, if x1<0 less\nthan 0 return 3, if it is more than 0, but x2 is also less than zero\nreturn 10\n\n\nlibrary(treemap)\nlibrary(DiagrammeR)\n\n\n\n\n\nfig8_1_4 <- Node$new(\"figure 8.14\")\n    top_node <- fig8_1_4$AddChild(\"If x1<1\")\n    return_5 <- top_node$AddChild(\"5\")\n    second_node <- top_node$AddChild(\"If x2<1\")\n      return_15 <- second_node$AddChild(\"15\")\n      third_node <- second_node$AddChild(\"If x1<0\")\n        return_3 <- third_node$AddChild(\"3\")\n          fourth_node <-  third_node$AddChild(\"x2<0\")\n              fifth_node <-  fourth_node$AddChild(\"10\")\n              fifth_node <-  fourth_node$AddChild(\"0\")\n\nplot(fig8_1_4)\n\n\n\n{\"x\":{\"diagram\":\"digraph {\\n\\n\\n\\n\\n  \\\"1\\\" [label = \\\"figure 8.14\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"2\\\" [label = \\\"If x1<1\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"3\\\" [label = \\\"5\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"4\\\" [label = \\\"If x2<1\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"5\\\" [label = \\\"15\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"6\\\" [label = \\\"If x1<0\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"7\\\" [label = \\\"3\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"8\\\" [label = \\\"x2<0\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"9\\\" [label = \\\"10\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"10\\\" [label = \\\"0\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"1\\\"->\\\"2\\\" \\n  \\\"2\\\"->\\\"3\\\" \\n  \\\"2\\\"->\\\"4\\\" \\n  \\\"4\\\"->\\\"5\\\" \\n  \\\"4\\\"->\\\"6\\\" \\n  \\\"6\\\"->\\\"7\\\" \\n  \\\"6\\\"->\\\"8\\\" \\n  \\\"8\\\"->\\\"9\\\" \\n  \\\"8\\\"->\\\"10\\\" \\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\nPart (b)\nQuestion: Create a diagram similar to the left-hand\npanel of Figure 8.14, using the tree illustrated in the right-hand panel\nof the same figure. You should divide up the predictor space into the\ncorrect regions, and indicate the mean for each region.\n\n\n?plot\n\n\nHelp on topic 'plot' was found in the following packages:\n\n  Package               Library\n  base                  /Library/Frameworks/R.framework/Resources/library\n  graphics              /Library/Frameworks/R.framework/Versions/4.1/Resources/library\n\n\nUsing the first match ...\n\nplot(NA, NA, type = \"n\", xlim = c(0,3), \n     ylim = c(-1, 1.25), xlab = \"x-2\", ylab = \"x-1\")\n\nlines(x = c(1, 1), y = c(-1.0, 1.20))\n\nlines(x = c(2, 2), y = c(-1.0, 1.20))\n\nlines(x = c(0, 1), y = c(1, 1))\n\nlines(x = c(1, 2), y = c(0, 0))\n\ntext(0.5, 1.2 , \"0.63\") \n\ntext(0.5, 0.0 , \"-1.80\") \n\ntext(2.5, -0.0, \"-2.49\") \n\ntext(1.5, -0.5, \"-1.06\") \n\ntext(1.5, 0.5, \"0.21\") \n\n\n\n\nCh. 8, Exercise 7\nQuestion: In the lab, we applied random forests to\nthe Boston data using mtry = 6 and using ntree = 25 and ntree = 500.\nCreate a plot displaying the test error resulting from random forests on\nthis data set for a more comprehensive range of values for mtry and\nntree. You can model your plot after Figure 8.10. Describe the results\nobtained.\nAnswer: I will be modeling my response from that in\nfigure 8.10, in this case they use 3 m-values being m=p, m=p/2, and\nm=sqrt(p), in addition to this I will vary the number of trees in each\ncase. As seen below, I am using 13, 6.5 rounded up to 7 predictions, and\n3.6 rounded up to 4 for the square root of p.\n\n\n(ncol(Boston)-1)  %>% kable()\n\n\nx\n13\n\n(13/2) %>% kable()\n\n\nx\n6.5\n\n(sqrt(13)) %>% kable()\n\n\nx\n3.605551\n\nAnswer Continued: As in the lab, I will divide my\ndata into testing and training set, I will use 70% of the data in my\ntraining set for the model\n\n\nset.seed(888) \n\ntrain_boston_set <- sample(1:nrow(Boston), nrow(Boston)*0.7)\n\nRf_train_boston <- Boston[train_boston_set,]\n\nRf_test_boston <- Boston[-train_boston_set,]\n\nBoston_rf_mtry_13_ntree_25 <- randomForest(medv~. ,subset = train_boston_set, data = Boston, mtry = 13, ntree = 25)\n\nBoston_rf_mtry_7_ntree_25 <- randomForest(medv~.,subset = train_boston_set, data = Boston, mtry = 7, ntree = 25)\n\nBoston_rf_mtry_4_ntree_25 <- randomForest(medv~.,subset = train_boston_set,  data = Boston, mtry = 4, ntree = 25)\n\nBoston_rf_mtry_13_ntree_100 <- randomForest(medv~. ,subset = train_boston_set, data = Boston, mtry = 13, ntree = 100)\n\nBoston_rf_mtry_7_ntree_100 <- randomForest(medv~.,subset = train_boston_set, data = Boston, mtry = 7, ntree = 100)\n\nBoston_rf_mtry_4_ntree_100 <-randomForest(medv~.,subset = train_boston_set,  data = Boston, mtry = 4, ntree = 100)\n\nBoston_rf_mtry_13_ntree_500 <- randomForest(medv~. ,subset = train_boston_set, data = Boston, mtry = 13, ntree = 500)\n\nBoston_rf_mtry_7_ntree_500<-randomForest(medv~.,subset = train_boston_set, data = Boston, mtry = 7, ntree = 500)\n\nBoston_rf_mtry_4_ntree_500<-randomForest(medv~.,subset = train_boston_set,  data = Boston, mtry = 4, ntree = 500)\n\n\n\n\n\n\nAnswer continued: The plot above indicates the\naverage errors plotted against the number of trees used in the\nderivation, in this case I have included a legend as in 8.10 to indicate\nboth the number of trees used in addition to my comparison between m and\np used in my random forest. In this case it is apparent that trees in\nthe 500s initially have the lowest mean squared errors when compared to\n25 trees, and to a lesser extent, 100 trees. Below I will plot graphs\ncontaining only the 25 trees, 100 trees, and 500 trees.\n\n\n\nAnswer continued: Comparing the 25 trees to each\nother and 100 trees, it does firstly appear that 100 trees decreases the\nerror rate across the differing m and p values.\n\n\n\nAnswer continued: Comparing the 100 trees and 500s\ntrees it also appears that the higher number of trees decreases the\nerror rate in the data, with all plotted against each other as in the\nfirst graph it does appear that the larger number of trees decreases the\nerror rate more rapidly as the number of trees increases.\n\n\n\nCh. 8, Exercise 9\nBasis: This problem involves the OJ data set which\nis part of the ISLR2 package.\nPart (a)\nQuestion: Create a training set containing a random\nsample of 800 observations, and a test set containing the remaining\nobservations.\nAnswer: In order to take a sample 800 observations\nfrom OJ I use the sample function to take 800 from 1 through the total\nnumber of rows in the dataset. The test is then taken as the rows not\nincluded in this set of sampled rows.\n\n\nset.seed(727)\n\ntrain_orange_juice <- sample(1:nrow(OJ), 800, replace = FALSE)\n\nOJ_train <- OJ[train_orange_juice,]\n\nOJ_test <- OJ[-train_orange_juice,]\n\n\n\nPart (b)\nQuestion: Fit a tree to the training data, with\nPurchase as the response and the other variables as predictors. Use the\nsummary() function to produce summary statistics about the tree, and\ndescribe the results obtained. What is the training error rate? How many\nterminal nodes does the tree have?\nAnswer: Using the training data below I fit a tree\nusing the tree function on the training subset. The training error rate\nfrom the summary is 0.1475 and there are 8 terminal nodes.\n\n\nset.seed(727)\n\ntree.OJ_train <- tree(Purchase ~ ., OJ, subset = train_orange_juice)\n\nsummary(tree.OJ_train)\n\n\n\nClassification tree:\ntree(formula = Purchase ~ ., data = OJ, subset = train_orange_juice)\nVariables actually used in tree construction:\n[1] \"LoyalCH\"       \"PriceDiff\"     \"ListPriceDiff\" \"PctDiscMM\"    \nNumber of terminal nodes:  8 \nResidual mean deviance:  0.7145 = 565.9 / 792 \nMisclassification error rate: 0.1475 = 118 / 800 \n\nPart (c)\nQuestion: Type in the name of the tree object in\norder to get a detailed text output. Pick one of the terminal nodes, and\ninterpret the information displayed.\n\n\ntree.OJ_train\n\n\nnode), split, n, deviance, yval, (yprob)\n      * denotes terminal node\n\n 1) root 800 1065.00 CH ( 0.61625 0.38375 )  \n   2) LoyalCH < 0.5036 344  407.30 MM ( 0.27907 0.72093 )  \n     4) LoyalCH < 0.276142 172  131.50 MM ( 0.12791 0.87209 )  \n       8) LoyalCH < 0.051325 62   10.24 MM ( 0.01613 0.98387 ) *\n       9) LoyalCH > 0.051325 110  107.30 MM ( 0.19091 0.80909 ) *\n     5) LoyalCH > 0.276142 172  235.10 MM ( 0.43023 0.56977 )  \n      10) PriceDiff < 0.05 69   60.54 MM ( 0.15942 0.84058 ) *\n      11) PriceDiff > 0.05 103  137.60 CH ( 0.61165 0.38835 ) *\n   3) LoyalCH > 0.5036 456  351.30 CH ( 0.87061 0.12939 )  \n     6) LoyalCH < 0.764572 188  215.70 CH ( 0.73936 0.26064 )  \n      12) ListPriceDiff < 0.235 74  102.60 MM ( 0.50000 0.50000 )  \n        24) PctDiscMM < 0.196196 54   71.19 CH ( 0.62963 0.37037 ) *\n        25) PctDiscMM > 0.196196 20   16.91 MM ( 0.15000 0.85000 ) *\n      13) ListPriceDiff > 0.235 114   76.72 CH ( 0.89474 0.10526 ) *\n     7) LoyalCH > 0.764572 268   85.39 CH ( 0.96269 0.03731 ) *\n\nAnswer: One of the terminal nodes is the 7th one\nindicated, “7) LoyalCH > 0.764572 268, 85.39 CH ( 0.96269 0.03731 )”\nthis node indicates 6 attributes of the node the first, node), is\n7) in this case. The next attribute is the split in the\ndata that it represents in this case split, which is when the variable\nLoyalCH > 0.764572 next is the n, or number of observations that\nfulfill this attribute. Following this is the deviance of this\nobservation denoted as 85.39, yval is 0.96269, and finally (yprob) is\n0.03731.\nPart (d)\nQuestion: Create a plot of the tree, and interpret\nthe results.\n\n\n\nAnswer: The tree above indicates predicted\nclassification into 2 categories, either purchasing citrus hill or\nminute maid using class proportions in our training data, each split\nrepresents a distinction between observations in each predictor category\nas determined by prior steps to eliminate the most classification error.\nFirst Loyal CH, or customer brand loyalty for citrus hill is observed\nand split to be above or below a loyalty of 0.5036, this split then,\nagain distinguishes loyal CH. On the 2nd node on the left loyal ch is\nbelow 0.5036, it is then tested whether or not loyalty to CH is above or\nbelow 0.276, if it is below this, it is then test on the third level\nnode on the far left whether or not loyalty CH is below or above\n0.051325, in this case whether loyalty is below this or above it\n(indicating it is between 0.051325 and 0.276) the customers will always\nbe classified as being fans of minute main. Continuing from the 2nd node\non the left this time we test LoyalCH above 0.276, this leads us to the\nthird node PriceDiff < 0.05, in this case if the price difference is\nbetween the two and CH loyalty is above 0.276, the customer is\ncategorized as buying CH, however if the loyalty to CH is 0.276, but the\nprice difference is less than 0.05 then the customers is categorized as\nchoosing MM. Going to the second node on the right, LoyalCH < 0.764,\nwe categorize a customer as buying CH if their loyalty to the brand is\ngreater than 0.764. However if that node LoyalCH < 0.764 is less than\nthe stated value then we look to ListPriceDiff, in this case LoyalCH is\nbetween 0.50 and 0.764, this third node then predicts a customer as CH\nif the listed price difference is above 0.235, if the list price\ndifference is less we move to PctDiscMM or the percentage discount for\nMM, if the discount is above 0.196 the customer will choose MM, if it is\nbelow they will be categorized as choosing CH.\nPart (e)\nQuestion: Predict the response on the test data, and\nproduce a confusion matrix comparing the test labels to the predicted\ntest labels. What is the test error rate?\n\n\nset.seed(727)\n\ntest.pred <- predict(tree.OJ_train, OJ_test, type = \"class\")\n\ntable(test.pred, OJ_test$Purchase)  %>% kable()\n\n\n\nCH\nMM\nCH\n144\n40\nMM\n16\n70\n\nAnswer: The error rate is the number of incorrectly\nidentified purchasers of orange juice in terms of their actual purcahse\ncompared to whether or not they were predicted to purchase it by the\nmodel. It is calculated here as 0.2074074, or 20.7%, this is calculated\nbelow as one minus the difference between correctly identified buyers\nand incorrectly identified buyers.\n\n\n(1-(144+70)/(144+70+40+16) )  %>% kable()\n\n\nx\n0.2074074\n\nPart (f)\nQuestion: Apply the cv.tree() function to the\ntraining set in order to determine the optimal tree size.\nAnswer: The dev, or deviation represents the error\nrate. In this case it is lowest when size=8 and dev is 646.6050.\n\n\nset.seed(727)\n\ncv.OJ <- cv.tree(tree.OJ_train)\n\ncv.OJ \n\n\n$size\n[1] 8 7 6 5 4 3 2 1\n\n$dev\n[1]  646.6050  674.2620  673.7721  766.0959  770.2143  751.8258\n[7]  772.1636 1066.2872\n\n$k\n[1]      -Inf  14.04513  14.48895  36.41322  36.93239  40.72147\n[7]  50.20841 306.72764\n\n$method\n[1] \"deviance\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\nPart (g)\nQuestion: Produce a plot with tree size on the\nx-axis and cross-validated classification error rate on the y-axis.\nAnswer: Below I have plotted the tree size compared\nto the classification error rate. As can be seen 8 is much lower than\nthe tree sizes that come before them in terms of error rate.\n\n\nset.seed(727)\n\nplot(cv.OJ$size, cv.OJ$dev, type = \"b\", xlab = \"Tree Size\", ylab=\"Cross Validated Misclass\") \n\n\n\n\nPart (h)\nQuestion: Which tree size corresponds to the lowest\ncross-validated classification error rate?\nAnswer: It appears that the tree with the lowest\npossible error is 8 in this case.\nPart (i)\nQuestion: Produce a pruned tree corresponding to the\noptimal tree size obtained using cross-validation. If cross-validation\ndoes not lead to selection of a pruned tree, then create a pruned tree\nwith five terminal nodes.\n\n\nset.seed(727)\n\nprune.OJ <- prune.misclass(tree.OJ_train, best = 8)\n\nsummary(prune.OJ)\n\n\n\nClassification tree:\ntree(formula = Purchase ~ ., data = OJ, subset = train_orange_juice)\nVariables actually used in tree construction:\n[1] \"LoyalCH\"       \"PriceDiff\"     \"ListPriceDiff\" \"PctDiscMM\"    \nNumber of terminal nodes:  8 \nResidual mean deviance:  0.7145 = 565.9 / 792 \nMisclassification error rate: 0.1475 = 118 / 800 \n\nAnswer: In this case the misclassification error\nrate is 0.1475 as indicated by the summary above.\nPart (j)\nQuestion: Compare the training error rates between\nthe pruned and un-pruned trees. Which is higher?\nAnswer: The training error rate in the pruned tree\nis 0.1475, this is far lower than the unpruned rate of 0.2074074.\nPart (k)\nQuestion: Compare the test error rates between the\npruned and unpruned trees. Which is higher?\n\n\nset.seed(727)\n\nprediction_pruned <- predict(prune.OJ, OJ_test,\ntype = \"class\")\n\ntable(prediction_pruned, OJ_test$Purchase) %>% kable()\n\n\n\nCH\nMM\nCH\n144\n40\nMM\n16\n70\n\nAnswer: Here we see the misclassification error rate\nis 0.2074074, or about 20.7% this is the same as the prior rate.\n\n\n(1-(144+70)/(144+40+16+70))  %>% kable()\n\n\nx\n0.2074074\n\nPossible Data Set Final\nQuestion:\nChosen Dataset: For my final project I wanted to\ncombine machine learning techniques with a networks dataset to see if\ncertain models could be used on it. I am using data on conflict which I\nam treating as a network with node and edge attributes, using these\nattributes I think it would be interesting to see if it is possible to\nuse classification techniques like KNN, LDA, and logistic regression to\npredict centrality and see what node and edge attributes contribute to\ncentrality ranking.\n\n\n\n",
    "preview": "posts/2022-03-29-ml-homework-2/ml-homework-2_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-03-29T13:27:06-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-29-tad-post-4/",
    "title": "TAD Post 4",
    "description": "In last weeks post I combined a the material for blog posts 2, 3, and began on the 4th post. As a result this will be a continuation of dictionary validation methods.",
    "author": [
      {
        "name": "Nora Jones",
        "url": {}
      }
    ],
    "date": "2022-03-29",
    "categories": [],
    "contents": "\n\n\n\nBlog Post 3 and 4:\nThird Blog Post and\nContinuation\nExplanation: This post covers material from week 5\npre-processing, week 6 representing text, and week 7 dictionary methods.\nIn this case I have slightly modified the data I am working with, this\nweek I am using 1000 new posts for a dictionary analysis rather than the\n“top” posts to see if there is a tangible difference in content.\nInital Loading and\nProcessing\nExplanation: My code below illustrates how I\ninitially got my information from reddit by scraping. In this case I\nused RedditExtractoR. The author of this packages describes it as a\nminimalist r wrapper it scrapes a limited number of posts from reddit.\nThe api on reddit itself only allows 60 requests per minute. In this\ncase I chose posts that were “new” as of march 26 2022 at 11:17 P.M.\nThis has resulted in 980 reddit posts, this subreddit is described as\n“Firearms and related articles” and much of the subreddit is\ndescriptions of, reviews, and highlights of firearms owned by users.\n\n\n# New_guns_urls <- find_thread_urls(subreddit=\"guns\", sort_by=\"new\")\n\n\nloadRData <- function(fileName){\n#loads an RData file, and returns it\n    load(fileName)\n    get(ls()[ls() != \"fileName\"])\n}\nNew_guns_urls_df <- loadRData(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/New_reddit_posts_3_26.RData\")\n\n\n\n\n\nstr(New_guns_urls_df)\n\n\n'data.frame':   980 obs. of  7 variables:\n $ date_utc : chr  \"2022-03-15\" \"2022-03-15\" \"2022-03-15\" \"2022-03-15\" ...\n $ timestamp: num  1.65e+09 1.65e+09 1.65e+09 1.65e+09 1.65e+09 ...\n $ title    : chr  \"With tiny ar15s and mcx/similar, where does a pcc fit in the arsenal now?\" \"I still don\\031t feel like this is good enough. Ha. Anyone else this anal about this stuff?\" \"Best Home Defense Round in 5.56?\" \"5\\0365*5/555&53 5\\r50535.5\\\"5-\" ...\n $ text     : chr  \"With the advent of small ar15s and similar foldy boys like the mcx, where does the pcc fit in now?\\n\\nSeems tha\"| __truncated__ \"\" \"I'm looking for some defensive rounds for my AR-15 that are reliable. Lately I've been shooting 55 Grain XTac g\"| __truncated__ \"\" ...\n $ subreddit: chr  \"guns\" \"guns\" \"guns\" \"guns\" ...\n $ comments : num  32 23 73 12 51 29 25 13 32 7 ...\n $ url      : chr  \"https://www.reddit.com/r/guns/comments/teruk1/with_tiny_ar15s_and_mcxsimilar_where_does_a_pcc/\" \"https://www.reddit.com/r/guns/comments/ter4zi/i_still_dont_feel_like_this_is_good_enough_ha/\" \"https://www.reddit.com/r/guns/comments/ter48v/best_home_defense_round_in_556/\" \"https://www.reddit.com/r/guns/comments/teqkl1/5\\0365*5/555&53_5\\r50535.5\\\"5-/\" ...\n\nConversion From Data Frame\nto Corpus\nExplanation: Below I have processed my initial\ndataframe from Reddit into a corpus and saved a summary of the resulting\ndata.\n\n\nnew_guns_urls_df<-New_guns_urls_df[,c(\"title\", \"date_utc\", \"comments\")]\n\nnew_guns_corpus<-corpus(new_guns_urls_df$title)\n\nnew_guns_documents<-new_guns_corpus[1:10,]\n\nnew_guns_corpus_summary <- summary(new_guns_corpus)\n\n\n\nBroad Characteristics\nExplanation: In order to clean the documents for\npre-processing and analysis I have removed punctuation, converted to\nlowercase and removed stopwords. Though the language of firearms is\noften associated with punctuation, such as 5.56, 3.57, and a variety of\nother calibers, which represent the diameter of the barrel required to\nfire each ammunition. However, losing the punctuation in caliber and\nfirearm titles would not reduce their comprehensibility in analysis if\nthey retain their form as 5.56 and 556 can be considered equally while\nreducing the complexity of tokens and potentially even sentences.\nConverting the documents to lowercase can also simplifies the data.\nHowever, for my inital analysis I will just being making a lowercase\ndocument feature matrix and a more edited one.\n\n\nnew_guns_corpus_dfm_tl<-tokens(new_guns_corpus) %>%  dfm(tolower=TRUE) \n\nnew_guns_corpus_dfm_punct_tl_sw <- tokens(new_guns_corpus,\n                                    remove_punct = TRUE,) %>%\n                           dfm(tolower=TRUE) %>%\n                           dfm_remove(stopwords('english'))\n\n\n\nTop features\nExplanation: Examining the top 20 features below we\nsee a fairly predictable set of response, as may be expected from the\ngun subreddit, the most used word is gun. Rifle and pistol are also in\nthe top 20. Individual letters such as “ar”, “s”, and “m” appear\nfrequently as they are commonly used designations for types of firearms\nor model names, ar-12, ar-15, m-4, m-16, m1911, 5-mm, and s559, s-12 and\nother designations. This indicates that these numbers are valuable, if\ndifficult to comprehend on their own. With no punctuation removed the\nfirst 20 features are not informative.\n\n\ntopfeatures(new_guns_corpus_dfm_tl, 20)\n\n\n    .     ?     a   the    my     ,     i    to   for   and    is \n  425   260   259   211   183   175   175   158   112   108    99 \n   in    it     /    of  with   gun  this    on first \n   93    89    84    81    80    78    76    76    60 \n\n\n\ntopfeatures(new_guns_corpus_dfm_punct_tl_sw, 20)\n\n\n     gun    first      new       22      can        s question \n      78       60       57       43       41       35       31 \n   rifle     just      amp     help   anyone       ar   pistol \n      29       29       28       27       26       26       26 \n    time     guns     know     good      got        m \n      25       24       23       22       22       22 \n\nWorld Cloud\nExplanation: Though not necessarily statistically\ninformative, the wordcloud below can give some sense of comparative\nfrequency using the limit of minimum count being 6. Reading through\nthese can give a sense of both the communal nature of the forum in\nasking for recommendations, but also the importance of the word purchase\nand other words associated with working with, and buying firearms. In\nthe case of the only lowercased data we can gather much less\ninformation.\n\n\nset.seed(123456)\n\ntextplot_wordcloud(new_guns_corpus_dfm_tl, min_count = 12, random_order = T, rotation = 0)\n\n\n\n\n\n\nset.seed(123456)\n\ntextplot_wordcloud(new_guns_corpus_dfm_punct_tl_sw, min_count = 6, random_order = T, rotation = 0)\n\n\n\n\nTypes, Tokens, and Sentances\nTypes\nExplanation: The gun corpus summary gives 3 counting\ncategories that we can interpret in order to get a sense of the\ncomplexity of the documents that we are using. Looking at the number of\ntypes on average we see a mean of 9.18 and qunantiles that indicate a\nrange of 2-45 with 50% being between 4 and 13 types.\n\n\nmean(new_guns_corpus_summary$Types)\n\n\n[1] 9.18\n\nquantile(new_guns_corpus_summary$Types)\n\n\n  0%  25%  50%  75% 100% \n 2.0  4.0  7.5 13.0 45.0 \n\nTokens\nExplanation: Tokens are relatively similar to types\nin this case. Here there is a mean of 9.72 but a range of 2-55 with the\nmiddle 50% ranging from 4-13 tokens, as was the case for types.\n\n\nmean(new_guns_corpus_summary$Tokens)\n\n\n[1] 9.72\n\nquantile(new_guns_corpus_summary$Tokens)\n\n\n  0%  25%  50%  75% 100% \n   2    4    8   13   55 \n\nSentances\nExplanation: As is indicated below, it appears that\nthe number of sentences in each post is generally one. Arroding to the\nqunatile statistics the most sentences in any post\n\n\nmean(new_guns_corpus_summary$Sentences)\n\n\n[1] 1.18\n\nquantile(new_guns_corpus_summary$Sentences)\n\n\n  0%  25%  50%  75% 100% \n   1    1    1    1    3 \n\nWord counts\nExplanation: Looking at word counts we see a similar\ntrend reflected where including stopwords and punctuation decreases the\nquality of data as little information but punctuation and stopwords are\nincluded.\n\n\nword_counts_new_1 <- as.data.frame(sort(colSums(new_guns_corpus_dfm_tl),dec=T))\n\ncolnames(word_counts_new_1) <- c(\"Frequency\")\n\nword_counts_new_1$Rank <- c(1:ncol(new_guns_corpus_dfm_tl))\n\nhead(word_counts_new_1)\n\n\n    Frequency Rank\n.         425    1\n?         260    2\na         259    3\nthe       211    4\nmy        183    5\n,         175    6\n\n\n\nword_counts_new <- as.data.frame(sort(colSums(new_guns_corpus_dfm_punct_tl_sw),dec=T))\n\ncolnames(word_counts_new) <- c(\"Frequency\")\n\nword_counts_new$Rank <- c(1:ncol(new_guns_corpus_dfm_punct_tl_sw))\n\nhead(word_counts_new)\n\n\n      Frequency Rank\ngun          78    1\nfirst        60    2\nnew          57    3\n22           43    4\ncan          41    5\ns            35    6\n\nZipf’s Law\nExplanation: As can be seen from the frequency\ngraphs below, Ziph’s Law of inverse proportion. In this case a words\nrank in freqency is inversely prorportional to the number of times it is\nobserved. Though the uncleaned dataset has far more frequency for its\nmost common words (much of which is punctuation) it appears to follow\nthe law.\n\n\nggplot(word_counts_new, mapping = aes(x = Rank, y = Frequency)) + \n  geom_point() +\n  labs(title = \"Zipf's Law\", x = \"Rank\", y = \"Frequency\") + \n  theme_bw()\n\n\n\n\n\n\nggplot(word_counts_new_1, mapping = aes(x = Rank, y = Frequency)) + \n  geom_point() +\n  labs(title = \"Zipf's Law\", x = \"Rank\", y = \"Frequency\") + \n  theme_bw()\n\n\n\n\nData Trimming\nExplanation: Much of what I do here will be\nexplained in the code and results. Many words appear with a minimum\nfrequency of 4, though non are included in 10% and only 3 words are\nincluded in 5%. At a level of 2.5% we get 4 words.\n\n\n# First I trim the data to only include words that appear at least 4 times\n\nsmaller_dfm_4_freq <- dfm_trim(new_guns_corpus_dfm_punct_tl_sw, min_termfreq = 4)\n\n# Next I will look at proportions are see if there are words that are seen in\n# More than 10% and 5% of documents\nsmaller_dfm_10_p <- dfm_trim(smaller_dfm_4_freq, min_docfreq = 0.1, docfreq_type = \"prop\")\n\nsmaller_dfm_5_p <- dfm_trim(smaller_dfm_4_freq, min_docfreq = 0.05, docfreq_type = \"prop\")\n\nsmaller_dfm_2.5_p <- dfm_trim(smaller_dfm_4_freq, min_docfreq = 0.025,  docfreq_type = \"prop\")\n\n\n\nData Readability\nExplanation: Before making general modifications to\nthe data, it is valuable to also get a sense of readability, as in week\n5. In this case we will calculate readability scores based on 3\ndifferent measures, FOG, Coleman Liau, and Flesch Kincaid. Though this\nstep will not indicate what sort of pre-processing is best, or how the\ndata should be reduced, it does give us some insight into the complexity\nof the language in our data. In this case we just observe the\nreadability based on the post number.\n\n\nreadability_new_guns <- textstat_readability(new_guns_corpus, \n                                    measure = c(\"Flesch.Kincaid\", \"FOG\", \"Coleman.Liau.grade\")) \n\n# add in a chapter number\n\nreadability_new_guns$reddit_post <- c(1:nrow(readability_new_guns))\n\n# plot results\nggplot(readability_new_guns, aes(x = reddit_post)) +\n  geom_line(aes(y = Flesch.Kincaid), color = \"black\",  alpha=0.3) + \n  geom_line(aes(y = FOG), color = \"red\", alpha=0.3) + \n  geom_line(aes(y = Coleman.Liau.grade), color = \"blue\", alpha=0.3) + \n  theme_bw()\n\n\n\n\nExplanation: In this part we will add dates to our\ndata to see how the complexity changes over time or if it was relatively\nconstant. As can be seen below, the amount of complexity in the data\nvaries more smoothly when the data are sorted by data and not\narbitrarily by their post number, in this case all 3 complexity method\nexhibit similar trends.\n\n\nreadability_new_guns$added_dates <- as.Date(New_guns_urls_df$date_utc)\n\nggplot(readability_new_guns, aes(x = added_dates)) +\n  geom_smooth(aes(y = Flesch.Kincaid), color = \"black\") + \n  geom_smooth(aes(y = FOG), color = \"red\") + \n  geom_smooth(aes(y = Coleman.Liau.grade), color = \"blue\") + \n  theme_minimal()\n\n\n\n\nExplanation: Looking at the readability of the we\nsee that all correlations between the methods of complexity measurement\nare similar except of FOG and Coleman Liau, however the graphs above do\nindicate some similarity in trend between them, though not direct\ncorrelation in their estimates potentially.\n\n\ncor(readability_new_guns$Flesch.Kincaid, readability_new_guns$FOG, use = \"complete.obs\")\n\n\n[1] 0.8321041\n\n\n\ncor(readability_new_guns$Flesch.Kincaid, readability_new_guns$Coleman.Liau.grade, use = \"complete.obs\")\n\n\n[1] 0.751674\n\n\n\ncor(readability_new_guns$FOG, readability_new_guns$Coleman.Liau.grade, use = \"complete.obs\")\n\n\n[1] 0.5859859\n\nPre-processing\nBefore Reduction and Co-Occurance\nExplanation: Next I used the\nfactorial_preprocessing() command to both use n-grams processing and use\nan infrequent term threshold. This is in order to see what techniques,\nsuch as removing punctuation, stopwords, etc lead to a pre-text score\ndevised by Denny and Spirling. This pre-text score indicatess how many\nk-pairs of terms change the most when the pre-processing strategy is\nchanged. Lower scores indicate more usual results while higher scores\nindicate more unusual results and they are between 0 and 1. Here we have\nused n-grams and set an infreqent term threshold. Because of the nature\nof our data I will use 30% of documents as\n\n\n?factorial_preprocessing\npreprocessed_documents <- factorial_preprocessing(\n    new_guns_corpus,\n    use_ngrams = TRUE,\n    infrequent_term_threshold = 0.3,\n    verbose = FALSE)\n\n\nPreprocessing 980 documents 128 different ways...\n\n\n\nnames(preprocessed_documents)\n\n\n[1] \"choices\"  \"dfm_list\" \"labels\"  \n\nExplanation: As can be seen below the possible\nchoices are coded on the first column with each subsequent column\nindicating whether or not each choice includes each of the specified\nchoices in its assessment.\n\n\nhead(preprocessed_documents$choices)\n\n\n              removePunctuation removeNumbers lowercase stem\nP-N-L-S-W-I-3              TRUE          TRUE      TRUE TRUE\nN-L-S-W-I-3               FALSE          TRUE      TRUE TRUE\nP-L-S-W-I-3                TRUE         FALSE      TRUE TRUE\nL-S-W-I-3                 FALSE         FALSE      TRUE TRUE\nP-N-S-W-I-3                TRUE          TRUE     FALSE TRUE\nN-S-W-I-3                 FALSE          TRUE     FALSE TRUE\n              removeStopwords infrequent_terms use_ngrams\nP-N-L-S-W-I-3            TRUE             TRUE       TRUE\nN-L-S-W-I-3              TRUE             TRUE       TRUE\nP-L-S-W-I-3              TRUE             TRUE       TRUE\nL-S-W-I-3                TRUE             TRUE       TRUE\nP-N-S-W-I-3              TRUE             TRUE       TRUE\nN-S-W-I-3                TRUE             TRUE       TRUE\n\nExplanation: Next preText is calculated using 50\ncomparisons and a cosine distance calculation.\n\n\n#set.seed(12366)\n#preText_results <- preText(\n#    preprocessed_documents,\n#   dataset_name = \"Gun Pretext Results\",\n#   distance_method = \"cosine\",\n#   num_comparisons = 50,\n#  verbose = TRUE)\n\n\n\n\n\n#save(preText_results, file=\"preText_results_3_27_gun_50_comp.RData\")\n\n\n\n\n\npreText_results <- loadRData(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/preText_results_3_27_gun_50_comp.RData\")\n\npreText_results\n\n\n$preText_scores\n    preText_score preprocessing_steps\n1      0.04157825       P-N-L-S-W-I-3\n2      0.04157825         N-L-S-W-I-3\n3      0.04157825         P-L-S-W-I-3\n4      0.04157825           L-S-W-I-3\n5      0.04157825         P-N-S-W-I-3\n6      0.04157825           N-S-W-I-3\n7      0.04157825           P-S-W-I-3\n8      0.04157825             S-W-I-3\n9      0.04157825         P-N-L-W-I-3\n10     0.04157825           N-L-W-I-3\n11     0.04157825           P-L-W-I-3\n12     0.04157825             L-W-I-3\n13     0.04157825           P-N-W-I-3\n14     0.04157825             N-W-I-3\n15     0.04157825             P-W-I-3\n16     0.04157825               W-I-3\n17     0.04157825         P-N-L-S-I-3\n18     0.04157825           N-L-S-I-3\n19     0.04157825           P-L-S-I-3\n20     0.04157825             L-S-I-3\n21     0.04157825           P-N-S-I-3\n22     0.04157825             N-S-I-3\n23     0.04157825             P-S-I-3\n24     0.04157825               S-I-3\n25     0.04157825           P-N-L-I-3\n26     0.04157825             N-L-I-3\n27     0.04157825             P-L-I-3\n28     0.04157825               L-I-3\n29     0.04157825             P-N-I-3\n30     0.04157825               N-I-3\n31     0.04157825               P-I-3\n32     0.04157825                 I-3\n33     0.04815400         P-N-L-S-W-3\n34     0.08058534           N-L-S-W-3\n35     0.04403401           P-L-S-W-3\n36     0.08427290             L-S-W-3\n37     0.04864279           P-N-S-W-3\n38     0.07599185             N-S-W-3\n39     0.04470926             P-S-W-3\n40     0.08186325               S-W-3\n41     0.02723578           P-N-L-W-3\n42     0.02629912             N-L-W-3\n43     0.02280946             P-L-W-3\n44     0.02777490               L-W-3\n45     0.02723578             P-N-W-3\n46     0.02629912               N-W-3\n47     0.02280946               P-W-3\n48     0.02777490                 W-3\n49     0.02647384           P-N-L-S-3\n50     0.08304385             N-L-S-3\n51     0.02334213             P-L-S-3\n52     0.07225887               L-S-3\n53     0.02706859             P-N-S-3\n54     0.08697989               N-S-3\n55     0.02384351               P-S-3\n56     0.07767802                 S-3\n57     0.02647384             P-N-L-3\n58     0.02777727               N-L-3\n59     0.02334213               P-L-3\n60     0.01888648                 L-3\n61     0.02647384               P-N-3\n62     0.02777727                 N-3\n63     0.02334213                 P-3\n64     0.01888648                   3\n65     0.04157825         P-N-L-S-W-I\n66     0.04157825           N-L-S-W-I\n67     0.04157825           P-L-S-W-I\n68     0.04157825             L-S-W-I\n69     0.04157825           P-N-S-W-I\n70     0.04157825             N-S-W-I\n71     0.04157825             P-S-W-I\n72     0.04157825               S-W-I\n73     0.04157825           P-N-L-W-I\n74     0.04157825             N-L-W-I\n75     0.04157825             P-L-W-I\n76     0.04157825               L-W-I\n77     0.04157825             P-N-W-I\n78     0.04157825               N-W-I\n79     0.04157825               P-W-I\n80     0.04157825                 W-I\n81     0.04157825           P-N-L-S-I\n82     0.04157825             N-L-S-I\n83     0.04157825             P-L-S-I\n84     0.04157825               L-S-I\n85     0.04157825             P-N-S-I\n86     0.04157825               N-S-I\n87     0.04157825               P-S-I\n88     0.04157825                 S-I\n89     0.04157825             P-N-L-I\n90     0.04157825               N-L-I\n91     0.04157825               P-L-I\n92     0.04157825                 L-I\n93     0.04157825               P-N-I\n94     0.04157825                 N-I\n95     0.04157825                 P-I\n96     0.04157825                   I\n97     0.07109905           P-N-L-S-W\n98     0.16021925             N-L-S-W\n99     0.06767075             P-L-S-W\n100    0.17095529               L-S-W\n101    0.07098343             P-N-S-W\n102    0.15776534               N-S-W\n103    0.06802275               P-S-W\n104    0.16850136                 S-W\n105    0.02723578             P-N-L-W\n106    0.02344916               N-L-W\n107    0.02280946               P-L-W\n108    0.02739263                 L-W\n109    0.02723578               P-N-W\n110    0.02344916                 N-W\n111    0.02280946                 P-W\n112    0.02739263                   W\n113    0.05006683             P-N-L-S\n114    0.22396649               N-L-S\n115    0.04704759               P-L-S\n116    0.23717659                 L-S\n117    0.05064631               P-N-S\n118    0.21022978                 N-S\n119    0.04752479                 P-S\n120    0.23146371                   S\n121    0.02635536               P-N-L\n122    0.03527452                 N-L\n123    0.02342946                 P-L\n124    0.10226138                   L\n125    0.02635536                 P-N\n126    0.03527452                   N\n127    0.02342946                   P\n\n$ranked_preText_scores\n    preText_score preprocessing_steps\n1      0.23717659                 L-S\n2      0.23146371                   S\n3      0.22396649               N-L-S\n4      0.21022978                 N-S\n5      0.17095529               L-S-W\n6      0.16850136                 S-W\n7      0.16021925             N-L-S-W\n8      0.15776534               N-S-W\n9      0.10226138                   L\n10     0.08697989               N-S-3\n11     0.08427290             L-S-W-3\n12     0.08304385             N-L-S-3\n13     0.08186325               S-W-3\n14     0.08058534           N-L-S-W-3\n15     0.07767802                 S-3\n16     0.07599185             N-S-W-3\n17     0.07225887               L-S-3\n18     0.07109905           P-N-L-S-W\n19     0.07098343             P-N-S-W\n20     0.06802275               P-S-W\n21     0.06767075             P-L-S-W\n22     0.05064631               P-N-S\n23     0.05006683             P-N-L-S\n24     0.04864279           P-N-S-W-3\n25     0.04815400         P-N-L-S-W-3\n26     0.04752479                 P-S\n27     0.04704759               P-L-S\n28     0.04470926             P-S-W-3\n29     0.04403401           P-L-S-W-3\n30     0.04157825       P-N-L-S-W-I-3\n31     0.04157825         N-L-S-W-I-3\n32     0.04157825         P-L-S-W-I-3\n33     0.04157825           L-S-W-I-3\n34     0.04157825         P-N-S-W-I-3\n35     0.04157825           N-S-W-I-3\n36     0.04157825           P-S-W-I-3\n37     0.04157825             S-W-I-3\n38     0.04157825         P-N-L-W-I-3\n39     0.04157825           N-L-W-I-3\n40     0.04157825           P-L-W-I-3\n41     0.04157825             L-W-I-3\n42     0.04157825           P-N-W-I-3\n43     0.04157825             N-W-I-3\n44     0.04157825             P-W-I-3\n45     0.04157825               W-I-3\n46     0.04157825         P-N-L-S-I-3\n47     0.04157825           N-L-S-I-3\n48     0.04157825           P-L-S-I-3\n49     0.04157825             L-S-I-3\n50     0.04157825           P-N-S-I-3\n51     0.04157825             N-S-I-3\n52     0.04157825             P-S-I-3\n53     0.04157825               S-I-3\n54     0.04157825           P-N-L-I-3\n55     0.04157825             N-L-I-3\n56     0.04157825             P-L-I-3\n57     0.04157825               L-I-3\n58     0.04157825             P-N-I-3\n59     0.04157825               N-I-3\n60     0.04157825               P-I-3\n61     0.04157825                 I-3\n62     0.04157825         P-N-L-S-W-I\n63     0.04157825           N-L-S-W-I\n64     0.04157825           P-L-S-W-I\n65     0.04157825             L-S-W-I\n66     0.04157825           P-N-S-W-I\n67     0.04157825             N-S-W-I\n68     0.04157825             P-S-W-I\n69     0.04157825               S-W-I\n70     0.04157825           P-N-L-W-I\n71     0.04157825             N-L-W-I\n72     0.04157825             P-L-W-I\n73     0.04157825               L-W-I\n74     0.04157825             P-N-W-I\n75     0.04157825               N-W-I\n76     0.04157825               P-W-I\n77     0.04157825                 W-I\n78     0.04157825           P-N-L-S-I\n79     0.04157825             N-L-S-I\n80     0.04157825             P-L-S-I\n81     0.04157825               L-S-I\n82     0.04157825             P-N-S-I\n83     0.04157825               N-S-I\n84     0.04157825               P-S-I\n85     0.04157825                 S-I\n86     0.04157825             P-N-L-I\n87     0.04157825               N-L-I\n88     0.04157825               P-L-I\n89     0.04157825                 L-I\n90     0.04157825               P-N-I\n91     0.04157825                 N-I\n92     0.04157825                 P-I\n93     0.04157825                   I\n94     0.03527452                 N-L\n95     0.03527452                   N\n96     0.02777727               N-L-3\n97     0.02777727                 N-3\n98     0.02777490               L-W-3\n99     0.02777490                 W-3\n100    0.02739263                 L-W\n101    0.02739263                   W\n102    0.02723578           P-N-L-W-3\n103    0.02723578             P-N-W-3\n104    0.02723578             P-N-L-W\n105    0.02723578               P-N-W\n106    0.02706859             P-N-S-3\n107    0.02647384           P-N-L-S-3\n108    0.02647384             P-N-L-3\n109    0.02647384               P-N-3\n110    0.02635536               P-N-L\n111    0.02635536                 P-N\n112    0.02629912             N-L-W-3\n113    0.02629912               N-W-3\n114    0.02384351               P-S-3\n115    0.02344916               N-L-W\n116    0.02344916                 N-W\n117    0.02342946                 P-L\n118    0.02342946                   P\n119    0.02334213             P-L-S-3\n120    0.02334213               P-L-3\n121    0.02334213                 P-3\n122    0.02280946             P-L-W-3\n123    0.02280946               P-W-3\n124    0.02280946               P-L-W\n125    0.02280946                 P-W\n126    0.01888648                 L-3\n127    0.01888648                   3\n\n$choices\n              removePunctuation removeNumbers lowercase  stem\nP-N-L-S-W-I-3              TRUE          TRUE      TRUE  TRUE\nN-L-S-W-I-3               FALSE          TRUE      TRUE  TRUE\nP-L-S-W-I-3                TRUE         FALSE      TRUE  TRUE\nL-S-W-I-3                 FALSE         FALSE      TRUE  TRUE\nP-N-S-W-I-3                TRUE          TRUE     FALSE  TRUE\nN-S-W-I-3                 FALSE          TRUE     FALSE  TRUE\nP-S-W-I-3                  TRUE         FALSE     FALSE  TRUE\nS-W-I-3                   FALSE         FALSE     FALSE  TRUE\nP-N-L-W-I-3                TRUE          TRUE      TRUE FALSE\nN-L-W-I-3                 FALSE          TRUE      TRUE FALSE\nP-L-W-I-3                  TRUE         FALSE      TRUE FALSE\nL-W-I-3                   FALSE         FALSE      TRUE FALSE\nP-N-W-I-3                  TRUE          TRUE     FALSE FALSE\nN-W-I-3                   FALSE          TRUE     FALSE FALSE\nP-W-I-3                    TRUE         FALSE     FALSE FALSE\nW-I-3                     FALSE         FALSE     FALSE FALSE\nP-N-L-S-I-3                TRUE          TRUE      TRUE  TRUE\nN-L-S-I-3                 FALSE          TRUE      TRUE  TRUE\nP-L-S-I-3                  TRUE         FALSE      TRUE  TRUE\nL-S-I-3                   FALSE         FALSE      TRUE  TRUE\nP-N-S-I-3                  TRUE          TRUE     FALSE  TRUE\nN-S-I-3                   FALSE          TRUE     FALSE  TRUE\nP-S-I-3                    TRUE         FALSE     FALSE  TRUE\nS-I-3                     FALSE         FALSE     FALSE  TRUE\nP-N-L-I-3                  TRUE          TRUE      TRUE FALSE\nN-L-I-3                   FALSE          TRUE      TRUE FALSE\nP-L-I-3                    TRUE         FALSE      TRUE FALSE\nL-I-3                     FALSE         FALSE      TRUE FALSE\nP-N-I-3                    TRUE          TRUE     FALSE FALSE\nN-I-3                     FALSE          TRUE     FALSE FALSE\nP-I-3                      TRUE         FALSE     FALSE FALSE\nI-3                       FALSE         FALSE     FALSE FALSE\nP-N-L-S-W-3                TRUE          TRUE      TRUE  TRUE\nN-L-S-W-3                 FALSE          TRUE      TRUE  TRUE\nP-L-S-W-3                  TRUE         FALSE      TRUE  TRUE\nL-S-W-3                   FALSE         FALSE      TRUE  TRUE\nP-N-S-W-3                  TRUE          TRUE     FALSE  TRUE\nN-S-W-3                   FALSE          TRUE     FALSE  TRUE\nP-S-W-3                    TRUE         FALSE     FALSE  TRUE\nS-W-3                     FALSE         FALSE     FALSE  TRUE\nP-N-L-W-3                  TRUE          TRUE      TRUE FALSE\nN-L-W-3                   FALSE          TRUE      TRUE FALSE\nP-L-W-3                    TRUE         FALSE      TRUE FALSE\nL-W-3                     FALSE         FALSE      TRUE FALSE\nP-N-W-3                    TRUE          TRUE     FALSE FALSE\nN-W-3                     FALSE          TRUE     FALSE FALSE\nP-W-3                      TRUE         FALSE     FALSE FALSE\nW-3                       FALSE         FALSE     FALSE FALSE\nP-N-L-S-3                  TRUE          TRUE      TRUE  TRUE\nN-L-S-3                   FALSE          TRUE      TRUE  TRUE\nP-L-S-3                    TRUE         FALSE      TRUE  TRUE\nL-S-3                     FALSE         FALSE      TRUE  TRUE\nP-N-S-3                    TRUE          TRUE     FALSE  TRUE\nN-S-3                     FALSE          TRUE     FALSE  TRUE\nP-S-3                      TRUE         FALSE     FALSE  TRUE\nS-3                       FALSE         FALSE     FALSE  TRUE\nP-N-L-3                    TRUE          TRUE      TRUE FALSE\nN-L-3                     FALSE          TRUE      TRUE FALSE\nP-L-3                      TRUE         FALSE      TRUE FALSE\nL-3                       FALSE         FALSE      TRUE FALSE\nP-N-3                      TRUE          TRUE     FALSE FALSE\nN-3                       FALSE          TRUE     FALSE FALSE\nP-3                        TRUE         FALSE     FALSE FALSE\n3                         FALSE         FALSE     FALSE FALSE\nP-N-L-S-W-I                TRUE          TRUE      TRUE  TRUE\nN-L-S-W-I                 FALSE          TRUE      TRUE  TRUE\nP-L-S-W-I                  TRUE         FALSE      TRUE  TRUE\nL-S-W-I                   FALSE         FALSE      TRUE  TRUE\nP-N-S-W-I                  TRUE          TRUE     FALSE  TRUE\nN-S-W-I                   FALSE          TRUE     FALSE  TRUE\nP-S-W-I                    TRUE         FALSE     FALSE  TRUE\nS-W-I                     FALSE         FALSE     FALSE  TRUE\nP-N-L-W-I                  TRUE          TRUE      TRUE FALSE\nN-L-W-I                   FALSE          TRUE      TRUE FALSE\nP-L-W-I                    TRUE         FALSE      TRUE FALSE\nL-W-I                     FALSE         FALSE      TRUE FALSE\nP-N-W-I                    TRUE          TRUE     FALSE FALSE\nN-W-I                     FALSE          TRUE     FALSE FALSE\nP-W-I                      TRUE         FALSE     FALSE FALSE\nW-I                       FALSE         FALSE     FALSE FALSE\nP-N-L-S-I                  TRUE          TRUE      TRUE  TRUE\nN-L-S-I                   FALSE          TRUE      TRUE  TRUE\nP-L-S-I                    TRUE         FALSE      TRUE  TRUE\nL-S-I                     FALSE         FALSE      TRUE  TRUE\nP-N-S-I                    TRUE          TRUE     FALSE  TRUE\nN-S-I                     FALSE          TRUE     FALSE  TRUE\nP-S-I                      TRUE         FALSE     FALSE  TRUE\nS-I                       FALSE         FALSE     FALSE  TRUE\nP-N-L-I                    TRUE          TRUE      TRUE FALSE\nN-L-I                     FALSE          TRUE      TRUE FALSE\nP-L-I                      TRUE         FALSE      TRUE FALSE\nL-I                       FALSE         FALSE      TRUE FALSE\nP-N-I                      TRUE          TRUE     FALSE FALSE\nN-I                       FALSE          TRUE     FALSE FALSE\nP-I                        TRUE         FALSE     FALSE FALSE\nI                         FALSE         FALSE     FALSE FALSE\nP-N-L-S-W                  TRUE          TRUE      TRUE  TRUE\nN-L-S-W                   FALSE          TRUE      TRUE  TRUE\nP-L-S-W                    TRUE         FALSE      TRUE  TRUE\nL-S-W                     FALSE         FALSE      TRUE  TRUE\nP-N-S-W                    TRUE          TRUE     FALSE  TRUE\nN-S-W                     FALSE          TRUE     FALSE  TRUE\nP-S-W                      TRUE         FALSE     FALSE  TRUE\nS-W                       FALSE         FALSE     FALSE  TRUE\nP-N-L-W                    TRUE          TRUE      TRUE FALSE\nN-L-W                     FALSE          TRUE      TRUE FALSE\nP-L-W                      TRUE         FALSE      TRUE FALSE\nL-W                       FALSE         FALSE      TRUE FALSE\nP-N-W                      TRUE          TRUE     FALSE FALSE\nN-W                       FALSE          TRUE     FALSE FALSE\nP-W                        TRUE         FALSE     FALSE FALSE\nW                         FALSE         FALSE     FALSE FALSE\nP-N-L-S                    TRUE          TRUE      TRUE  TRUE\nN-L-S                     FALSE          TRUE      TRUE  TRUE\nP-L-S                      TRUE         FALSE      TRUE  TRUE\nL-S                       FALSE         FALSE      TRUE  TRUE\nP-N-S                      TRUE          TRUE     FALSE  TRUE\nN-S                       FALSE          TRUE     FALSE  TRUE\nP-S                        TRUE         FALSE     FALSE  TRUE\nS                         FALSE         FALSE     FALSE  TRUE\nP-N-L                      TRUE          TRUE      TRUE FALSE\nN-L                       FALSE          TRUE      TRUE FALSE\nP-L                        TRUE         FALSE      TRUE FALSE\nL                         FALSE         FALSE      TRUE FALSE\nP-N                        TRUE          TRUE     FALSE FALSE\nN                         FALSE          TRUE     FALSE FALSE\nP                          TRUE         FALSE     FALSE FALSE\n                          FALSE         FALSE     FALSE FALSE\n              removeStopwords infrequent_terms use_ngrams\nP-N-L-S-W-I-3            TRUE             TRUE       TRUE\nN-L-S-W-I-3              TRUE             TRUE       TRUE\nP-L-S-W-I-3              TRUE             TRUE       TRUE\nL-S-W-I-3                TRUE             TRUE       TRUE\nP-N-S-W-I-3              TRUE             TRUE       TRUE\nN-S-W-I-3                TRUE             TRUE       TRUE\nP-S-W-I-3                TRUE             TRUE       TRUE\nS-W-I-3                  TRUE             TRUE       TRUE\nP-N-L-W-I-3              TRUE             TRUE       TRUE\nN-L-W-I-3                TRUE             TRUE       TRUE\nP-L-W-I-3                TRUE             TRUE       TRUE\nL-W-I-3                  TRUE             TRUE       TRUE\nP-N-W-I-3                TRUE             TRUE       TRUE\nN-W-I-3                  TRUE             TRUE       TRUE\nP-W-I-3                  TRUE             TRUE       TRUE\nW-I-3                    TRUE             TRUE       TRUE\nP-N-L-S-I-3             FALSE             TRUE       TRUE\nN-L-S-I-3               FALSE             TRUE       TRUE\nP-L-S-I-3               FALSE             TRUE       TRUE\nL-S-I-3                 FALSE             TRUE       TRUE\nP-N-S-I-3               FALSE             TRUE       TRUE\nN-S-I-3                 FALSE             TRUE       TRUE\nP-S-I-3                 FALSE             TRUE       TRUE\nS-I-3                   FALSE             TRUE       TRUE\nP-N-L-I-3               FALSE             TRUE       TRUE\nN-L-I-3                 FALSE             TRUE       TRUE\nP-L-I-3                 FALSE             TRUE       TRUE\nL-I-3                   FALSE             TRUE       TRUE\nP-N-I-3                 FALSE             TRUE       TRUE\nN-I-3                   FALSE             TRUE       TRUE\nP-I-3                   FALSE             TRUE       TRUE\nI-3                     FALSE             TRUE       TRUE\nP-N-L-S-W-3              TRUE            FALSE       TRUE\nN-L-S-W-3                TRUE            FALSE       TRUE\nP-L-S-W-3                TRUE            FALSE       TRUE\nL-S-W-3                  TRUE            FALSE       TRUE\nP-N-S-W-3                TRUE            FALSE       TRUE\nN-S-W-3                  TRUE            FALSE       TRUE\nP-S-W-3                  TRUE            FALSE       TRUE\nS-W-3                    TRUE            FALSE       TRUE\nP-N-L-W-3                TRUE            FALSE       TRUE\nN-L-W-3                  TRUE            FALSE       TRUE\nP-L-W-3                  TRUE            FALSE       TRUE\nL-W-3                    TRUE            FALSE       TRUE\nP-N-W-3                  TRUE            FALSE       TRUE\nN-W-3                    TRUE            FALSE       TRUE\nP-W-3                    TRUE            FALSE       TRUE\nW-3                      TRUE            FALSE       TRUE\nP-N-L-S-3               FALSE            FALSE       TRUE\nN-L-S-3                 FALSE            FALSE       TRUE\nP-L-S-3                 FALSE            FALSE       TRUE\nL-S-3                   FALSE            FALSE       TRUE\nP-N-S-3                 FALSE            FALSE       TRUE\nN-S-3                   FALSE            FALSE       TRUE\nP-S-3                   FALSE            FALSE       TRUE\nS-3                     FALSE            FALSE       TRUE\nP-N-L-3                 FALSE            FALSE       TRUE\nN-L-3                   FALSE            FALSE       TRUE\nP-L-3                   FALSE            FALSE       TRUE\nL-3                     FALSE            FALSE       TRUE\nP-N-3                   FALSE            FALSE       TRUE\nN-3                     FALSE            FALSE       TRUE\nP-3                     FALSE            FALSE       TRUE\n3                       FALSE            FALSE       TRUE\nP-N-L-S-W-I              TRUE             TRUE      FALSE\nN-L-S-W-I                TRUE             TRUE      FALSE\nP-L-S-W-I                TRUE             TRUE      FALSE\nL-S-W-I                  TRUE             TRUE      FALSE\nP-N-S-W-I                TRUE             TRUE      FALSE\nN-S-W-I                  TRUE             TRUE      FALSE\nP-S-W-I                  TRUE             TRUE      FALSE\nS-W-I                    TRUE             TRUE      FALSE\nP-N-L-W-I                TRUE             TRUE      FALSE\nN-L-W-I                  TRUE             TRUE      FALSE\nP-L-W-I                  TRUE             TRUE      FALSE\nL-W-I                    TRUE             TRUE      FALSE\nP-N-W-I                  TRUE             TRUE      FALSE\nN-W-I                    TRUE             TRUE      FALSE\nP-W-I                    TRUE             TRUE      FALSE\nW-I                      TRUE             TRUE      FALSE\nP-N-L-S-I               FALSE             TRUE      FALSE\nN-L-S-I                 FALSE             TRUE      FALSE\nP-L-S-I                 FALSE             TRUE      FALSE\nL-S-I                   FALSE             TRUE      FALSE\nP-N-S-I                 FALSE             TRUE      FALSE\nN-S-I                   FALSE             TRUE      FALSE\nP-S-I                   FALSE             TRUE      FALSE\nS-I                     FALSE             TRUE      FALSE\nP-N-L-I                 FALSE             TRUE      FALSE\nN-L-I                   FALSE             TRUE      FALSE\nP-L-I                   FALSE             TRUE      FALSE\nL-I                     FALSE             TRUE      FALSE\nP-N-I                   FALSE             TRUE      FALSE\nN-I                     FALSE             TRUE      FALSE\nP-I                     FALSE             TRUE      FALSE\nI                       FALSE             TRUE      FALSE\nP-N-L-S-W                TRUE            FALSE      FALSE\nN-L-S-W                  TRUE            FALSE      FALSE\nP-L-S-W                  TRUE            FALSE      FALSE\nL-S-W                    TRUE            FALSE      FALSE\nP-N-S-W                  TRUE            FALSE      FALSE\nN-S-W                    TRUE            FALSE      FALSE\nP-S-W                    TRUE            FALSE      FALSE\nS-W                      TRUE            FALSE      FALSE\nP-N-L-W                  TRUE            FALSE      FALSE\nN-L-W                    TRUE            FALSE      FALSE\nP-L-W                    TRUE            FALSE      FALSE\nL-W                      TRUE            FALSE      FALSE\nP-N-W                    TRUE            FALSE      FALSE\nN-W                      TRUE            FALSE      FALSE\nP-W                      TRUE            FALSE      FALSE\nW                        TRUE            FALSE      FALSE\nP-N-L-S                 FALSE            FALSE      FALSE\nN-L-S                   FALSE            FALSE      FALSE\nP-L-S                   FALSE            FALSE      FALSE\nL-S                     FALSE            FALSE      FALSE\nP-N-S                   FALSE            FALSE      FALSE\nN-S                     FALSE            FALSE      FALSE\nP-S                     FALSE            FALSE      FALSE\nS                       FALSE            FALSE      FALSE\nP-N-L                   FALSE            FALSE      FALSE\nN-L                     FALSE            FALSE      FALSE\nP-L                     FALSE            FALSE      FALSE\nL                       FALSE            FALSE      FALSE\nP-N                     FALSE            FALSE      FALSE\nN                       FALSE            FALSE      FALSE\nP                       FALSE            FALSE      FALSE\n                        FALSE            FALSE      FALSE\n\n$regression_results\n    Coefficient          SE                Variable\n1  0.0692437554 0.008765654               Intercept\n2 -0.0248352010 0.006026387      Remove Punctuation\n3 -0.0012912608 0.006026387          Remove Numbers\n4  0.0008042152 0.006026387               Lowercase\n5  0.0315263364 0.006026387                Stemming\n6 -0.0031236074 0.006026387        Remove Stopwords\n7 -0.0194667091 0.006026387 Remove Infrequent Terms\n8 -0.0194780796 0.006026387              Use NGrams\n                Model\n1 Gun Pretext Results\n2 Gun Pretext Results\n3 Gun Pretext Results\n4 Gun Pretext Results\n5 Gun Pretext Results\n6 Gun Pretext Results\n7 Gun Pretext Results\n8 Gun Pretext Results\n\n\n\n#load(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/preText_results_gun_50_comps.RData\")\n\npreText_score_plot(preText_results)\n\n\n\n\nExplanation: After plotting we access the pretext\nscore with the minimum score, which is least unusual. This is the row\nwith the pre-processing steps refered to as “3” in the data. In addition\nL-3 results in the same preText Score.\n\n\nscores_new_pretext<-preText_results$preText_score \n\n# head(sort(scores_new_pretext))\n\n\n\nExplanation: Looking at the choices below I see that\n“3” does not do anything but use n-grams. L-3 does use lowercase and\nn-grams.\n\n\n# preprocessed_documents$choices\n\n\n\nExplanation Continued: Looking at the regression\ncoefficients we see negative scores as usual results and positive\ncoefficients as unusual ones. In this case removing puncuation,\nstopwords, and n-grams would not lead to a great deal of abnormality.\nThe scores below indicate that stemming would result in the most\nabnormality while all others but lowercase is the only other that has a\nnon-negative coefficinet.\n\n\nregression_coefficient_plot(preText_results,\n                            remove_intercept = TRUE)\n\n\n\n\nFeature Co-occurance Matrix\nExplanation: The feature co-occurance matrix can\ngive us a sense of which words in the dataset are occurring together\n\n\n# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of chapters\nsmaller_dfm <- dfm_trim(new_guns_corpus_dfm_punct_tl_sw, min_termfreq = 5)\n\n#smaller_dfm <- dfm_trim(smaller_dfm, min_docfreq = .3, docfreq_type = \"prop\")\n\n# create fcm from dfm\nsmaller_fcm <- fcm(smaller_dfm)\n\n# check the dimensions (i.e., the number of rows and the number of columnns)\n# of the matrix we created\ndim(smaller_fcm)\n\n\n[1] 226 226\n\n\n\n# pull the top features\nmyFeatures <- names(topfeatures(smaller_fcm, 40))\n\n# retain only those top features as part of our matrix\neven_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = \"keep\")\n\n# check dimensions\ndim(even_smaller_fcm)\n\n\n[1] 40 40\n\n# compute size weight for vertices in network\nsize <- log(colSums(even_smaller_fcm))\n\n# create plot\ntextplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)\n\n\n\n\nSentiment Results Using NRC\n\n\n# get_sentiments(\"nrc\")\n# get_sentiments(\"bing\")\n# get_sentiments(\"afinn\")\n\n\n\n\n\nsentimetnsdf <- read_csv(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/sentimetnsdf.csv\")\n\n\n\n\n\nnew_guns_urls_df_2<-new_guns_urls_df\n\nnew_guns_urls_df_2$text<- seq(1, 980, by=1)\n\nnrc_joy <- sentimetnsdf %>% \n  filter(sentiment == \"joy\")\n\ntidy_posts_for_guns <- new_guns_urls_df_2 %>%\n  unnest_tokens(word, title) \n\ntidy_posts_for_guns %>%\n  inner_join(nrc_joy) %>%\n  count(word, sort = TRUE) %>% head() %>% kable()\n\n\nword\nn\ngood\n22\nlove\n12\nfinally\n11\nsafe\n10\nfun\n7\nfavorite\n6\n\n\n\ntidy_posts_for_guns_sentiment <- tidy_posts_for_guns %>%\n  inner_join(sentimetnsdf) %>%\n  count(text, sentiment) %>%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% \n  mutate(sentiment = positive - negative)\n\n\n\n\n\nnrc_sentiment <- get_sentiments(\"nrc\")\n\n\nnrc_guns_word_counts <- tidy_posts_for_guns %>%\n  inner_join(nrc_sentiment) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  ungroup()\n\nnrc_guns_word_counts %>%\n  group_by(sentiment) %>%\n  slice_max(n, n = 10) %>% \n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL)\n\n\n\n\n\n\nBing_sentiments<-get_sentiments(\"bing\")\n\ntidy_posts_for_guns_sentiment <- tidy_posts_for_guns %>%\n  inner_join(Bing_sentiments) %>%\n  count(text, sentiment) %>%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% \n  mutate(sentiment = positive - negative)\n\nbing_word_counts <- tidy_posts_for_guns %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  ungroup()\n\nbing_word_counts %>%\n  group_by(sentiment) %>%\n  slice_max(n, n = 10) %>% \n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL)\n\n\n\n\n\n\ntidy_posts_for_guns$added_dates <- as.Date(tidy_posts_for_guns$date_utc)\n\n\nafinn <- tidy_posts_for_guns %>% \n  inner_join(get_sentiments(\"afinn\")) %>% \n  group_by(index = added_dates) %>% \n  summarise(sentiment = sum(value)) %>% \n  mutate(method = \"AFINN\")\nafinn\n\n\n# A tibble: 14 × 3\n   index      sentiment method\n   <date>         <dbl> <chr> \n 1 2022-03-14         9 AFINN \n 2 2022-03-15        17 AFINN \n 3 2022-03-16         0 AFINN \n 4 2022-03-17        23 AFINN \n 5 2022-03-18         7 AFINN \n 6 2022-03-19        30 AFINN \n 7 2022-03-20        13 AFINN \n 8 2022-03-21        11 AFINN \n 9 2022-03-22        19 AFINN \n10 2022-03-23         8 AFINN \n11 2022-03-24        32 AFINN \n12 2022-03-25         7 AFINN \n13 2022-03-26         6 AFINN \n14 2022-03-27         0 AFINN \n\n\n\nafinn %>%\n  ggplot(aes(index, sentiment, fill = method)) +\n  geom_col(show.legend = FALSE,   width = 0.7)  + \n  geom_smooth(aes(y = sentiment), color = \"black\")+\nfacet_wrap(~method, ncol = 1, scales = \"free_y\")+\n  theme_minimal()\n\n\n\n\nSentiment Results Using BING\nExplanation: Using nrc appears to have had some\nunintended effects that may require an analysis of the specific words\nused to describe sentiment. One difficult part of the data being used is\nthat firearms, and the words used to describe them, are percieved\n\n\nlibrary(methods)\n\ntoo_gun_dfm<- quanteda::dfm(new_guns_corpus, verbose = FALSE)\n\ntoo_gun_dfm\n\n\nDocument-feature matrix of: 980 documents, 2,584 features (99.64% sparse) and 0 docvars.\n       features\ndocs    with tiny ar15s and mcx / similar , where does\n  text1    1    1     1   1   1 1       1 1     1    1\n  text2    0    0     0   0   0 0       0 0     0    0\n  text3    0    0     0   0   0 0       0 0     0    0\n  text4    0    0     0   0   0 1       0 0     0    0\n  text5    0    0     0   0   0 0       0 0     0    0\n  text6    1    0     0   1   0 0       0 0     0    0\n[ reached max_ndoc ... 974 more documents, reached max_nfeat ... 2,574 more features ]\n\n\n\nlibrary(topicmodels)\n\ngun_dfm_lda <- LDA(too_gun_dfm, k = 2, control = list(seed = 777))\n\n\n\n\n\ngun_dfm_lda_topics <- tidy(gun_dfm_lda, matrix = \"beta\")\n\n\n\n\n\ngun_top_terms <- gun_dfm_lda_topics %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>% \n  ungroup() %>%\n  arrange(topic, -beta)\n\ngun_top_terms %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\n\nlibrary(topicmodels)\n\ngun_dfm_lda <- LDA(too_gun_dfm, k = 4, control = list(seed = 777))\n\n\n\n\n\ngun_dfm_lda_topics <- tidy(gun_dfm_lda, matrix = \"beta\")\n\n\n\n\n\ngun_top_terms <- gun_dfm_lda_topics %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>% \n  ungroup() %>%\n  arrange(topic, -beta)\n\ngun_top_terms %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\n\nbeta_wide <- gun_dfm_lda_topics %>%\n  mutate(topic = paste0(\"topic\", topic)) %>%\n  pivot_wider(names_from = topic, values_from = beta) %>% \n  filter(topic1 > .001 | topic2 > .001) %>%\n  mutate(log_ratio = log2(topic2 / topic1))\n\n\n\n\n\nbeta_wide %>%\n  group_by(direction = log_ratio > 0) %>%\n  slice_max(abs(log_ratio), n = 10) %>% \n  ungroup() %>%\n  mutate(term = reorder(term, log_ratio)) %>%\n  ggplot(aes(log_ratio, term)) +\n  geom_col() +\n  labs(x = \"Log2 ratio of beta in topic 2 / topic 1\", y = NULL)\n\n\n\n\nTopic Modeling analysis\nResponse: As can be seen above topic modeling may\nbenefit from some data reduction, removing punctuation and stop words\nwould likely be beneficial as can be seen above where a number of the\ndifferences between topics are modeled as punctuation and stop\nwords.\n\n\ngun_tokens_stopwords_and_punct_removed <- tokens_remove(tokens(new_guns_corpus, remove_punct = TRUE), c(stopwords(\"english\")))\n\ngun_corpus_stopwords_and_punct_removed <- corpus(sapply(gun_tokens_stopwords_and_punct_removed, paste, collapse=\" \")\n)\n\ngun_corpus_stopwords_and_punct_removed\n\n\nCorpus consisting of 980 documents.\ntext1 :\n\"tiny ar15s mcx similar pcc fit arsenal now\"\n\ntext2 :\n\"still don t feel like good enough Ha Anyone else anal stuff\"\n\ntext3 :\n\"Best Home Defense Round 5.56\"\n\ntext4 :\n\"5 5 5 555 53 5 50535.5 5-\"\n\ntext5 :\n\"suppressed 12.5 AR size 14.5 AR\"\n\ntext6 :\n\"Springfield Hellcat RDP SilencerCo Omega 9k Streamlight TLR-...\"\n\n[ reached max_ndoc ... 974 more documents ]\n\n\n\nlibrary(methods)\n\ntoo_gun_dfm_no_punct_stopwords<- quanteda::dfm(tokens(gun_corpus_stopwords_and_punct_removed), verbose = FALSE)\n\n\ntoo_gun_dfm_no_punct_stopwords<-too_gun_dfm_no_punct_stopwords[rowSums(too_gun_dfm_no_punct_stopwords[])>0,]\n\ntoo_gun_dfm_no_punct_stopwords\n\n\nDocument-feature matrix of: 979 documents, 2,434 features (99.77% sparse) and 0 docvars.\n       features\ndocs    tiny ar15s mcx similar pcc fit arsenal now still don\n  text1    1     1   1       1   1   1       1   1     0   0\n  text2    0     0   0       0   0   0       0   0     1   1\n  text3    0     0   0       0   0   0       0   0     0   0\n  text4    0     0   0       0   0   0       0   0     0   0\n  text5    0     0   0       0   0   0       0   0     0   0\n  text6    0     0   0       0   0   0       0   0     0   0\n[ reached max_ndoc ... 973 more documents, reached max_nfeat ... 2,424 more features ]\n\n\n\nlibrary(topicmodels)\n\ngun_dfm_lda_nopunct_stop <- LDA(too_gun_dfm_no_punct_stopwords, k = 2, control = list(seed = 777))\n\ngun_dfm_lda_nopunct_stop\n\n\nA LDA_VEM topic model with 2 topics.\n\n\n\ngun_dfm_lda_topics_nopunct_stop <- tidy(gun_dfm_lda_nopunct_stop, matrix = \"beta\")\n\ngun_dfm_lda_topics_nopunct_stop\n\n\n# A tibble: 4,868 × 3\n   topic term         beta\n   <int> <chr>       <dbl>\n 1     1 tiny    0.000328 \n 2     2 tiny    0.0000310\n 3     1 ar15s   0.000207 \n 4     2 ar15s   0.000151 \n 5     1 mcx     0.000337 \n 6     2 mcx     0.000381 \n 7     1 similar 0.000407 \n 8     2 similar 0.000311 \n 9     1 pcc     0.00140  \n10     2 pcc     0.0000308\n# … with 4,858 more rows\n\n\n\ngun_top_terms_no_punct_or_stop<- gun_dfm_lda_topics_nopunct_stop %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>% \n  ungroup() %>%\n  arrange(topic, -beta)\n\ngun_top_terms_no_punct_or_stop %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\n\nbeta_wide_no_punct_stop <- gun_dfm_lda_topics_nopunct_stop %>%\n  mutate(topic = paste0(\"topic\", topic)) %>%\n  pivot_wider(names_from = topic, values_from = beta) %>% \n  filter(topic1 > .001 | topic2 > .001) %>%\n  mutate(log_ratio = log2(topic2 / topic1))\n\n\n\n\n\nbeta_wide_no_punct_stop %>%\n  group_by(direction = log_ratio > 0) %>%\n  slice_max(abs(log_ratio), n = 10) %>% \n  ungroup() %>%\n  mutate(term = reorder(term, log_ratio)) %>%\n  ggplot(aes(log_ratio, term)) +\n  geom_col() +\n  labs(x = \"Log2 ratio of beta in topic 2 / topic 1\", y = NULL)\n\n\n\n\nTokens and Corpus Work\n\n\ntop_guns_tokens <- tokens(new_guns_corpus)\n\nprint(top_guns_tokens)\n\n\nTokens consisting of 980 documents.\ntext1 :\n [1] \"With\"    \"tiny\"    \"ar15s\"   \"and\"     \"mcx\"     \"/\"      \n [7] \"similar\" \",\"       \"where\"   \"does\"    \"a\"       \"pcc\"    \n[ ... and 6 more ]\n\ntext2 :\n [1] \"I\"      \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"this\"  \n [8] \"is\"     \"good\"   \"enough\" \".\"      \"Ha\"    \n[ ... and 9 more ]\n\ntext3 :\n[1] \"Best\"    \"Home\"    \"Defense\" \"Round\"   \"in\"      \"5.56\"   \n[7] \"?\"      \n\ntext4 :\n [1] \"5\"       \"5\"       \"*\"       \"5\"       \"/\"       \"555\"    \n [7] \"&\"       \"53\"      \"5\"       \"50535.5\" \"\\\"\"      \"5-\"     \n\ntext5 :\n [1] \"My\"         \"suppressed\" \"12.5\"       \"AR\"         \"is\"        \n [6] \"the\"        \"same\"       \"size\"       \"as\"         \"my\"        \n[11] \"14.5\"       \"\\\"\"        \n[ ... and 1 more ]\n\ntext6 :\n [1] \"Springfield\" \"Hellcat\"     \"RDP\"         \"with\"       \n [5] \"SilencerCo\"  \"Omega\"       \"9k\"          \"and\"        \n [9] \"Streamlight\" \"TLR-7\"       \".\"          \n\n[ reached max_ndoc ... 974 more documents ]\n\ntop_guns_tokens_no_punct <- tokens(new_guns_corpus, \n    remove_punct = T)\n\nprint(top_guns_tokens_no_punct)\n\n\nTokens consisting of 980 documents.\ntext1 :\n [1] \"With\"    \"tiny\"    \"ar15s\"   \"and\"     \"mcx\"     \"similar\"\n [7] \"where\"   \"does\"    \"a\"       \"pcc\"     \"fit\"     \"in\"     \n[ ... and 3 more ]\n\ntext2 :\n [1] \"I\"      \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"this\"  \n [8] \"is\"     \"good\"   \"enough\" \"Ha\"     \"Anyone\"\n[ ... and 6 more ]\n\ntext3 :\n[1] \"Best\"    \"Home\"    \"Defense\" \"Round\"   \"in\"      \"5.56\"   \n\ntext4 :\n[1] \"5\"       \"5\"       \"5\"       \"555\"     \"53\"      \"5\"      \n[7] \"50535.5\" \"5-\"     \n\ntext5 :\n [1] \"My\"         \"suppressed\" \"12.5\"       \"AR\"         \"is\"        \n [6] \"the\"        \"same\"       \"size\"       \"as\"         \"my\"        \n[11] \"14.5\"       \"AR\"        \n\ntext6 :\n [1] \"Springfield\" \"Hellcat\"     \"RDP\"         \"with\"       \n [5] \"SilencerCo\"  \"Omega\"       \"9k\"          \"and\"        \n [9] \"Streamlight\" \"TLR-7\"      \n\n[ reached max_ndoc ... 974 more documents ]\n\n\n\ntop_guns_tokens_no_punct_no_upper <- tokens_tolower(top_guns_tokens_no_punct)\n\nprint(top_guns_tokens_no_punct_no_upper)\n\n\nTokens consisting of 980 documents.\ntext1 :\n [1] \"with\"    \"tiny\"    \"ar15s\"   \"and\"     \"mcx\"     \"similar\"\n [7] \"where\"   \"does\"    \"a\"       \"pcc\"     \"fit\"     \"in\"     \n[ ... and 3 more ]\n\ntext2 :\n [1] \"i\"      \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"this\"  \n [8] \"is\"     \"good\"   \"enough\" \"ha\"     \"anyone\"\n[ ... and 6 more ]\n\ntext3 :\n[1] \"best\"    \"home\"    \"defense\" \"round\"   \"in\"      \"5.56\"   \n\ntext4 :\n[1] \"5\"       \"5\"       \"5\"       \"555\"     \"53\"      \"5\"      \n[7] \"50535.5\" \"5-\"     \n\ntext5 :\n [1] \"my\"         \"suppressed\" \"12.5\"       \"ar\"         \"is\"        \n [6] \"the\"        \"same\"       \"size\"       \"as\"         \"my\"        \n[11] \"14.5\"       \"ar\"        \n\ntext6 :\n [1] \"springfield\" \"hellcat\"     \"rdp\"         \"with\"       \n [5] \"silencerco\"  \"omega\"       \"9k\"          \"and\"        \n [9] \"streamlight\" \"tlr-7\"      \n\n[ reached max_ndoc ... 974 more documents ]\n\n\n\ntop_guns_tokens_no_punct_no_upper_no_stop <-    tokens_select(top_guns_tokens_no_punct_no_upper, pattern = stopwords(\"en\"), selection = \"remove\")\n\nlength(top_guns_tokens_no_punct_no_upper_no_stop)\n\n\n[1] 980\n\nprint(top_guns_tokens_no_punct_no_upper_no_stop)\n\n\nTokens consisting of 980 documents.\ntext1 :\n[1] \"tiny\"    \"ar15s\"   \"mcx\"     \"similar\" \"pcc\"     \"fit\"    \n[7] \"arsenal\" \"now\"    \n\ntext2 :\n [1] \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"good\"   \"enough\"\n [8] \"ha\"     \"anyone\" \"else\"   \"anal\"   \"stuff\" \n\ntext3 :\n[1] \"best\"    \"home\"    \"defense\" \"round\"   \"5.56\"   \n\ntext4 :\n[1] \"5\"       \"5\"       \"5\"       \"555\"     \"53\"      \"5\"      \n[7] \"50535.5\" \"5-\"     \n\ntext5 :\n[1] \"suppressed\" \"12.5\"       \"ar\"         \"size\"       \"14.5\"      \n[6] \"ar\"        \n\ntext6 :\n[1] \"springfield\" \"hellcat\"     \"rdp\"         \"silencerco\" \n[5] \"omega\"       \"9k\"          \"streamlight\" \"tlr-7\"      \n\n[ reached max_ndoc ... 974 more documents ]\n\n\n\ntop_guns_corpus_tokens <- tokens(new_guns_corpus)\n\nprint(top_guns_corpus_tokens)\n\n\nTokens consisting of 980 documents.\ntext1 :\n [1] \"With\"    \"tiny\"    \"ar15s\"   \"and\"     \"mcx\"     \"/\"      \n [7] \"similar\" \",\"       \"where\"   \"does\"    \"a\"       \"pcc\"    \n[ ... and 6 more ]\n\ntext2 :\n [1] \"I\"      \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"this\"  \n [8] \"is\"     \"good\"   \"enough\" \".\"      \"Ha\"    \n[ ... and 9 more ]\n\ntext3 :\n[1] \"Best\"    \"Home\"    \"Defense\" \"Round\"   \"in\"      \"5.56\"   \n[7] \"?\"      \n\ntext4 :\n [1] \"5\"       \"5\"       \"*\"       \"5\"       \"/\"       \"555\"    \n [7] \"&\"       \"53\"      \"5\"       \"50535.5\" \"\\\"\"      \"5-\"     \n\ntext5 :\n [1] \"My\"         \"suppressed\" \"12.5\"       \"AR\"         \"is\"        \n [6] \"the\"        \"same\"       \"size\"       \"as\"         \"my\"        \n[11] \"14.5\"       \"\\\"\"        \n[ ... and 1 more ]\n\ntext6 :\n [1] \"Springfield\" \"Hellcat\"     \"RDP\"         \"with\"       \n [5] \"SilencerCo\"  \"Omega\"       \"9k\"          \"and\"        \n [9] \"Streamlight\" \"TLR-7\"       \".\"          \n\n[ reached max_ndoc ... 974 more documents ]\n\n\n\nhead(annotated.guns_corpus$token)\n\n\n# A tibble: 6 × 11\n  doc_id   sid tid   token token_with_ws lemma upos  xpos  feats      \n   <int> <int> <chr> <chr> <chr>         <chr> <chr> <chr> <chr>      \n1      1     1 1     With  \"With \"       with  ADP   IN    <NA>       \n2      1     1 2     tiny  \"tiny \"       tiny  ADJ   JJ    Degree=Pos \n3      1     1 3     ar    \"ar\"          ar    NOUN  NN    Number=Sing\n4      1     1 4     15s   \"15s \"        15s   NOUN  NNS   Number=Plur\n5      1     1 5     and   \"and \"        and   CCONJ CC    <NA>       \n6      1     1 6     mcx/  \"mcx/\"        mcx/  SYM   NFP   <NA>       \n# … with 2 more variables: tid_source <chr>, relation <chr>\n\nhead(annotated.guns_corpus$document)\n\n\n  doc_id\n1      1\n2      2\n3      3\n4      4\n5      5\n6      6\n\ndoc_id_guns<-annotated.guns_corpus$document\n\ndoc_id_guns$date<-new_guns_urls_df$date_utc\n\nannoData <- left_join(doc_id_guns, annotated.guns_corpus$token, by = \"doc_id\")\n\nannoData$date<-as.Date(annoData$date)\n\n\n\n\n\nannoData %>% \n  group_by(date) %>% \n  summarize(Sentences = max(sid)) %>%\n  ggplot(aes(date, Sentences)) +\n    geom_line() +\n    geom_smooth() +\n    theme_bw()\n\n\n\n\n\n\n#sentimetnsdf<-get_sentiments(\"nrc\")\n\n#write.csv(sentimetnsdf, file = \"sentimetnsdf.csv\")\n\n#save(sentimetnsdf, file=\"sentimetnsdf_2\")\n\n\n\n`\nTopic\nModeling analysis with stopwords and punctuation removed\nResponse: As can be seen from the results above,\nremoving stopwords and punctuation removes a good deal of the unwanted\nlanguage from the corpus and does a slightly more comprehensible job in\ndisplaying the information. However, any kind of stemming or reduction\nwill be difficult with posts about firearms for a number of reasons.\nFirstly the language surrounding firearms involves numbers for model\nnumbers, ammunition calibers and the capacity of magazines and other\ndevices that hold bullets. This results in difficulty removing both\npunctuation and numbers from the data as they give a sense of what sort\nof each of the aforementioned items people are interesting in talking\nabout. As a results removing the punctuation is difficult because it\nallows for more comprehensible data by reducing the usage of unneeded\npunctuation like exclamaintion points and questions marks that are\ncommon on a forum of this nature but not useful in analyzing the common\ntopics and language.\nUsing Top Guns Instead\n\n\n# New_guns_urls <- find_thread_urls(subreddit=\"guns\", sort_by=\"new\")\n\n\nloadRData <- function(fileName){\n#loads an RData file, and returns it\n    load(fileName)\n    get(ls()[ls() != \"fileName\"])\n}\nNew_guns_urls_df <- loadRData(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/top_guns_urls.RData\")\n\n\n\n\n\nstr(New_guns_urls_df)\n\n\n'data.frame':   996 obs. of  6 variables:\n $ date_utc : chr  \"2020-02-24\" \"2021-02-13\" \"2017-08-28\" \"2020-06-18\" ...\n $ title    : chr  \"Second AR build: 300blk pistol. Can't not have wood.\" \"Team Green - Master Chief / Doom Guy Wombo Combo\" \"It's Klobberin' Time!\" \"50k Round Deep Clean\" ...\n $ text     : chr  \"\" \"\" \"\" \"\" ...\n $ subreddit: chr  \"guns\" \"guns\" \"guns\" \"guns\" ...\n $ comments : num  148 216 184 275 220 360 182 312 274 139 ...\n $ url      : chr  \"https://www.reddit.com/r/guns/comments/f8suox/second_ar_build_300blk_pistol_cant_not_have_wood/\" \"https://www.reddit.com/r/guns/comments/liwecw/team_green_master_chief_doom_guy_wombo_combo/\" \"https://www.reddit.com/r/guns/comments/6wjx49/its_klobberin_time/\" \"https://www.reddit.com/r/guns/comments/hbflwy/50k_round_deep_clean/\" ...\n\nConversion From Data\nFrame to Corpus\nExplanation: Below I have processed my initial\ndataframe from Reddit into a corpus and saved a summary of the resulting\ndata.\n\n\nnew_guns_urls_df<-New_guns_urls_df[,c(\"title\", \"date_utc\", \"comments\")]\n\nnew_guns_corpus<-corpus(new_guns_urls_df$title)\n\nnew_guns_documents<-new_guns_corpus[1:10,]\n\nnew_guns_corpus_summary <- summary(new_guns_corpus)\n\n\n\nBroad Characteristics\nExplanation: In order to clean the documents for\npre-processing and analysis I have removed punctuation, converted to\nlowercase and removed stopwords. Though the language of firearms is\noften associated with punctuation, such as 5.56, 3.57, and a variety of\nother calibers, which represent the diameter of the barrel required to\nfire each ammunition. However, losing the punctuation in caliber and\nfirearm titles would not reduce their comprehensibility in analysis if\nthey retain their form as 5.56 and 556 can be considered equally while\nreducing the complexity of tokens and potentially even sentences.\nConverting the documents to lowercase can also simplifies the data.\nHowever, for my inital analysis I will just being making a lowercase\ndocument feature matrix and a more edited one.\n\n\nnew_guns_corpus_dfm_tl<-tokens(new_guns_corpus) %>%  dfm(tolower=TRUE) \n\nnew_guns_corpus_dfm_punct_tl_sw <- tokens(new_guns_corpus,\n                                    remove_punct = TRUE,) %>%\n                           dfm(tolower=TRUE) %>%\n                           dfm_remove(stopwords('english'))\n\n\n\nTop features\nExplanation: Examining the top 20 features below we\nsee a fairly predictable set of response, as may be expected from the\ngun subreddit, the most used word is gun. Rifle and pistol are also in\nthe top 20. Individual letters such as “ar”, “s”, and “m” appear\nfrequently as they are commonly used designations for types of firearms\nor model names, ar-12, ar-15, m-4, m-16, m1911, 5-mm, and s559, s-12 and\nother designations. This indicates that these numbers are valuable, if\ndifficult to comprehend on their own. With no punctuation removed the\nfirst 20 features are not informative.\n\n\ntopfeatures(new_guns_corpus_dfm_tl, 20)\n\n\n   .    a  the    i   my    ,   to   of  and    !   in  for this   it \n 720  318  295  283  261  236  196  152  143  121  119  116  110   92 \n you  gun   is    ? with    \" \n  89   89   82   76   75   72 \n\n\n\ntopfeatures(new_guns_corpus_dfm_punct_tl_sw, 20)\n\n\n     gun        s    first      got     like     just      day \n      89       54       52       48       43       38       37 \n    guns    rifle      new      one     time    range    today \n      37       36       33       33       32       30       26 \n    made      old       3d  finally  printed shooting \n      22       20       20       19       19       18 \n\nWorld Cloud\nExplanation: Though not necessarily statistically\ninformative, the wordcloud below can give some sense of comparative\nfrequency using the limit of minimum count being 6. Reading through\nthese can give a sense of both the communal nature of the forum in\nasking for recommendations, but also the importance of the word purchase\nand other words associated with working with, and buying firearms. In\nthe case of the only lowercased data we can gather much less\ninformation.\n\n\nset.seed(123456)\n\ntextplot_wordcloud(new_guns_corpus_dfm_tl, min_count = 12, random_order = T, rotation = 0)\n\n\n\n\n\n\nset.seed(123456)\n\ntextplot_wordcloud(new_guns_corpus_dfm_punct_tl_sw, min_count = 6, random_order = T, rotation = 0)\n\n\n\n\nTypes, Tokens, and Sentances\nTypes\nExplanation: The gun corpus summary gives 3 counting\ncategories that we can interpret in order to get a sense of the\ncomplexity of the documents that we are using. Looking at the number of\ntypes on average we see a mean of 9.18 and qunantiles that indicate a\nrange of 2-45 with 50% being between 4 and 13 types.\n\n\nmean(new_guns_corpus_summary$Types)\n\n\n[1] 8.62\n\nquantile(new_guns_corpus_summary$Types)\n\n\n  0%  25%  50%  75% 100% \n   1    5    7   10   39 \n\nTokens\nExplanation: Tokens are relatively similar to types\nin this case. Here there is a mean of 9.72 but a range of 2-55 with the\nmiddle 50% ranging from 4-13 tokens, as was the case for types.\n\n\nmean(new_guns_corpus_summary$Tokens)\n\n\n[1] 9.23\n\nquantile(new_guns_corpus_summary$Tokens)\n\n\n   0%   25%   50%   75%  100% \n 1.00  5.00  7.00 10.25 45.00 \n\nSentances\nExplanation: As is indicated below, it appears that\nthe number of sentences in each post is generally one. Arroding to the\nqunatile statistics the most sentences in any post\n\n\nmean(new_guns_corpus_summary$Sentences)\n\n\n[1] 1.17\n\nquantile(new_guns_corpus_summary$Sentences)\n\n\n  0%  25%  50%  75% 100% \n   1    1    1    1    4 \n\nWord counts\nExplanation: Looking at word counts we see a similar\ntrend reflected where including stopwords and punctuation decreases the\nquality of data as little information but punctuation and stopwords are\nincluded.\n\n\nword_counts_new_1 <- as.data.frame(sort(colSums(new_guns_corpus_dfm_tl),dec=T))\n\ncolnames(word_counts_new_1) <- c(\"Frequency\")\n\nword_counts_new_1$Rank <- c(1:ncol(new_guns_corpus_dfm_tl))\n\nhead(word_counts_new_1)\n\n\n    Frequency Rank\n.         720    1\na         318    2\nthe       295    3\ni         283    4\nmy        261    5\n,         236    6\n\n\n\nword_counts_new <- as.data.frame(sort(colSums(new_guns_corpus_dfm_punct_tl_sw),dec=T))\n\ncolnames(word_counts_new) <- c(\"Frequency\")\n\nword_counts_new$Rank <- c(1:ncol(new_guns_corpus_dfm_punct_tl_sw))\n\nhead(word_counts_new)\n\n\n      Frequency Rank\ngun          89    1\ns            54    2\nfirst        52    3\ngot          48    4\nlike         43    5\njust         38    6\n\nZipf’s Law\nExplanation: As can be seen from the frequency\ngraphs below, Ziph’s Law of inverse proportion. In this case a words\nrank in freqency is inversely prorportional to the number of times it is\nobserved. Though the uncleaned dataset has far more frequency for its\nmost common words (much of which is punctuation) it appears to follow\nthe law.\n\n\nggplot(word_counts_new, mapping = aes(x = Rank, y = Frequency)) + \n  geom_point() +\n  labs(title = \"Zipf's Law\", x = \"Rank\", y = \"Frequency\") + \n  theme_bw()\n\n\n\n\n\n\nggplot(word_counts_new_1, mapping = aes(x = Rank, y = Frequency)) + \n  geom_point() +\n  labs(title = \"Zipf's Law\", x = \"Rank\", y = \"Frequency\") + \n  theme_bw()\n\n\n\n\nData Trimming\nExplanation: Much of what I do here will be\nexplained in the code and results. Many words appear with a minimum\nfrequency of 4, though non are included in 10% and only 3 words are\nincluded in 5%. At a level of 2.5% we get 4 words.\n\n\n# First I trim the data to only include words that appear at least 4 times\n\nsmaller_dfm_4_freq <- dfm_trim(new_guns_corpus_dfm_punct_tl_sw, min_termfreq = 4)\n\n# Next I will look at proportions are see if there are words that are seen in\n# More than 10% and 5% of documents\nsmaller_dfm_10_p <- dfm_trim(smaller_dfm_4_freq, min_docfreq = 0.1, docfreq_type = \"prop\")\n\nsmaller_dfm_5_p <- dfm_trim(smaller_dfm_4_freq, min_docfreq = 0.05, docfreq_type = \"prop\")\n\nsmaller_dfm_2.5_p <- dfm_trim(smaller_dfm_4_freq, min_docfreq = 0.025,  docfreq_type = \"prop\")\n\n\n\nData Readability\nExplanation: Before making general modifications to\nthe data, it is valuable to also get a sense of readability, as in week\n5. In this case we will calculate readability scores based on 3\ndifferent measures, FOG, Coleman Liau, and Flesch Kincaid. Though this\nstep will not indicate what sort of pre-processing is best, or how the\ndata should be reduced, it does give us some insight into the complexity\nof the language in our data. In this case we just observe the\nreadability based on the post number.\n\n\nreadability_new_guns <- textstat_readability(new_guns_corpus, \n                                    measure = c(\"Flesch.Kincaid\", \"FOG\", \"Coleman.Liau.grade\")) \n\n# add in a chapter number\n\nreadability_new_guns$reddit_post <- c(1:nrow(readability_new_guns))\n\n# plot results\nggplot(readability_new_guns, aes(x = reddit_post)) +\n  geom_line(aes(y = Flesch.Kincaid), color = \"black\",  alpha=0.3) + \n  geom_line(aes(y = FOG), color = \"red\", alpha=0.3) + \n  geom_line(aes(y = Coleman.Liau.grade), color = \"blue\", alpha=0.3) + \n  theme_bw()\n\n\n\n\nExplanation: In this part we will add dates to our\ndata to see how the complexity changes over time or if it was relatively\nconstant. As can be seen below, the amount of complexity in the data\nvaries more smoothly when the data are sorted by data and not\narbitrarily by their post number, in this case all 3 complexity method\nexhibit similar trends.\n\n\nreadability_new_guns$added_dates <- as.Date(New_guns_urls_df$date_utc)\n\nggplot(readability_new_guns, aes(x = added_dates)) +\n  geom_smooth(aes(y = Flesch.Kincaid), color = \"black\") + \n  geom_smooth(aes(y = FOG), color = \"red\") + \n  geom_smooth(aes(y = Coleman.Liau.grade), color = \"blue\") + \n  theme_minimal()\n\n\n\n\nExplanation: Looking at the readability of the we\nsee that all correlations between the methods of complexity measurement\nare similar except of FOG and Coleman Liau, however the graphs above do\nindicate some similarity in trend between them, though not direct\ncorrelation in their estimates potentially.\n\n\ncor(readability_new_guns$Flesch.Kincaid, readability_new_guns$FOG, use = \"complete.obs\")\n\n\n[1] 0.8386043\n\n\n\ncor(readability_new_guns$Flesch.Kincaid, readability_new_guns$Coleman.Liau.grade, use = \"complete.obs\")\n\n\n[1] 0.7382902\n\n\n\ncor(readability_new_guns$FOG, readability_new_guns$Coleman.Liau.grade, use = \"complete.obs\")\n\n\n[1] 0.563754\n\nPre-processing\nBefore Reduction and Co-Occurance\nExplanation: Next I used the\nfactorial_preprocessing() command to both use n-grams processing and use\nan infrequent term threshold. This is in order to see what techniques,\nsuch as removing punctuation, stopwords, etc lead to a pre-text score\ndevised by Denny and Spirling. This pre-text score indicatess how many\nk-pairs of terms change the most when the pre-processing strategy is\nchanged. Lower scores indicate more usual results while higher scores\nindicate more unusual results and they are between 0 and 1. Here we have\nused n-grams and set an infreqent term threshold. Because of the nature\nof our data I will use 30% of documents as\n\n\n?factorial_preprocessing\npreprocessed_documents <- factorial_preprocessing(\n    new_guns_corpus,\n    use_ngrams = TRUE,\n    infrequent_term_threshold = 0.3,\n    verbose = FALSE)\n\n\nPreprocessing 996 documents 128 different ways...\n\n\n\nnames(preprocessed_documents)\n\n\n[1] \"choices\"  \"dfm_list\" \"labels\"  \n\nExplanation: As can be seen below the possible\nchoices are coded on the first column with each subsequent column\nindicating whether or not each choice includes each of the specified\nchoices in its assessment.\n\n\nhead(preprocessed_documents$choices)\n\n\n              removePunctuation removeNumbers lowercase stem\nP-N-L-S-W-I-3              TRUE          TRUE      TRUE TRUE\nN-L-S-W-I-3               FALSE          TRUE      TRUE TRUE\nP-L-S-W-I-3                TRUE         FALSE      TRUE TRUE\nL-S-W-I-3                 FALSE         FALSE      TRUE TRUE\nP-N-S-W-I-3                TRUE          TRUE     FALSE TRUE\nN-S-W-I-3                 FALSE          TRUE     FALSE TRUE\n              removeStopwords infrequent_terms use_ngrams\nP-N-L-S-W-I-3            TRUE             TRUE       TRUE\nN-L-S-W-I-3              TRUE             TRUE       TRUE\nP-L-S-W-I-3              TRUE             TRUE       TRUE\nL-S-W-I-3                TRUE             TRUE       TRUE\nP-N-S-W-I-3              TRUE             TRUE       TRUE\nN-S-W-I-3                TRUE             TRUE       TRUE\n\nExplanation: Next preText is calculated using 50\ncomparisons and a cosine distance calculation.\n\n\n#set.seed(12366)\n#preText_results <- preText(\n#    preprocessed_documents,\n#   dataset_name = \"Gun Pretext Results\",\n#   distance_method = \"cosine\",\n#   num_comparisons = 50,\n#  verbose = TRUE)\n\n\n\n\n\n#save(preText_results, file=\"preText_results_3_27_gun_50_comp.RData\")\n\n\n\n\n\npreText_results <- loadRData(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/preText_results_3_27_gun_50_comp.RData\")\n\npreText_results\n\n\n$preText_scores\n    preText_score preprocessing_steps\n1      0.04157825       P-N-L-S-W-I-3\n2      0.04157825         N-L-S-W-I-3\n3      0.04157825         P-L-S-W-I-3\n4      0.04157825           L-S-W-I-3\n5      0.04157825         P-N-S-W-I-3\n6      0.04157825           N-S-W-I-3\n7      0.04157825           P-S-W-I-3\n8      0.04157825             S-W-I-3\n9      0.04157825         P-N-L-W-I-3\n10     0.04157825           N-L-W-I-3\n11     0.04157825           P-L-W-I-3\n12     0.04157825             L-W-I-3\n13     0.04157825           P-N-W-I-3\n14     0.04157825             N-W-I-3\n15     0.04157825             P-W-I-3\n16     0.04157825               W-I-3\n17     0.04157825         P-N-L-S-I-3\n18     0.04157825           N-L-S-I-3\n19     0.04157825           P-L-S-I-3\n20     0.04157825             L-S-I-3\n21     0.04157825           P-N-S-I-3\n22     0.04157825             N-S-I-3\n23     0.04157825             P-S-I-3\n24     0.04157825               S-I-3\n25     0.04157825           P-N-L-I-3\n26     0.04157825             N-L-I-3\n27     0.04157825             P-L-I-3\n28     0.04157825               L-I-3\n29     0.04157825             P-N-I-3\n30     0.04157825               N-I-3\n31     0.04157825               P-I-3\n32     0.04157825                 I-3\n33     0.04815400         P-N-L-S-W-3\n34     0.08058534           N-L-S-W-3\n35     0.04403401           P-L-S-W-3\n36     0.08427290             L-S-W-3\n37     0.04864279           P-N-S-W-3\n38     0.07599185             N-S-W-3\n39     0.04470926             P-S-W-3\n40     0.08186325               S-W-3\n41     0.02723578           P-N-L-W-3\n42     0.02629912             N-L-W-3\n43     0.02280946             P-L-W-3\n44     0.02777490               L-W-3\n45     0.02723578             P-N-W-3\n46     0.02629912               N-W-3\n47     0.02280946               P-W-3\n48     0.02777490                 W-3\n49     0.02647384           P-N-L-S-3\n50     0.08304385             N-L-S-3\n51     0.02334213             P-L-S-3\n52     0.07225887               L-S-3\n53     0.02706859             P-N-S-3\n54     0.08697989               N-S-3\n55     0.02384351               P-S-3\n56     0.07767802                 S-3\n57     0.02647384             P-N-L-3\n58     0.02777727               N-L-3\n59     0.02334213               P-L-3\n60     0.01888648                 L-3\n61     0.02647384               P-N-3\n62     0.02777727                 N-3\n63     0.02334213                 P-3\n64     0.01888648                   3\n65     0.04157825         P-N-L-S-W-I\n66     0.04157825           N-L-S-W-I\n67     0.04157825           P-L-S-W-I\n68     0.04157825             L-S-W-I\n69     0.04157825           P-N-S-W-I\n70     0.04157825             N-S-W-I\n71     0.04157825             P-S-W-I\n72     0.04157825               S-W-I\n73     0.04157825           P-N-L-W-I\n74     0.04157825             N-L-W-I\n75     0.04157825             P-L-W-I\n76     0.04157825               L-W-I\n77     0.04157825             P-N-W-I\n78     0.04157825               N-W-I\n79     0.04157825               P-W-I\n80     0.04157825                 W-I\n81     0.04157825           P-N-L-S-I\n82     0.04157825             N-L-S-I\n83     0.04157825             P-L-S-I\n84     0.04157825               L-S-I\n85     0.04157825             P-N-S-I\n86     0.04157825               N-S-I\n87     0.04157825               P-S-I\n88     0.04157825                 S-I\n89     0.04157825             P-N-L-I\n90     0.04157825               N-L-I\n91     0.04157825               P-L-I\n92     0.04157825                 L-I\n93     0.04157825               P-N-I\n94     0.04157825                 N-I\n95     0.04157825                 P-I\n96     0.04157825                   I\n97     0.07109905           P-N-L-S-W\n98     0.16021925             N-L-S-W\n99     0.06767075             P-L-S-W\n100    0.17095529               L-S-W\n101    0.07098343             P-N-S-W\n102    0.15776534               N-S-W\n103    0.06802275               P-S-W\n104    0.16850136                 S-W\n105    0.02723578             P-N-L-W\n106    0.02344916               N-L-W\n107    0.02280946               P-L-W\n108    0.02739263                 L-W\n109    0.02723578               P-N-W\n110    0.02344916                 N-W\n111    0.02280946                 P-W\n112    0.02739263                   W\n113    0.05006683             P-N-L-S\n114    0.22396649               N-L-S\n115    0.04704759               P-L-S\n116    0.23717659                 L-S\n117    0.05064631               P-N-S\n118    0.21022978                 N-S\n119    0.04752479                 P-S\n120    0.23146371                   S\n121    0.02635536               P-N-L\n122    0.03527452                 N-L\n123    0.02342946                 P-L\n124    0.10226138                   L\n125    0.02635536                 P-N\n126    0.03527452                   N\n127    0.02342946                   P\n\n$ranked_preText_scores\n    preText_score preprocessing_steps\n1      0.23717659                 L-S\n2      0.23146371                   S\n3      0.22396649               N-L-S\n4      0.21022978                 N-S\n5      0.17095529               L-S-W\n6      0.16850136                 S-W\n7      0.16021925             N-L-S-W\n8      0.15776534               N-S-W\n9      0.10226138                   L\n10     0.08697989               N-S-3\n11     0.08427290             L-S-W-3\n12     0.08304385             N-L-S-3\n13     0.08186325               S-W-3\n14     0.08058534           N-L-S-W-3\n15     0.07767802                 S-3\n16     0.07599185             N-S-W-3\n17     0.07225887               L-S-3\n18     0.07109905           P-N-L-S-W\n19     0.07098343             P-N-S-W\n20     0.06802275               P-S-W\n21     0.06767075             P-L-S-W\n22     0.05064631               P-N-S\n23     0.05006683             P-N-L-S\n24     0.04864279           P-N-S-W-3\n25     0.04815400         P-N-L-S-W-3\n26     0.04752479                 P-S\n27     0.04704759               P-L-S\n28     0.04470926             P-S-W-3\n29     0.04403401           P-L-S-W-3\n30     0.04157825       P-N-L-S-W-I-3\n31     0.04157825         N-L-S-W-I-3\n32     0.04157825         P-L-S-W-I-3\n33     0.04157825           L-S-W-I-3\n34     0.04157825         P-N-S-W-I-3\n35     0.04157825           N-S-W-I-3\n36     0.04157825           P-S-W-I-3\n37     0.04157825             S-W-I-3\n38     0.04157825         P-N-L-W-I-3\n39     0.04157825           N-L-W-I-3\n40     0.04157825           P-L-W-I-3\n41     0.04157825             L-W-I-3\n42     0.04157825           P-N-W-I-3\n43     0.04157825             N-W-I-3\n44     0.04157825             P-W-I-3\n45     0.04157825               W-I-3\n46     0.04157825         P-N-L-S-I-3\n47     0.04157825           N-L-S-I-3\n48     0.04157825           P-L-S-I-3\n49     0.04157825             L-S-I-3\n50     0.04157825           P-N-S-I-3\n51     0.04157825             N-S-I-3\n52     0.04157825             P-S-I-3\n53     0.04157825               S-I-3\n54     0.04157825           P-N-L-I-3\n55     0.04157825             N-L-I-3\n56     0.04157825             P-L-I-3\n57     0.04157825               L-I-3\n58     0.04157825             P-N-I-3\n59     0.04157825               N-I-3\n60     0.04157825               P-I-3\n61     0.04157825                 I-3\n62     0.04157825         P-N-L-S-W-I\n63     0.04157825           N-L-S-W-I\n64     0.04157825           P-L-S-W-I\n65     0.04157825             L-S-W-I\n66     0.04157825           P-N-S-W-I\n67     0.04157825             N-S-W-I\n68     0.04157825             P-S-W-I\n69     0.04157825               S-W-I\n70     0.04157825           P-N-L-W-I\n71     0.04157825             N-L-W-I\n72     0.04157825             P-L-W-I\n73     0.04157825               L-W-I\n74     0.04157825             P-N-W-I\n75     0.04157825               N-W-I\n76     0.04157825               P-W-I\n77     0.04157825                 W-I\n78     0.04157825           P-N-L-S-I\n79     0.04157825             N-L-S-I\n80     0.04157825             P-L-S-I\n81     0.04157825               L-S-I\n82     0.04157825             P-N-S-I\n83     0.04157825               N-S-I\n84     0.04157825               P-S-I\n85     0.04157825                 S-I\n86     0.04157825             P-N-L-I\n87     0.04157825               N-L-I\n88     0.04157825               P-L-I\n89     0.04157825                 L-I\n90     0.04157825               P-N-I\n91     0.04157825                 N-I\n92     0.04157825                 P-I\n93     0.04157825                   I\n94     0.03527452                 N-L\n95     0.03527452                   N\n96     0.02777727               N-L-3\n97     0.02777727                 N-3\n98     0.02777490               L-W-3\n99     0.02777490                 W-3\n100    0.02739263                 L-W\n101    0.02739263                   W\n102    0.02723578           P-N-L-W-3\n103    0.02723578             P-N-W-3\n104    0.02723578             P-N-L-W\n105    0.02723578               P-N-W\n106    0.02706859             P-N-S-3\n107    0.02647384           P-N-L-S-3\n108    0.02647384             P-N-L-3\n109    0.02647384               P-N-3\n110    0.02635536               P-N-L\n111    0.02635536                 P-N\n112    0.02629912             N-L-W-3\n113    0.02629912               N-W-3\n114    0.02384351               P-S-3\n115    0.02344916               N-L-W\n116    0.02344916                 N-W\n117    0.02342946                 P-L\n118    0.02342946                   P\n119    0.02334213             P-L-S-3\n120    0.02334213               P-L-3\n121    0.02334213                 P-3\n122    0.02280946             P-L-W-3\n123    0.02280946               P-W-3\n124    0.02280946               P-L-W\n125    0.02280946                 P-W\n126    0.01888648                 L-3\n127    0.01888648                   3\n\n$choices\n              removePunctuation removeNumbers lowercase  stem\nP-N-L-S-W-I-3              TRUE          TRUE      TRUE  TRUE\nN-L-S-W-I-3               FALSE          TRUE      TRUE  TRUE\nP-L-S-W-I-3                TRUE         FALSE      TRUE  TRUE\nL-S-W-I-3                 FALSE         FALSE      TRUE  TRUE\nP-N-S-W-I-3                TRUE          TRUE     FALSE  TRUE\nN-S-W-I-3                 FALSE          TRUE     FALSE  TRUE\nP-S-W-I-3                  TRUE         FALSE     FALSE  TRUE\nS-W-I-3                   FALSE         FALSE     FALSE  TRUE\nP-N-L-W-I-3                TRUE          TRUE      TRUE FALSE\nN-L-W-I-3                 FALSE          TRUE      TRUE FALSE\nP-L-W-I-3                  TRUE         FALSE      TRUE FALSE\nL-W-I-3                   FALSE         FALSE      TRUE FALSE\nP-N-W-I-3                  TRUE          TRUE     FALSE FALSE\nN-W-I-3                   FALSE          TRUE     FALSE FALSE\nP-W-I-3                    TRUE         FALSE     FALSE FALSE\nW-I-3                     FALSE         FALSE     FALSE FALSE\nP-N-L-S-I-3                TRUE          TRUE      TRUE  TRUE\nN-L-S-I-3                 FALSE          TRUE      TRUE  TRUE\nP-L-S-I-3                  TRUE         FALSE      TRUE  TRUE\nL-S-I-3                   FALSE         FALSE      TRUE  TRUE\nP-N-S-I-3                  TRUE          TRUE     FALSE  TRUE\nN-S-I-3                   FALSE          TRUE     FALSE  TRUE\nP-S-I-3                    TRUE         FALSE     FALSE  TRUE\nS-I-3                     FALSE         FALSE     FALSE  TRUE\nP-N-L-I-3                  TRUE          TRUE      TRUE FALSE\nN-L-I-3                   FALSE          TRUE      TRUE FALSE\nP-L-I-3                    TRUE         FALSE      TRUE FALSE\nL-I-3                     FALSE         FALSE      TRUE FALSE\nP-N-I-3                    TRUE          TRUE     FALSE FALSE\nN-I-3                     FALSE          TRUE     FALSE FALSE\nP-I-3                      TRUE         FALSE     FALSE FALSE\nI-3                       FALSE         FALSE     FALSE FALSE\nP-N-L-S-W-3                TRUE          TRUE      TRUE  TRUE\nN-L-S-W-3                 FALSE          TRUE      TRUE  TRUE\nP-L-S-W-3                  TRUE         FALSE      TRUE  TRUE\nL-S-W-3                   FALSE         FALSE      TRUE  TRUE\nP-N-S-W-3                  TRUE          TRUE     FALSE  TRUE\nN-S-W-3                   FALSE          TRUE     FALSE  TRUE\nP-S-W-3                    TRUE         FALSE     FALSE  TRUE\nS-W-3                     FALSE         FALSE     FALSE  TRUE\nP-N-L-W-3                  TRUE          TRUE      TRUE FALSE\nN-L-W-3                   FALSE          TRUE      TRUE FALSE\nP-L-W-3                    TRUE         FALSE      TRUE FALSE\nL-W-3                     FALSE         FALSE      TRUE FALSE\nP-N-W-3                    TRUE          TRUE     FALSE FALSE\nN-W-3                     FALSE          TRUE     FALSE FALSE\nP-W-3                      TRUE         FALSE     FALSE FALSE\nW-3                       FALSE         FALSE     FALSE FALSE\nP-N-L-S-3                  TRUE          TRUE      TRUE  TRUE\nN-L-S-3                   FALSE          TRUE      TRUE  TRUE\nP-L-S-3                    TRUE         FALSE      TRUE  TRUE\nL-S-3                     FALSE         FALSE      TRUE  TRUE\nP-N-S-3                    TRUE          TRUE     FALSE  TRUE\nN-S-3                     FALSE          TRUE     FALSE  TRUE\nP-S-3                      TRUE         FALSE     FALSE  TRUE\nS-3                       FALSE         FALSE     FALSE  TRUE\nP-N-L-3                    TRUE          TRUE      TRUE FALSE\nN-L-3                     FALSE          TRUE      TRUE FALSE\nP-L-3                      TRUE         FALSE      TRUE FALSE\nL-3                       FALSE         FALSE      TRUE FALSE\nP-N-3                      TRUE          TRUE     FALSE FALSE\nN-3                       FALSE          TRUE     FALSE FALSE\nP-3                        TRUE         FALSE     FALSE FALSE\n3                         FALSE         FALSE     FALSE FALSE\nP-N-L-S-W-I                TRUE          TRUE      TRUE  TRUE\nN-L-S-W-I                 FALSE          TRUE      TRUE  TRUE\nP-L-S-W-I                  TRUE         FALSE      TRUE  TRUE\nL-S-W-I                   FALSE         FALSE      TRUE  TRUE\nP-N-S-W-I                  TRUE          TRUE     FALSE  TRUE\nN-S-W-I                   FALSE          TRUE     FALSE  TRUE\nP-S-W-I                    TRUE         FALSE     FALSE  TRUE\nS-W-I                     FALSE         FALSE     FALSE  TRUE\nP-N-L-W-I                  TRUE          TRUE      TRUE FALSE\nN-L-W-I                   FALSE          TRUE      TRUE FALSE\nP-L-W-I                    TRUE         FALSE      TRUE FALSE\nL-W-I                     FALSE         FALSE      TRUE FALSE\nP-N-W-I                    TRUE          TRUE     FALSE FALSE\nN-W-I                     FALSE          TRUE     FALSE FALSE\nP-W-I                      TRUE         FALSE     FALSE FALSE\nW-I                       FALSE         FALSE     FALSE FALSE\nP-N-L-S-I                  TRUE          TRUE      TRUE  TRUE\nN-L-S-I                   FALSE          TRUE      TRUE  TRUE\nP-L-S-I                    TRUE         FALSE      TRUE  TRUE\nL-S-I                     FALSE         FALSE      TRUE  TRUE\nP-N-S-I                    TRUE          TRUE     FALSE  TRUE\nN-S-I                     FALSE          TRUE     FALSE  TRUE\nP-S-I                      TRUE         FALSE     FALSE  TRUE\nS-I                       FALSE         FALSE     FALSE  TRUE\nP-N-L-I                    TRUE          TRUE      TRUE FALSE\nN-L-I                     FALSE          TRUE      TRUE FALSE\nP-L-I                      TRUE         FALSE      TRUE FALSE\nL-I                       FALSE         FALSE      TRUE FALSE\nP-N-I                      TRUE          TRUE     FALSE FALSE\nN-I                       FALSE          TRUE     FALSE FALSE\nP-I                        TRUE         FALSE     FALSE FALSE\nI                         FALSE         FALSE     FALSE FALSE\nP-N-L-S-W                  TRUE          TRUE      TRUE  TRUE\nN-L-S-W                   FALSE          TRUE      TRUE  TRUE\nP-L-S-W                    TRUE         FALSE      TRUE  TRUE\nL-S-W                     FALSE         FALSE      TRUE  TRUE\nP-N-S-W                    TRUE          TRUE     FALSE  TRUE\nN-S-W                     FALSE          TRUE     FALSE  TRUE\nP-S-W                      TRUE         FALSE     FALSE  TRUE\nS-W                       FALSE         FALSE     FALSE  TRUE\nP-N-L-W                    TRUE          TRUE      TRUE FALSE\nN-L-W                     FALSE          TRUE      TRUE FALSE\nP-L-W                      TRUE         FALSE      TRUE FALSE\nL-W                       FALSE         FALSE      TRUE FALSE\nP-N-W                      TRUE          TRUE     FALSE FALSE\nN-W                       FALSE          TRUE     FALSE FALSE\nP-W                        TRUE         FALSE     FALSE FALSE\nW                         FALSE         FALSE     FALSE FALSE\nP-N-L-S                    TRUE          TRUE      TRUE  TRUE\nN-L-S                     FALSE          TRUE      TRUE  TRUE\nP-L-S                      TRUE         FALSE      TRUE  TRUE\nL-S                       FALSE         FALSE      TRUE  TRUE\nP-N-S                      TRUE          TRUE     FALSE  TRUE\nN-S                       FALSE          TRUE     FALSE  TRUE\nP-S                        TRUE         FALSE     FALSE  TRUE\nS                         FALSE         FALSE     FALSE  TRUE\nP-N-L                      TRUE          TRUE      TRUE FALSE\nN-L                       FALSE          TRUE      TRUE FALSE\nP-L                        TRUE         FALSE      TRUE FALSE\nL                         FALSE         FALSE      TRUE FALSE\nP-N                        TRUE          TRUE     FALSE FALSE\nN                         FALSE          TRUE     FALSE FALSE\nP                          TRUE         FALSE     FALSE FALSE\n                          FALSE         FALSE     FALSE FALSE\n              removeStopwords infrequent_terms use_ngrams\nP-N-L-S-W-I-3            TRUE             TRUE       TRUE\nN-L-S-W-I-3              TRUE             TRUE       TRUE\nP-L-S-W-I-3              TRUE             TRUE       TRUE\nL-S-W-I-3                TRUE             TRUE       TRUE\nP-N-S-W-I-3              TRUE             TRUE       TRUE\nN-S-W-I-3                TRUE             TRUE       TRUE\nP-S-W-I-3                TRUE             TRUE       TRUE\nS-W-I-3                  TRUE             TRUE       TRUE\nP-N-L-W-I-3              TRUE             TRUE       TRUE\nN-L-W-I-3                TRUE             TRUE       TRUE\nP-L-W-I-3                TRUE             TRUE       TRUE\nL-W-I-3                  TRUE             TRUE       TRUE\nP-N-W-I-3                TRUE             TRUE       TRUE\nN-W-I-3                  TRUE             TRUE       TRUE\nP-W-I-3                  TRUE             TRUE       TRUE\nW-I-3                    TRUE             TRUE       TRUE\nP-N-L-S-I-3             FALSE             TRUE       TRUE\nN-L-S-I-3               FALSE             TRUE       TRUE\nP-L-S-I-3               FALSE             TRUE       TRUE\nL-S-I-3                 FALSE             TRUE       TRUE\nP-N-S-I-3               FALSE             TRUE       TRUE\nN-S-I-3                 FALSE             TRUE       TRUE\nP-S-I-3                 FALSE             TRUE       TRUE\nS-I-3                   FALSE             TRUE       TRUE\nP-N-L-I-3               FALSE             TRUE       TRUE\nN-L-I-3                 FALSE             TRUE       TRUE\nP-L-I-3                 FALSE             TRUE       TRUE\nL-I-3                   FALSE             TRUE       TRUE\nP-N-I-3                 FALSE             TRUE       TRUE\nN-I-3                   FALSE             TRUE       TRUE\nP-I-3                   FALSE             TRUE       TRUE\nI-3                     FALSE             TRUE       TRUE\nP-N-L-S-W-3              TRUE            FALSE       TRUE\nN-L-S-W-3                TRUE            FALSE       TRUE\nP-L-S-W-3                TRUE            FALSE       TRUE\nL-S-W-3                  TRUE            FALSE       TRUE\nP-N-S-W-3                TRUE            FALSE       TRUE\nN-S-W-3                  TRUE            FALSE       TRUE\nP-S-W-3                  TRUE            FALSE       TRUE\nS-W-3                    TRUE            FALSE       TRUE\nP-N-L-W-3                TRUE            FALSE       TRUE\nN-L-W-3                  TRUE            FALSE       TRUE\nP-L-W-3                  TRUE            FALSE       TRUE\nL-W-3                    TRUE            FALSE       TRUE\nP-N-W-3                  TRUE            FALSE       TRUE\nN-W-3                    TRUE            FALSE       TRUE\nP-W-3                    TRUE            FALSE       TRUE\nW-3                      TRUE            FALSE       TRUE\nP-N-L-S-3               FALSE            FALSE       TRUE\nN-L-S-3                 FALSE            FALSE       TRUE\nP-L-S-3                 FALSE            FALSE       TRUE\nL-S-3                   FALSE            FALSE       TRUE\nP-N-S-3                 FALSE            FALSE       TRUE\nN-S-3                   FALSE            FALSE       TRUE\nP-S-3                   FALSE            FALSE       TRUE\nS-3                     FALSE            FALSE       TRUE\nP-N-L-3                 FALSE            FALSE       TRUE\nN-L-3                   FALSE            FALSE       TRUE\nP-L-3                   FALSE            FALSE       TRUE\nL-3                     FALSE            FALSE       TRUE\nP-N-3                   FALSE            FALSE       TRUE\nN-3                     FALSE            FALSE       TRUE\nP-3                     FALSE            FALSE       TRUE\n3                       FALSE            FALSE       TRUE\nP-N-L-S-W-I              TRUE             TRUE      FALSE\nN-L-S-W-I                TRUE             TRUE      FALSE\nP-L-S-W-I                TRUE             TRUE      FALSE\nL-S-W-I                  TRUE             TRUE      FALSE\nP-N-S-W-I                TRUE             TRUE      FALSE\nN-S-W-I                  TRUE             TRUE      FALSE\nP-S-W-I                  TRUE             TRUE      FALSE\nS-W-I                    TRUE             TRUE      FALSE\nP-N-L-W-I                TRUE             TRUE      FALSE\nN-L-W-I                  TRUE             TRUE      FALSE\nP-L-W-I                  TRUE             TRUE      FALSE\nL-W-I                    TRUE             TRUE      FALSE\nP-N-W-I                  TRUE             TRUE      FALSE\nN-W-I                    TRUE             TRUE      FALSE\nP-W-I                    TRUE             TRUE      FALSE\nW-I                      TRUE             TRUE      FALSE\nP-N-L-S-I               FALSE             TRUE      FALSE\nN-L-S-I                 FALSE             TRUE      FALSE\nP-L-S-I                 FALSE             TRUE      FALSE\nL-S-I                   FALSE             TRUE      FALSE\nP-N-S-I                 FALSE             TRUE      FALSE\nN-S-I                   FALSE             TRUE      FALSE\nP-S-I                   FALSE             TRUE      FALSE\nS-I                     FALSE             TRUE      FALSE\nP-N-L-I                 FALSE             TRUE      FALSE\nN-L-I                   FALSE             TRUE      FALSE\nP-L-I                   FALSE             TRUE      FALSE\nL-I                     FALSE             TRUE      FALSE\nP-N-I                   FALSE             TRUE      FALSE\nN-I                     FALSE             TRUE      FALSE\nP-I                     FALSE             TRUE      FALSE\nI                       FALSE             TRUE      FALSE\nP-N-L-S-W                TRUE            FALSE      FALSE\nN-L-S-W                  TRUE            FALSE      FALSE\nP-L-S-W                  TRUE            FALSE      FALSE\nL-S-W                    TRUE            FALSE      FALSE\nP-N-S-W                  TRUE            FALSE      FALSE\nN-S-W                    TRUE            FALSE      FALSE\nP-S-W                    TRUE            FALSE      FALSE\nS-W                      TRUE            FALSE      FALSE\nP-N-L-W                  TRUE            FALSE      FALSE\nN-L-W                    TRUE            FALSE      FALSE\nP-L-W                    TRUE            FALSE      FALSE\nL-W                      TRUE            FALSE      FALSE\nP-N-W                    TRUE            FALSE      FALSE\nN-W                      TRUE            FALSE      FALSE\nP-W                      TRUE            FALSE      FALSE\nW                        TRUE            FALSE      FALSE\nP-N-L-S                 FALSE            FALSE      FALSE\nN-L-S                   FALSE            FALSE      FALSE\nP-L-S                   FALSE            FALSE      FALSE\nL-S                     FALSE            FALSE      FALSE\nP-N-S                   FALSE            FALSE      FALSE\nN-S                     FALSE            FALSE      FALSE\nP-S                     FALSE            FALSE      FALSE\nS                       FALSE            FALSE      FALSE\nP-N-L                   FALSE            FALSE      FALSE\nN-L                     FALSE            FALSE      FALSE\nP-L                     FALSE            FALSE      FALSE\nL                       FALSE            FALSE      FALSE\nP-N                     FALSE            FALSE      FALSE\nN                       FALSE            FALSE      FALSE\nP                       FALSE            FALSE      FALSE\n                        FALSE            FALSE      FALSE\n\n$regression_results\n    Coefficient          SE                Variable\n1  0.0692437554 0.008765654               Intercept\n2 -0.0248352010 0.006026387      Remove Punctuation\n3 -0.0012912608 0.006026387          Remove Numbers\n4  0.0008042152 0.006026387               Lowercase\n5  0.0315263364 0.006026387                Stemming\n6 -0.0031236074 0.006026387        Remove Stopwords\n7 -0.0194667091 0.006026387 Remove Infrequent Terms\n8 -0.0194780796 0.006026387              Use NGrams\n                Model\n1 Gun Pretext Results\n2 Gun Pretext Results\n3 Gun Pretext Results\n4 Gun Pretext Results\n5 Gun Pretext Results\n6 Gun Pretext Results\n7 Gun Pretext Results\n8 Gun Pretext Results\n\n\n\n#load(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/preText_results_gun_50_comps.RData\")\n\npreText_score_plot(preText_results)\n\n\n\n\nExplanation: After plotting we access the pretext\nscore with the minimum score, which is least unusual. This is the row\nwith the pre-processing steps refered to as “3” in the data. In addition\nL-3 results in the same preText Score.\n\n\nscores_new_pretext<-preText_results$preText_score \n\n# head(sort(scores_new_pretext))\n\n\n\nExplanation: Looking at the choices below I see that\n“3” does not do anything but use n-grams. L-3 does use lowercase and\nn-grams.\n\n\n# preprocessed_documents$choices\n\n\n\nExplanation Continued: Looking at the regression\ncoefficients we see negative scores as usual results and positive\ncoefficients as unusual ones. In this case removing puncuation,\nstopwords, and n-grams would not lead to a great deal of abnormality.\nThe scores below indicate that stemming would result in the most\nabnormality while all others but lowercase is the only other that has a\nnon-negative coefficinet.\n\n\nregression_coefficient_plot(preText_results,\n                            remove_intercept = TRUE)\n\n\n\n\nFeature Co-occurance Matrix\nExplanation: The feature co-occurance matrix can\ngive us a sense of which words in the dataset are occurring together\n\n\n# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of chapters\nsmaller_dfm <- dfm_trim(new_guns_corpus_dfm_punct_tl_sw, min_termfreq = 5)\n\n#smaller_dfm <- dfm_trim(smaller_dfm, min_docfreq = .3, docfreq_type = \"prop\")\n\n# create fcm from dfm\nsmaller_fcm <- fcm(smaller_dfm)\n\n# check the dimensions (i.e., the number of rows and the number of columnns)\n# of the matrix we created\ndim(smaller_fcm)\n\n\n[1] 253 253\n\n\n\n# pull the top features\nmyFeatures <- names(topfeatures(smaller_fcm, 40))\n\n# retain only those top features as part of our matrix\neven_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = \"keep\")\n\n# check dimensions\ndim(even_smaller_fcm)\n\n\n[1] 40 40\n\n# compute size weight for vertices in network\nsize <- log(colSums(even_smaller_fcm))\n\n# create plot\ntextplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)\n\n\n\n\nSentiment Results Using NRC\n\n\n# get_sentiments(\"nrc\")\n# get_sentiments(\"bing\")\n# get_sentiments(\"afinn\")\n\n\n\n\n\nsentimetnsdf <- read_csv(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/sentimetnsdf.csv\")\n\n\n\n\n\nnew_guns_urls_df_2<-new_guns_urls_df\n\nnew_guns_urls_df_2$text<- seq(1, 996, by=1)\n\nnrc_joy <- sentimetnsdf %>% \n  filter(sentiment == \"joy\")\n\ntidy_posts_for_guns <- new_guns_urls_df_2 %>%\n  unnest_tokens(word, title) \n\ntidy_posts_for_guns %>%\n  inner_join(nrc_joy) %>%\n  count(word, sort = TRUE) %>% head() %>% kable()\n\n\nword\nn\nfinally\n19\nhappy\n18\nfound\n17\nbirthday\n15\ngift\n12\nlove\n12\n\n\n\ntidy_posts_for_guns_sentiment <- tidy_posts_for_guns %>%\n  inner_join(sentimetnsdf) %>%\n  count(text, sentiment) %>%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% \n  mutate(sentiment = positive - negative)\n\n\n\n\n\nnrc_sentiment <- get_sentiments(\"nrc\")\n\n\nnrc_guns_word_counts <- tidy_posts_for_guns %>%\n  inner_join(nrc_sentiment) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  ungroup()\n\nnrc_guns_word_counts %>%\n  group_by(sentiment) %>%\n  slice_max(n, n = 10) %>% \n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL)\n\n\n\n\n\n\nBing_sentiments<-get_sentiments(\"bing\")\n\ntidy_posts_for_guns_sentiment <- tidy_posts_for_guns %>%\n  inner_join(Bing_sentiments) %>%\n  count(text, sentiment) %>%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% \n  mutate(sentiment = positive - negative)\n\nbing_word_counts <- tidy_posts_for_guns %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  ungroup()\n\nbing_word_counts %>%\n  group_by(sentiment) %>%\n  slice_max(n, n = 10) %>% \n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL)\n\n\n\n\n\n\ntidy_posts_for_guns$added_dates <- as.Date(tidy_posts_for_guns$date_utc)\n\n\nafinn <- tidy_posts_for_guns %>% \n  inner_join(get_sentiments(\"afinn\")) %>% \n  group_by(index = added_dates) %>% \n  summarise(sentiment = sum(value)) %>% \n  mutate(method = \"AFINN\")\n\n\n\n\n\nafinn %>%\n  ggplot(aes(index, sentiment, fill = method)) +\n  geom_col(show.legend = FALSE,   width = 0.7)  + \n  geom_smooth(aes(y = sentiment), color = \"black\")+\nfacet_wrap(~method, ncol = 1, scales = \"free_y\")+\n  theme_minimal()\n\n\n\n\nSentiment Results Using\nBING\nExplanation: Using nrc appears to have had some\nunintended effects that may require an analysis of the specific words\nused to describe sentiment. One difficult part of the data being used is\nthat firearms, and the words used to describe them, are percieved\n\n\nlibrary(methods)\n\ntoo_gun_dfm<- quanteda::dfm(new_guns_corpus, verbose = FALSE)\n\ntoo_gun_dfm\n\n\nDocument-feature matrix of: 996 documents, 2,824 features (99.63% sparse) and 0 docvars.\n       features\ndocs    second ar build : 300blk pistol . can't not have\n  text1      1  1     1 1      1      1 2     1   1    1\n  text2      0  0     0 0      0      0 0     0   0    0\n  text3      0  0     0 0      0      0 0     0   0    0\n  text4      0  0     0 0      0      0 0     0   0    0\n  text5      0  0     0 0      0      0 0     0   0    0\n  text6      0  0     0 0      0      0 1     0   0    0\n[ reached max_ndoc ... 990 more documents, reached max_nfeat ... 2,814 more features ]\n\n\n\nlibrary(topicmodels)\n\ngun_dfm_lda <- LDA(too_gun_dfm, k = 2, control = list(seed = 777))\ngun_dfm_lda\n\n\nA LDA_VEM topic model with 2 topics.\n\n\n\ngun_dfm_lda_topics <- tidy(gun_dfm_lda, matrix = \"beta\")\ngun_dfm_lda_topics\n\n\n# A tibble: 5,648 × 3\n   topic term        beta\n   <int> <chr>      <dbl>\n 1     1 second 0.000430 \n 2     2 second 0.000100 \n 3     1 ar     0.00175  \n 4     2 ar     0.00108  \n 5     1 build  0.00177  \n 6     2 build  0.000704 \n 7     1 :      0.00276  \n 8     2 :      0.00325  \n 9     1 300blk 0.000303 \n10     2 300blk 0.0000503\n# … with 5,638 more rows\n\n\n\ngun_top_terms <- gun_dfm_lda_topics %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>% \n  ungroup() %>%\n  arrange(topic, -beta)\n\ngun_top_terms %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\n\nbeta_wide <- gun_dfm_lda_topics %>%\n  mutate(topic = paste0(\"topic\", topic)) %>%\n  pivot_wider(names_from = topic, values_from = beta) %>% \n  filter(topic1 > .001 | topic2 > .001) %>%\n  mutate(log_ratio = log2(topic2 / topic1))\n\nbeta_wide\n\n\n# A tibble: 202 × 4\n   term    topic1   topic2 log_ratio\n   <chr>    <dbl>    <dbl>     <dbl>\n 1 ar     0.00175 0.00108     -0.693\n 2 build  0.00177 0.000704    -1.33 \n 3 :      0.00276 0.00325      0.234\n 4 pistol 0.00187 0.000423    -2.15 \n 5 .      0.0701  0.0572      -0.294\n 6 not    0.00571 0.000829    -2.78 \n 7 have   0.00190 0.00235      0.304\n 8 -      0.00200 0.00349      0.805\n 9 /      0.00161 0.00388      1.27 \n10 it's   0.00344 0.000619    -2.48 \n# … with 192 more rows\n\n\n\nbeta_wide %>%\n  group_by(direction = log_ratio > 0) %>%\n  slice_max(abs(log_ratio), n = 10) %>% \n  ungroup() %>%\n  mutate(term = reorder(term, log_ratio)) %>%\n  ggplot(aes(log_ratio, term)) +\n  geom_col() +\n  labs(x = \"Log2 ratio of beta in topic 2 / topic 1\", y = NULL)\n\n\n\n\nTopic Modeling analysis\nResponse: As can be seen above topic modeling may\nbenefit from some data reduction, removing punctuation and stop words\nwould likely be beneficial as can be seen above where a number of the\ndifferences between topics are modeled as punctuation and stop\nwords.\n\n\ngun_tokens_stopwords_and_punct_removed <- tokens_remove(tokens(new_guns_corpus, remove_punct = TRUE), c(stopwords(\"english\")))\n\ngun_corpus_stopwords_and_punct_removed <- corpus(sapply(gun_tokens_stopwords_and_punct_removed, paste, collapse=\" \")\n)\n\ngun_corpus_stopwords_and_punct_removed\n\n\nCorpus consisting of 996 documents.\ntext1 :\n\"Second AR build 300blk pistol wood\"\n\ntext2 :\n\"Team Green Master Chief Doom Guy Wombo Combo\"\n\ntext3 :\n\"Klobberin Time\"\n\ntext4 :\n\"50k Round Deep Clean\"\n\ntext5 :\n\"Guess Cowboy Now\"\n\ntext6 :\n\"s night 50 time s plan\"\n\n[ reached max_ndoc ... 990 more documents ]\n\n\n\nlibrary(methods)\n\ntoo_gun_dfm_no_punct_stopwords <- dfm(tokens(gun_corpus_stopwords_and_punct_removed), verbose = FALSE)\n\n\n\n\n\n# In my data frame removing stopwords and punctuation I have a row of zeros which cannot be used in topic modeling as LDA does not work. I fix this by finding and removing the row or post that drops out.\n\n# raw.sum=apply(too_gun_dfm_no_punct_stopwords,1,FUN=sum) #sum by raw each raw of the table\n\n\ntop_gun_np_sw_all_row<-too_gun_dfm_no_punct_stopwords[rowSums(too_gun_dfm_no_punct_stopwords[])>0,]\n\nsum(rowSums(too_gun_dfm_no_punct_stopwords)==0)\n\n\n[1] 1\n\n\n\nlibrary(topicmodels)\n\n\ngun_dfm_lda_nopunct_stop <- LDA(top_gun_np_sw_all_row, k = 4, control = list(seed = 777))\n\ngun_dfm_lda_nopunct_stop\n\n\nA LDA_VEM topic model with 4 topics.\n\n\n\ngun_dfm_lda_topics_nopunct_stop <- tidy(gun_dfm_lda_nopunct_stop, matrix = \"beta\")\n\ngun_dfm_lda_topics_nopunct_stop\n\n\n# A tibble: 10,696 × 3\n   topic term        beta\n   <int> <chr>      <dbl>\n 1     1 second 2.76e- 39\n 2     2 second 1.90e-  3\n 3     3 second 5.88e- 19\n 4     4 second 4.08e-154\n 5     1 ar     7.98e-  3\n 6     2 ar     2.65e-  3\n 7     3 ar     2.17e- 26\n 8     4 ar     7.69e- 30\n 9     1 build  7.28e-  3\n10     2 build  2.34e-  6\n# … with 10,686 more rows\n\n\n\ngun_top_terms_no_punct_or_stop<- gun_dfm_lda_topics_nopunct_stop %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>% \n  ungroup() %>%\n  arrange(topic, -beta)\n\ngun_top_terms_no_punct_or_stop %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\n\nlibrary(topicmodels)\n\n\ngun_dfm_lda_nopunct_stop <- LDA(top_gun_np_sw_all_row, k = 8, control = list(seed = 777))\n\ngun_dfm_lda_nopunct_stop\n\n\nA LDA_VEM topic model with 8 topics.\n\n\n\ngun_dfm_lda_topics_nopunct_stop <- tidy(gun_dfm_lda_nopunct_stop, matrix = \"beta\")\n\ngun_dfm_lda_topics_nopunct_stop\n\n\n# A tibble: 21,392 × 3\n   topic term        beta\n   <int> <chr>      <dbl>\n 1     1 second 1.35e-  3\n 2     2 second 6.36e- 49\n 3     3 second 7.03e-269\n 4     4 second 3.79e-264\n 5     5 second 4.30e-270\n 6     6 second 1.39e-  3\n 7     7 second 7.30e-273\n 8     8 second 1.35e-  3\n 9     1 ar     2.16e-  2\n10     2 ar     2.06e- 33\n# … with 21,382 more rows\n\n\n\ngun_top_terms_no_punct_or_stop<- gun_dfm_lda_topics_nopunct_stop %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>% \n  ungroup() %>%\n  arrange(topic, -beta)\n\ngun_top_terms_no_punct_or_stop %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\n\nbeta_wide_no_punct_stop <- gun_dfm_lda_topics_nopunct_stop %>%\n  mutate(topic = paste0(\"topic\", topic)) %>%\n  pivot_wider(names_from = topic, values_from = beta) %>% \n  filter(topic1 > .001 | topic2 > .001) %>%\n  mutate(log_ratio = log2(topic2 / topic1))\n\n\n\n\n\nbeta_wide_no_punct_stop %>%\n  group_by(direction = log_ratio > 0) %>%\n  slice_max(abs(log_ratio), n = 10) %>% \n  ungroup() %>%\n  mutate(term = reorder(term, log_ratio)) %>%\n  ggplot(aes(log_ratio, term)) +\n  geom_col() +\n  labs(x = \"Log2 ratio of beta in topic 2 / topic 1\", y = NULL)\n\n\n\n\nTokens and Corpus Work\n\n\ntop_guns_tokens <- tokens(new_guns_corpus)\n\nprint(top_guns_tokens)\n\n\nTokens consisting of 996 documents.\ntext1 :\n [1] \"Second\" \"AR\"     \"build\"  \":\"      \"300blk\" \"pistol\" \".\"     \n [8] \"Can't\"  \"not\"    \"have\"   \"wood\"   \".\"     \n\ntext2 :\n [1] \"Team\"   \"Green\"  \"-\"      \"Master\" \"Chief\"  \"/\"      \"Doom\"  \n [8] \"Guy\"    \"Wombo\"  \"Combo\" \n\ntext3 :\n[1] \"It's\"      \"Klobberin\" \"'\"         \"Time\"      \"!\"        \n\ntext4 :\n[1] \"50k\"   \"Round\" \"Deep\"  \"Clean\"\n\ntext5 :\n[1] \"Guess\"  \"I'm\"    \"a\"      \"Cowboy\" \"Now\"   \n\ntext6 :\n [1] \"It\"    \"s\"     \"night\" \"50\"    \"%\"     \"of\"    \"the\"   \"time\" \n [9] \".\"     \"What\"  \"s\"     \"your\" \n[ ... and 2 more ]\n\n[ reached max_ndoc ... 990 more documents ]\n\ntop_guns_tokens_no_punct <- tokens(new_guns_corpus, \n    remove_punct = T)\n\nprint(top_guns_tokens_no_punct)\n\n\nTokens consisting of 996 documents.\ntext1 :\n[1] \"Second\" \"AR\"     \"build\"  \"300blk\" \"pistol\" \"Can't\"  \"not\"   \n[8] \"have\"   \"wood\"  \n\ntext2 :\n[1] \"Team\"   \"Green\"  \"Master\" \"Chief\"  \"Doom\"   \"Guy\"    \"Wombo\" \n[8] \"Combo\" \n\ntext3 :\n[1] \"It's\"      \"Klobberin\" \"Time\"     \n\ntext4 :\n[1] \"50k\"   \"Round\" \"Deep\"  \"Clean\"\n\ntext5 :\n[1] \"Guess\"  \"I'm\"    \"a\"      \"Cowboy\" \"Now\"   \n\ntext6 :\n [1] \"It\"    \"s\"     \"night\" \"50\"    \"of\"    \"the\"   \"time\"  \"What\" \n [9] \"s\"     \"your\"  \"plan\" \n\n[ reached max_ndoc ... 990 more documents ]\n\n\n\ntop_guns_tokens_no_punct_no_upper <- tokens_tolower(top_guns_tokens_no_punct)\n\nprint(top_guns_tokens_no_punct_no_upper)\n\n\nTokens consisting of 996 documents.\ntext1 :\n[1] \"second\" \"ar\"     \"build\"  \"300blk\" \"pistol\" \"can't\"  \"not\"   \n[8] \"have\"   \"wood\"  \n\ntext2 :\n[1] \"team\"   \"green\"  \"master\" \"chief\"  \"doom\"   \"guy\"    \"wombo\" \n[8] \"combo\" \n\ntext3 :\n[1] \"it's\"      \"klobberin\" \"time\"     \n\ntext4 :\n[1] \"50k\"   \"round\" \"deep\"  \"clean\"\n\ntext5 :\n[1] \"guess\"  \"i'm\"    \"a\"      \"cowboy\" \"now\"   \n\ntext6 :\n [1] \"it\"    \"s\"     \"night\" \"50\"    \"of\"    \"the\"   \"time\"  \"what\" \n [9] \"s\"     \"your\"  \"plan\" \n\n[ reached max_ndoc ... 990 more documents ]\n\n\n\ntop_guns_tokens_no_punct_no_upper_no_stop <-    tokens_select(top_guns_tokens_no_punct_no_upper, pattern = stopwords(\"en\"), selection = \"remove\")\n\nlength(top_guns_tokens_no_punct_no_upper_no_stop)\n\n\n[1] 996\n\nprint(top_guns_tokens_no_punct_no_upper_no_stop)\n\n\nTokens consisting of 996 documents.\ntext1 :\n[1] \"second\" \"ar\"     \"build\"  \"300blk\" \"pistol\" \"wood\"  \n\ntext2 :\n[1] \"team\"   \"green\"  \"master\" \"chief\"  \"doom\"   \"guy\"    \"wombo\" \n[8] \"combo\" \n\ntext3 :\n[1] \"klobberin\" \"time\"     \n\ntext4 :\n[1] \"50k\"   \"round\" \"deep\"  \"clean\"\n\ntext5 :\n[1] \"guess\"  \"cowboy\" \"now\"   \n\ntext6 :\n[1] \"s\"     \"night\" \"50\"    \"time\"  \"s\"     \"plan\" \n\n[ reached max_ndoc ... 990 more documents ]\n\n\n\ntop_guns_corpus_tokens <- tokens(new_guns_corpus)\n\nprint(top_guns_corpus_tokens)\n\n\nTokens consisting of 996 documents.\ntext1 :\n [1] \"Second\" \"AR\"     \"build\"  \":\"      \"300blk\" \"pistol\" \".\"     \n [8] \"Can't\"  \"not\"    \"have\"   \"wood\"   \".\"     \n\ntext2 :\n [1] \"Team\"   \"Green\"  \"-\"      \"Master\" \"Chief\"  \"/\"      \"Doom\"  \n [8] \"Guy\"    \"Wombo\"  \"Combo\" \n\ntext3 :\n[1] \"It's\"      \"Klobberin\" \"'\"         \"Time\"      \"!\"        \n\ntext4 :\n[1] \"50k\"   \"Round\" \"Deep\"  \"Clean\"\n\ntext5 :\n[1] \"Guess\"  \"I'm\"    \"a\"      \"Cowboy\" \"Now\"   \n\ntext6 :\n [1] \"It\"    \"s\"     \"night\" \"50\"    \"%\"     \"of\"    \"the\"   \"time\" \n [9] \".\"     \"What\"  \"s\"     \"your\" \n[ ... and 2 more ]\n\n[ reached max_ndoc ... 990 more documents ]\n\n\n\nhead(annotated.guns_corpus$token)\n\n\n# A tibble: 6 × 11\n  doc_id   sid tid   token  token_with_ws lemma  upos  xpos  feats    \n   <int> <int> <chr> <chr>  <chr>         <chr>  <chr> <chr> <chr>    \n1      1     1 1     Second \"Second \"     second ADJ   JJ    Degree=P…\n2      1     1 2     AR     \"AR \"         ar     NOUN  NN    Number=S…\n3      1     1 3     build  \"build\"       build  NOUN  NN    Number=S…\n4      1     1 4     :      \": \"          :      PUNCT :     <NA>     \n5      1     1 5     300    \"300\"         300    NUM   CD    NumType=…\n6      1     1 6     blk    \"blk \"        blk    NOUN  NN    Number=S…\n# … with 2 more variables: tid_source <chr>, relation <chr>\n\nhead(annotated.guns_corpus$document)\n\n\n  doc_id\n1      1\n2      2\n3      3\n4      4\n5      5\n6      6\n\ndoc_id_guns<-annotated.guns_corpus$document\n\ndoc_id_guns$date<-new_guns_urls_df$date_utc\n\nannoData <- left_join(doc_id_guns, annotated.guns_corpus$token, by = \"doc_id\")\n\nannoData$date<-as.Date(annoData$date)\n\n\n\n\n\nannoData %>% \n  group_by(date) %>% \n  summarize(Sentences = max(sid)) %>%\n  ggplot(aes(date, Sentences)) +\n    geom_line() +\n    geom_smooth() +\n    theme_bw()\n\n\n\n\n\n\n#sentimetnsdf<-get_sentiments(\"nrc\")\n\n#write.csv(sentimetnsdf, file = \"sentimetnsdf.csv\")\n\n#save(sentimetnsdf, file=\"sentimetnsdf_2\")\n\n\n\nUnsupervised Learning KNN\n\n\n\n",
    "preview": "posts/2022-03-29-tad-post-4/tad-post-4_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2022-03-31T12:46:25-04:00",
    "input_file": "tad-post-4.knit.md"
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to My Blog",
    "description": "Welcome to our new blog, My Blog. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-03-29",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "preview": "posts/welcome/welcome_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-03-29T13:42:03-04:00",
    "input_file": {}
  }
]
