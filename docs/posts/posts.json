[
  {
    "path": "posts/welcome/",
    "title": "Welcome to My Blog",
    "description": "Welcome to our new blog, My Blog. We hope you enjoy \nreading what we have to say!",
    "author": [
      {
        "name": "Nora Jones",
        "url": {}
      }
    ],
    "date": "2022-03-29",
    "categories": [],
    "contents": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "preview": "posts/welcome/welcome_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-03-29T13:23:16-04:00",
    "input_file": "welcome.knit.md"
  },
  {
    "path": "posts/2022-03-29-blog-post-7-networks/",
    "title": "Blog Post 7 Networks",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-03-29",
    "categories": [],
    "contents": "\n\n\n\n\n\n\nAn Introduction to\nthe Project and Dataset\nThe project that I am doing involves conflict in\nthe high middle ages. This was the period between 1000 and 1200\n\n\n\nPart 1:\nDescribe the Dataset You Are\nUsing:\nThe Dataset Being Used: The dataset that I am using\nis wikipedia list of wars throughout history, this article is the “List\nof wars: 1000–1499” which acts as a subset of the “2nd-millennium\nconflicts” I chose this dataset as an exemplar of popular history’s\ndepiction of the centralization of worldwide conflict. Wikipedia, being\nan accessible source generally created from relevant citations makes it\na good case study to see where historical writers and academics center\ntheir world are relevant conflicts.\nIdentify initial network\nformat:\nAnswer: The initial network format is as an edge\nlist, the first, in column contains the winners of each\nwar while the second, out column contains the losers of\neach. These sets of belligerents are directed\nNetwork\nStructure: Wars Startings in the 1000s\n\n Network attributes:\n  vertices = 111 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 153 \n    missing edges= 0 \n    non-missing edges= 153 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nNetwork\nStructure: Wars Startings in the 1100s\n\n Network attributes:\n  vertices = 97 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 238 \n    missing edges= 0 \n    non-missing edges= 238 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nNetwork Structure:\nWars Starting in the 1200s\n\n Network attributes:\n  vertices = 161 \n  directed = TRUE \n  hyper = FALSE \n  loops = FALSE \n  multiple = TRUE \n  bipartite = FALSE \n  total edges= 313 \n    missing edges= 0 \n    non-missing edges= 313 \n\n Vertex attribute names: \n    vertex.names \n\nNo edge attributes\n\nIdentify Nodes: Describe and identify the nodes\n(including how many nodes are in the dataset)\nAnswer: Nodes or vertices in these datasets\nrepresent belligerents in wars throughout history, the involved parties\nin each conflict can be a nation, province, individual, or group so long\nas they are listed as involved in the conflict. In the 1000s there are\n117, in the 1100s there are 78 and in the 1200s there are 161.\nWhat Constitutes a Tie: What constitutes a tie or\nedge (including how many ties, whether ties are directed/undirected and\nweighted/binary, and how to interpret the value of the tie if any)\nAnswer: A tie or edge in this dataset represents a\nwar, this war can be between two nations or groups within a nation.\nThese edges can represent a war that involved many more nations but are\nalways tied to each and every party involved on both sides. These edges\nare directed and the direction indicates which side “won” the conflict\n(if an edge has an arrow pointing to another the node that originated\nthat arrow won the war against them. There are 153 edges in the 1000s,\n225 edges in 1100s and 313 edges in the 1200s.\nEdge Attributes and Subset: Whether or not there are\nedge attributes that might be used to subset data or stack multiple\nnetworks (e.g., tie type, year, etc).\nAnswer: There are a number of attributes that could\nbe used to subset the data, year that the conflict began or the length\nof time it lasted are available. Aspects like each side’s religion and\nthe area where the conflict took place could be used to subset the data\nitself.\nPart 2:\nBrokerage and Betweeness\ncentrality\nWhat are betweeness and brokerage cenrrality\nCalculate brokerage and betweenneess centrality measures for one or more\nsubsets of your network data, and write up the results and your\ninterpretation of them.\nAnswer: I will be calculating these measures for\nwars in 1000-1099, 1100-1199, and 1200-1399.\n\n\n\nBrokerage scores in the\n1000s\n\n\n\n\n\n(wars_in_1000s.nodes.stat_2%>%\n  arrange(desc(broker.tot))%>%\n  slice(1:10))[,c(1,11:15)] %>%kable()\n\n\nname\nbroker.tot\nbroker.coord\nbroker.itin\nbroker.rep\nbroker.gate\nByzantine Empire\n22.7376579\nNaN\n3.1654785\nNaN\nNaN\nHoly Roman Empire\n9.2813605\nNaN\n2.2468427\nNaN\nNaN\nSultanate of Rum\n9.2813605\nNaN\n-0.5090648\nNaN\nNaN\nEngland\n6.9745666\nNaN\n5.0036896\n-0.0853606\n-0.0853606\nKingdom of Sicily\n5.0522384\n-0.0176111\n4.0866123\n-0.1201631\n-0.1201631\nSeljuk Empire\n1.9765133\n-0.0176111\n-0.5084146\n3.4677529\n-0.1201631\nKingdom of France\n1.9765133\nNaN\n-0.5090648\nNaN\nNaN\nKingdom of Georgia\n0.8231164\n-0.0176111\n-0.5084146\n-0.1201631\n-0.1201631\nPapal States\n0.4386507\n-0.0176111\n-0.5084146\n-0.1201631\n10.6435850\nGhaznavids\n0.0541851\n-0.1380791\n-0.4907567\n-0.2903366\n-0.2903366\n\nBrokerage scores in the\n1100s\n\n\n\n\n\n\n\n\n(wars_in_1100s.nodes.stat_2%>%\n  arrange(desc(broker.tot))%>%\n  slice(1:10))[,c(1,10:14)] %>%kable()\n\n\nname\nbroker.tot\nbroker.coord\nbroker.itin\nbroker.rep\nbroker.gate\nKingdom of Jerusalem\n17.1050061\nNaN\n2.8705599\n24.5610650\n-0.1357675\nFatimid Caliphate\n10.2415178\nNaN\n-0.6472506\nNaN\nNaN\nAyyubid Dynasty\n9.3615834\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nZengid Dynasty\n7.4257278\nNaN\n0.7591543\nNaN\nNaN\nByzantine Empire\n6.8977671\nNaN\n0.7602887\n-0.1357675\n-0.1357675\nEngland\n5.8418459\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nHoly Roman Empire\n3.0260558\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nKingdom of France\n1.6181608\nNaN\n-0.6465587\n-0.1357675\n-0.1357675\nKingdom of Sicily\n0.5622395\n-0.1467125\n-0.6293842\n-0.3476788\n-0.3476788\nPapal States\n0.0342789\n-0.1264908\n-0.6336748\n-0.3236913\n2.5014147\n\nBrokerage scores in the\n1200s\n\n\n\n\n\n\n\n\n\n\nname\nbroker.tot\nbroker.coord\nbroker.itin\nbroker.rep\nbroker.gate\nMongol Empire\n47.964825\nNaN\n-0.5966483\nNaN\nNaN\nKingdom of France\n28.663539\nNaN\n-0.5966483\nNaN\nNaN\nAyyubid Dynasty\n26.995527\nNaN\n2.3528915\nNaN\nNaN\nKingdom of England\n21.991489\nNaN\n8.9893561\nNaN\nNaN\nRepublic of Genoa\n11.983415\nNaN\n-0.5966483\nNaN\nNaN\nKnights Templar\n10.077115\nNaN\n1.6155066\nNaN\nNaN\nHoly Roman Empire\n4.834790\n-0.0170801\n-0.5961482\n10.865523\n10.865523\nPrincipality of Antioch\n4.834790\n-0.0170801\n2.3541101\n13.613565\n-0.126648\nKingdom of Cyprus\n4.596503\n58.5391124\n0.1414163\n13.613565\n10.865523\nArmenian Kingdom of Cilicia\n3.881640\n-0.0170801\n-0.5961482\n-0.126648\n-0.126648\n\n\nname\nbroker.gate\nPapal States\n10.6435850\nCounty of Aversa\n-0.0853606\nCounty of Sicily\n-0.0853606\nEngland\n-0.0853606\nChola Empire\n-0.0853606\nCounty of Apulia\n-0.1201631\nKingdom of Sicily\n-0.1201631\nKingdom of Georgia\n-0.1201631\nGreat Seljuq Empire\n-0.1201631\nSeljuk Empire\n-0.1201631\nname\nbroker.tot\nByzantine Empire\n22.7376579\nHoly Roman Empire\n9.2813605\nSultanate of Rum\n9.2813605\nEngland\n6.9745666\nKingdom of Sicily\n5.0522384\nSeljuk Empire\n1.9765133\nKingdom of France\n1.9765133\nKingdom of Georgia\n0.8231164\nPapal States\n0.4386507\nGhaznavids\n0.0541851\n\nOption 2.A\nFor a Specific Research Question: If you have a\nspecific research question, please feel free to use that to guide your\nanalysis. Otherwise, you may want to orient your analysis as follows in\norder to identify a compelling question or noteworthy pattern in the\ndata that can be interpreted.\nAnswer: Since I am interested in the relative power\nof nations by their relative position ad centrality in the worldwide\nconflict, network brokerage can be used to illustrate significant\npositions in global conflict. Below I wanted to look at 4 kinds of\nbrokerage, these are broker.gate or gatekeeper, coordinator, liason, and\nitinerant. I am interested to see if these specific coordination types\nare primarily done by specific nations.\n\n\n\n\n\n\n\n\n\nTotal Brokerage\nExplanation: Looking at total brokerage in this\ndataset gives a sense of which factions were responsible for highest\nconnection of unconnected actors through conflict. Given the crusades\nigniting conflict between Europe and the middle east it is sensible that\nthe Byzantine Empire in the center of both connects the most unconnected\nactors through conflict closely followed by the Sultanate of Rum, a\nmajor Muslim faction that fought against the crusades and third being\nthe Holy Roman Empire who participated in many conflicts including the\ncrusades. These are followed by England who centered the wars in the\nBritish isles and the Kingdom of Sicily who were also in a position of\nconflict.\n\nname\nbroker.tot\nByzantine Empire\n22.737658\nHoly Roman Empire\n9.281360\nSultanate of Rum\n9.281360\nEngland\n6.974567\nKingdom of Sicily\n5.052238\n\nCoordinator Brokerage\nExplanation: In this case no particular country is\nvery high above any other in terms of their coordinator brokerage,\nmeaning that within groups no particular nations appear to be brokering\nmore within the groups.\n\nname\nbroker.coord\nCounty of Apulia\n-0.0176111\nKingdom of Sicily\n-0.0176111\nKingdom of Georgia\n-0.0176111\nGreat Seljuq Empire\n-0.0176111\nPapal States\n-0.0176111\n\nItinerant Brokerage\nExplanation: Itinerant brokerage represents when a\nnon-group actor connects 2 actors in a group it is no in to each other,\nin this case England has the highest score. Looking at the network graph\nthey do appear to connect 2 actors in a group together.\n\nname\nbroker.itin\nEngland\n5.0036896\nKingdom of Sicily\n4.0866123\nByzantine Empire\n3.1654785\nHoly Roman Empire\n2.2468427\nPrincipality of Kiev\n0.4812412\n\nRepresentative Brokerage\nExplanation: Representative brokerage indicates that\nthe broker, or nation in question loses a war to another in their group,\nbut wins another against a faction outside of their group. This can be\nthough of as their directed connections to them. In this case the Seljuk\nEmpire and Kingdom of Aragon have instances in which they lose to\nfactions within their group before beating those outside of it.\n\nname\nbroker.rep\nSeljuk Empire\n3.4677529\nKingdom of Aragon\n0.9281821\nCounty of Aversa\n-0.0853606\nCounty of Sicily\n-0.0853606\nEngland\n-0.0853606\n\nGatekeeper Brokerage\nExplanation: The Papal states being ranked highest\nin gatekeeper brokerage is an interesting observation as no other nation\nin the dataset appears to be close to their level as most are negative\nin this category. In this cae being a gatekeeper means that they are in\nat conflict in a group with another while the nation in a different\ngroup of conflicts is only at war with them from the group. This is an\ninteresting observation given the Papal states role as a coordinator of\nthe war, but not a participant in the conflcit as directly as other\nbelligerents. (This being the crusade given the period)\n\nname\nbroker.gate\nPapal States\n10.6435850\nCounty of Aversa\n-0.0853606\nCounty of Sicily\n-0.0853606\nEngland\n-0.0853606\nChola Empire\n-0.0853606\n\nLiaison Brokerage\nExplanation: A liaison broker, in this case, is a\nfaction that loses a war to a group they do not belong to and wins a war\nagainst a different group than the first that they also do not belong\nto. The Byzantine Empire, Sultanate of Rum, and Holy Roman Empire are\nhighest in this category likely owing to their frequent states of\nconflict beyond the crusades against a variety of groups.\n\nname\nbroker.lia\nByzantine Empire\n28.140866\nSultanate of Rum\n12.477603\nHoly Roman Empire\n10.961803\nEngland\n6.548214\nKingdom of Sicily\n4.589419\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork\n1000s Plot Grouping Determined with No Cluster Method\n\n\n\nNetwork\n1000s Plot Grouping Determined with the Average Cluster Method\n\n\n\nNetwork\n1000s Plot Grouping Determined with the Single Cluster Method\n\n\n\nNetwork\n1000s Plot Grouping Determined with the Ward.D Cluster Method\n\n\n\nNetwork 1000s Plot igraph\n\n\n\nNetwork Graphing 1100s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork\n1100s Plot Grouping Determined with No Cluster Method\n\n\n\nNetwork\n1100s Plot Grouping Determined with the Average Cluster Method\n\n\n\nNetwork\n1100s Plot Grouping Determined with the Single Cluster Method\n\n\n\nNetwork\n1100s Plot Grouping Determined with the Ward.D Cluster Method\n\n\n\nNetwork 1100s Plot igraph\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwars_in_1000s_edgelist <- as.matrix(wars_in_1000s)\n\nwars_in_1000s_edgelist_network_edgelist <- graph.edgelist(wars_in_1000s_edgelist, directed=TRUE)\n\nwars_in_1000s.ig<-graph_from_data_frame(wars_in_1000s)\n\nwars_in_1000s_network <- asNetwork(wars_in_1000s.ig)\n\n\n\n\n\naspects_of_1000s_states <- read_excel(\"~/Desktop/Spring 2022/Networks/aspects_of_1000s_states.xlsx\")\n\ntotal_1000s <- merge(aspects_of_1000s_states, wars_in_1000s.nodes.stat_2, by=\"name\")\n\n\n\n\n\ntotal_1000s_brokerag_reg<-total_1000s\n\ntotal_1000s_brokerag_reg$win_rate <- (total_1000s_brokerag_reg$outdegree/total_1000s_brokerag_reg$totdegree)\n\ntotal_1000s_brokerag_reg$loss_rate <- (total_1000s_brokerag_reg$indegree/total_1000s_brokerag_reg$totdegree)\n\ntotal_1000s_brokerag_reg_binom <- total_1000s_brokerag_reg %>% mutate(more_win_or_loss = case_when(\n  win_rate < 0.5 ~ 0,\n    win_rate >= 0.5 ~ 1))\n\nFirst_1000s_regression <- glm(more_win_or_loss~.-name-totdegree-indegree-outdegree-dc-eigen.dc-win_rate-loss_rate, total_1000s_brokerag_reg_binom, family=binomial)\n\nFirst_1000s_regression\n\n\n\nCall:  glm(formula = more_win_or_loss ~ . - name - totdegree - indegree - \n    outdegree - dc - eigen.dc - win_rate - loss_rate, family = binomial, \n    data = total_1000s_brokerag_reg_binom)\n\nCoefficients:\n (Intercept)      Catholic         Islam      Orthodox      Buddhist  \n  -2.090e+01     1.446e-01    -7.108e-02    -4.043e-01    -8.572e-02  \n       Pagan      Tengrism        Shinto         Hindu     Shamanism  \n   5.506e-01    -5.656e+01     1.820e+00    -2.142e+00    -1.506e+00  \n       eigen         close            rc      eigen.rc    broker.tot  \n  -1.877e+03     5.146e+03    -3.979e+00     1.574e+03     2.378e+02  \nbroker.coord   broker.itin    broker.rep   broker.gate    broker.lia  \n  -9.610e+01    -9.449e+01    -7.164e+01    -2.810e+01    -1.298e+02  \n\nDegrees of Freedom: 101 Total (i.e. Null);  82 Residual\n  (8 observations deleted due to missingness)\nNull Deviance:      140.8 \nResidual Deviance: 4.53e-09     AIC: 40\n\n\n\nset.seed(292)\n\ntotal_1000s_for_regression <- total_1000s[,-c(1, 20:25)]\n\ntotal_1000s_for_regression$win_rate <- (total_1000s_for_regression$outdegree/total_1000s_for_regression$totdegree)\n\ntotal_1000s_for_regression$loss_rate <- (total_1000s_for_regression$indegree/total_1000s_for_regression$totdegree)\n\ntotal_1000s_for_regression <- total_1000s_for_regression %>% mutate(more_win_or_loss = case_when(\n  win_rate < 0.5 ~ 0,\n    win_rate >= 0.5 ~ 1))\n\nFirst_1000s_regression <- glm(more_win_or_loss~.-loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, total_1000s_for_regression, family=binomial)\n\nFirst_1000s_regression\n\n\n\nCall:  glm(formula = more_win_or_loss ~ . - loss_rate - win_rate - totdegree - \n    indegree - outdegree - dc - eigen.dc, family = binomial, \n    data = total_1000s_for_regression)\n\nCoefficients:\n(Intercept)     Catholic        Islam     Orthodox     Buddhist  \n   -15.1948      13.9008      12.7531      14.6893      15.0858  \n      Pagan     Tengrism       Shinto        Hindu    Shamanism  \n     0.9610      11.6691      16.0623       9.1358      -0.1497  \n      eigen        close           rc     eigen.rc  \n   -82.1100     256.5294      -3.3322     -17.3152  \n\nDegrees of Freedom: 109 Total (i.e. Null);  96 Residual\nNull Deviance:      152.3 \nResidual Deviance: 58.4     AIC: 86.4\n\n\n\nset.seed(6738)\n\nin_training<- sample(1:nrow(total_1000s_for_regression),  nrow(total_1000s_for_regression) * 0.7 )\n\ntraining_1000s <- total_1000s_for_regression[in_training,]\n\ntest_1000s <- total_1000s_for_regression[-in_training,]\n\nlm_1000s_binom_subset_1 <- glm(more_win_or_loss~.-loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, total_1000s_for_regression, family=binomial, subset = in_training )\n\nlogsitic_1_1000s_prob <- predict(lm_1000s_binom_subset_1, test_1000s,\ntype = \"response\")\n\nlog_preds_1<-ifelse(logsitic_1_1000s_prob >= 0.5, 1, 0)\n\nprediction_1_logs <-mean(log_preds_1 == test_1000s$more_win_or_loss)\n\nprediction_1_logs %>% kable()\n\n\nx\n0.9090909\n\n\n\nlibrary(glmnet)\nlibrary(MASS)\n\n\n\n\n\nset.seed(246)\n\nx_ridge <- model.matrix(more_win_or_loss ~ .-loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, total_1000s_for_regression)[, -1] \n\ny_ridge <- total_1000s_for_regression$more_win_or_loss\n\ngrid <- 10^seq(10, -2, length = 100)\n\nridge.mod <- glmnet(x_ridge, y_ridge, alpha = 0, lambda = grid)\n\ndim(coef(ridge.mod))\n\n\n[1]  14 100\n\n\n\nset.seed(729)\ntrain_ridge <- sample(1:nrow(x_ridge), nrow(x_ridge)*0.8 ) \n\ntest_ridge <- (-train_ridge)\n\ny.test_ridge <- y_ridge[test_ridge]\n\n\n\n\n\nset.seed(9292)\n\nridge.mod <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], \n                    alpha = 0, lambda = grid, thresh = 1e-12)\n\nridge.pred <- predict(ridge.mod, s = 4, newx = x_ridge[test_ridge,])\n\nmean((ridge.pred - y.test_ridge)^2) %>% kable()\n\n\nx\n0.2416376\n\n\n\nset.seed(231)\nridge.pred <- predict(ridge.mod, s = 0, newx = x_ridge[test_ridge, ], \n                      exact = T, x = x_ridge[train_ridge, ], y = y_ridge[train_ridge])\n\npredict(ridge.mod, s = 0, exact = T, type = \"coefficients\", \n        x = x_ridge[train_ridge, ], y = y_ridge[train_ridge])[1:14, ]\n\n\n(Intercept)    Catholic       Islam    Orthodox    Buddhist \n 0.21024033  0.21827317 -0.01160454  0.21312966  0.35601806 \n      Pagan    Tengrism      Shinto       Hindu   Shamanism \n 0.08955257  0.14069809  0.38278477 -0.07034364 -0.01038790 \n      eigen       close          rc    eigen.rc \n-4.61480591 12.51011844 -0.29977861  4.64835194 \n\n\n\nset.seed(9292)\n\ncv.out <- cv.glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], alpha = 0) \n\nplot(cv.out)\n\n\n\n\n\n\nset.seed(9292)\n\nbestlam <- cv.out$lambda.min\n\nbestlam\n\n\n[1] 0.415338\n\n\n\nset.seed(9292)\n\nridge.pred <- predict(cv.out, s = bestlam, newx = x_ridge[test_ridge,])\n\nmean((ridge.pred - y.test_ridge)^2) %>% kable()\n\n\nx\n0.174632\n\n\n\nset.seed(2897)\n\nx_lasso <- model.matrix(more_win_or_loss ~ .-loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, total_1000s_for_regression)[, -1] \n\ny_lasso <- total_1000s_for_regression$more_win_or_loss\n\ngrid <- 10^seq(10, -2, length = 100)\n\nlasso.mod <- glmnet(x_lasso, y_lasso, alpha = 0, lambda = grid)\n\ndim(coef(lasso.mod))\n\n\n[1]  14 100\n\n\n\nset.seed(729)\n\ntrain_lasso <- sample(1:nrow(x_ridge), nrow(x_ridge)*0.8 ) \n\ntest_lasso <- (-train_lasso)\n\ny.test_lasso <- y_lasso[test_lasso]\n\n\n\n\n\nset.seed(9292)\n\nlasso.mod <- glmnet(x_lasso[train_lasso, ], y_lasso[train_lasso], \n                    alpha = 1, lambda = grid)\n\nplot(lasso.mod)\n\n\n\n\n\n\nset.seed(1029)\n\ncv.out_2 <- cv.glmnet(x_lasso[train_lasso, ], y_lasso[train_lasso], alpha = 1) \n\nplot(cv.out_2)\n\n\n\n\n\n\nset.seed(1920)\n\nbestlam_2 <- cv.out_2$lambda.min\n\nlasso.pred <- predict(cv.out_2, s = bestlam_2, newx = x_ridge[test_ridge,])\n\nmean((lasso.pred - y.test_ridge)^2) %>% kable()\n\n\nx\n0.1749583\n\n\n\nset.seed(2739)\n\nout <- glmnet(x_lasso[train_lasso, ], y_lasso[train_lasso], \n              alpha = 1, lambda = grid)\n\nlasso.coef <- predict(out, type = \"coefficients\", s = bestlam_2)[1:14, ]\n\nlasso.coef\n\n\n(Intercept)    Catholic       Islam    Orthodox    Buddhist \n 0.42561685  0.05577020 -0.09275344  0.00000000  0.00000000 \n      Pagan    Tengrism      Shinto       Hindu   Shamanism \n 0.00000000  0.00000000  0.00000000  0.00000000  0.00000000 \n      eigen       close          rc    eigen.rc \n 0.00000000  3.22570629 -0.21240622  0.00000000 \n\n\n\naspects_of_1100s_states <- read_excel(\"~/Desktop/Spring 2022/Networks/aspects_of_1100s_states.xlsx\")\n\ntotal_1100s <- merge(aspects_of_1100s_states, wars_in_1100s.nodes.stat_2, by=\"name\")\n\n\n\n\n\naspects_of_1200s_states <- read_excel(\"~/Desktop/Spring 2022/Networks/aspects_of_1200s_states.xlsx\")\n\ntotal_1200s <- merge(aspects_of_1200s_states, wars_in_1200s.nodes.stat_2, by=\"name\")\n\n\n\nCommunity Grouping\nLabel Propagation 1000s:\nThe first community cluster below is done using label propagation.\nThis results in 39 groups\n\n\nset.seed(23)\ncomm.lab<-label.propagation.community(wars_in_1000s.ig)\n#Inspect clustering object\n# igraph::groups(comm.lab)\n\n\n\n\n\n\nWalktrap 1000s:\nWalktrap classification as seen below results in 19 distinct\ncommunities.\n\n\nset.seed(238)\n#Run clustering algorithm: fast_greedy\nwars_in_1000s.wt<-walktrap.community(wars_in_1000s.ig)\n\n#igraph::groups(wars_in_1000s.wt)\n\n\n\nAdding more steps resulted in 19 groups for both 10 and 20 steps.\n\n\n#Run & inspect clustering algorithm: 10 steps\n#igraph::groups(walktrap.community(wars_in_1000s.ig, steps=10)) \n#Run & inspect clustering algorithm: 20 steps\n#igraph::groups(walktrap.community(wars_in_1000s.ig ,steps=20))\n#Run & inspect clustering algorithm\n\n\n\n\n\n\nMachine\nLearning, Regression and Principle Components:\n\n\ntotal_1000s_for_PCA <- total_1000s_brokerag_reg_binom[-c(20:27)]\n\napply(total_1000s_for_PCA[-1], 2, mean)\n\n\n        Catholic            Islam         Orthodox         Buddhist \n     0.454545455      0.181818182      0.154545455      0.063636364 \n           Pagan         Tengrism           Shinto            Hindu \n     0.036363636      0.018181818      0.054545455      0.045454545 \n       Shamanism        totdegree         indegree        outdegree \n     0.009090909      2.754545455      1.336363636      1.418181818 \n           eigen            close               rc         eigen.rc \n     0.028058711      0.023546832      0.287358773      0.003637773 \n              dc         eigen.dc more_win_or_loss \n     0.712641227      0.024420939      0.481818182 \n\n\n\napply(total_1000s_for_PCA[-1], 2, var)\n\n\n        Catholic            Islam         Orthodox         Buddhist \n    0.2502085071     0.1501251043     0.1318598832     0.0601334445 \n           Pagan         Tengrism           Shinto            Hindu \n    0.0353628023     0.0180150125     0.0520433695     0.0437864887 \n       Shamanism        totdegree         indegree        outdegree \n    0.0090909091     8.9208507089     2.6656380317     6.3189324437 \n           eigen            close               rc         eigen.rc \n    0.0076304265     0.0019575460     0.1260782284     0.0004728954 \n              dc         eigen.dc more_win_or_loss \n    0.1260782284     0.0056490031     0.2519599666 \n\n\n\npr.out <- prcomp(total_1000s_for_PCA[-1], scale = TRUE)\n\n\n\n\n\nnames(pr.out)\n\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\n\npr.out$center\n\n\n        Catholic            Islam         Orthodox         Buddhist \n     0.454545455      0.181818182      0.154545455      0.063636364 \n           Pagan         Tengrism           Shinto            Hindu \n     0.036363636      0.018181818      0.054545455      0.045454545 \n       Shamanism        totdegree         indegree        outdegree \n     0.009090909      2.754545455      1.336363636      1.418181818 \n           eigen            close               rc         eigen.rc \n     0.028058711      0.023546832      0.287358773      0.003637773 \n              dc         eigen.dc more_win_or_loss \n     0.712641227      0.024420939      0.481818182 \n\n\n\npr.out$scale\n\n\n        Catholic            Islam         Orthodox         Buddhist \n      0.50020846       0.38745981       0.36312516       0.24522122 \n           Pagan         Tengrism           Shinto            Hindu \n      0.18805000       0.13422002       0.22813016       0.20925221 \n       Shamanism        totdegree         indegree        outdegree \n      0.09534626       2.98677932       1.63267818       2.51374868 \n           eigen            close               rc         eigen.rc \n      0.08735231       0.04424416       0.35507496       0.02174616 \n              dc         eigen.dc more_win_or_loss \n      0.35507496       0.07515985       0.50195614 \n\n\n\n\n\n\nbiplot(pr.out, scale = 0)\n\n\n\n\n\n\npr.out$rotation = -pr.out$rotation \n\npr.out$x = -pr.out$x\n\nbiplot(pr.out, scale = 0)\n\n\n\n\n\n\npr.out$sdev\n\n\n [1] 2.217501e+00 1.681548e+00 1.239242e+00 1.211199e+00 1.065982e+00\n [6] 1.037692e+00 1.029507e+00 1.011117e+00 1.005425e+00 9.514802e-01\n[11] 8.848499e-01 7.782431e-01 6.162540e-01 4.426224e-01 2.541422e-01\n[16] 1.091189e-01 7.597269e-16 6.258811e-16 2.174635e-16\n\n\n\npr.var <- pr.out$sdev^2\n\npr.var\n\n\n [1] 4.917311e+00 2.827605e+00 1.535720e+00 1.467004e+00 1.136318e+00\n [6] 1.076804e+00 1.059884e+00 1.022359e+00 1.010879e+00 9.053146e-01\n[11] 7.829594e-01 6.056623e-01 3.797690e-01 1.959146e-01 6.458828e-02\n[16] 1.190694e-02 5.771849e-31 3.917271e-31 4.729037e-32\n\n\n\npve <- pr.var / sum(pr.var)\n\npve\n\n\n [1] 2.588059e-01 1.488213e-01 8.082739e-02 7.721075e-02 5.980622e-02\n [6] 5.667390e-02 5.578337e-02 5.380835e-02 5.320417e-02 4.764814e-02\n[11] 4.120839e-02 3.187696e-02 1.998784e-02 1.031129e-02 3.399383e-03\n[16] 6.266808e-04 3.037815e-32 2.061722e-32 2.488967e-33\n\n\n\npar(mfrow = c(1, 2))\nplot(pve, xlab = \"Principal Component\",\nylab = \"Proportion of Variance Explained\", ylim = c(0, 1),\ntype = \"b\")\n\nplot(cumsum(pve), xlab = \"Principal Component\",\nylab = \"Cumulative Proportion of Variance Explained\", ylim = c(0, 1), type = \"b\")\n\n\n\n\n\n\nnames(total_1200s)\n\n\n [1] \"name\"         \"Catholic\"     \"Islam\"        \"Orthodox\"    \n [5] \"Buddhist\"     \"Pagan\"        \"Tengrism\"     \"Shinto\"      \n [9] \"Hindu\"        \"Shamanism\"    \"totdegree\"    \"indegree\"    \n[13] \"outdegree\"    \"eigen\"        \"rc\"           \"eigen.rc\"    \n[17] \"dc\"           \"eigen.dc\"     \"broker.tot\"   \"broker.coord\"\n[21] \"broker.itin\"  \"broker.rep\"   \"broker.gate\"  \"broker.lia\"  \n\n\n\ntotal_1200s_brokerag_reg<-total_1200s\n\n\n\n\n\ntotal_1200s_brokerag_reg$win_rate <- (total_1200s_brokerag_reg$outdegree/total_1200s_brokerag_reg$totdegree)\n\n\n\n\n\ntotal_1200s_brokerag_reg$loss_rate <- (total_1200s_brokerag_reg$indegree/total_1200s_brokerag_reg$totdegree)\n\n\n\n\n\ntotal_1200s_brokerag_reg_binom <- total_1200s_brokerag_reg %>% mutate(more_win_or_loss = case_when(\n  win_rate < 0.5 ~ 0,\n    win_rate >= 0.5 ~ 1))\n\n\n\n\n\ntotal_1200s_for_PCA <- total_1200s_brokerag_reg_binom[-c(20:27)]\n\n\napply(total_1200s_for_PCA[-1], 2, mean)\n\n\n   Catholic       Islam    Orthodox    Buddhist       Pagan \n0.712500000 0.068750000 0.087500000 0.087500000 0.012500000 \n   Tengrism      Shinto       Hindu   Shamanism   totdegree \n0.025000000 0.000000000 0.006250000 0.000000000 3.918750000 \n   indegree   outdegree       eigen          rc    eigen.rc \n1.962500000 1.956250000 0.025567955 0.158754617 0.002192746 \n         dc    eigen.dc  broker.tot \n0.841245383 0.023375209 0.341581810 \n\n\n\napply(total_1200s_for_PCA[-1], 2, var)\n\n\n    Catholic        Islam     Orthodox     Buddhist        Pagan \n2.061321e-01 6.442610e-02 8.034591e-02 8.034591e-02 1.242138e-02 \n    Tengrism       Shinto        Hindu    Shamanism    totdegree \n2.452830e-02 0.000000e+00 6.250000e-03 0.000000e+00 2.666631e+01 \n    indegree    outdegree        eigen           rc     eigen.rc \n6.237579e+00 1.595405e+01 5.631476e-03 7.141295e-02 7.316162e-05 \n          dc     eigen.dc   broker.tot \n7.141295e-02 4.574350e-03 3.001236e+01 \n\n\n\n# I cannot scale variables with \n\ntotal_1200s_for_PCA<-total_1200s_for_PCA[-c(8,10)]\n\n\n\n\n\npr.out_2 <- prcomp(total_1200s_for_PCA[-1], scale = TRUE)\n\n\n\n\n\nnames(pr.out_2)\n\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n\n\npr.out_2$center\n\n\n   Catholic       Islam    Orthodox    Buddhist       Pagan \n0.712500000 0.068750000 0.087500000 0.087500000 0.012500000 \n   Tengrism       Hindu   totdegree    indegree   outdegree \n0.025000000 0.006250000 3.918750000 1.962500000 1.956250000 \n      eigen          rc    eigen.rc          dc    eigen.dc \n0.025567955 0.158754617 0.002192746 0.841245383 0.023375209 \n broker.tot \n0.341581810 \n\n\n\npr.out_2$scale\n\n\n   Catholic       Islam    Orthodox    Buddhist       Pagan \n0.454017704 0.253822971 0.283453545 0.283453545 0.111451261 \n   Tengrism       Hindu   totdegree    indegree   outdegree \n0.156615139 0.079056942 5.163943541 2.497514488 3.994251963 \n      eigen          rc    eigen.rc          dc    eigen.dc \n0.075043164 0.267232010 0.008553457 0.267232010 0.067633938 \n broker.tot \n5.478353760 \n\n\n\n\n\n\nbiplot(pr.out_2, scale = 0)\n\n\n\n\n\n\npr.out_2$rotation = -pr.out_2$rotation \n\npr.out_2$x = -pr.out_2$x\n\nbiplot(pr.out_2, scale = 0)\n\n\n\n\n\n\npr.out$sdev\n\n\n [1] 2.217501e+00 1.681548e+00 1.239242e+00 1.211199e+00 1.065982e+00\n [6] 1.037692e+00 1.029507e+00 1.011117e+00 1.005425e+00 9.514802e-01\n[11] 8.848499e-01 7.782431e-01 6.162540e-01 4.426224e-01 2.541422e-01\n[16] 1.091189e-01 7.597269e-16 6.258811e-16 2.174635e-16\n\n\n\npr.var_2 <- pr.out_2$sdev^2\n\npr.var_2\n\n\n [1] 4.903737e+00 2.344663e+00 1.670548e+00 1.250176e+00 1.132904e+00\n [6] 1.097802e+00 1.011326e+00 9.460639e-01 8.661454e-01 5.139677e-01\n[11] 1.659928e-01 9.667541e-02 2.916516e-30 4.832251e-31 2.292490e-31\n[16] 1.889562e-32\n\n\n\npve_2 <- pr.var_2 / sum(pr.var_2)\n\npve_2\n\n\n [1] 3.064835e-01 1.465414e-01 1.044092e-01 7.813602e-02 7.080651e-02\n [6] 6.861260e-02 6.320785e-02 5.912899e-02 5.413409e-02 3.212298e-02\n[11] 1.037455e-02 6.042213e-03 1.822822e-31 3.020157e-32 1.432806e-32\n[16] 1.180977e-33\n\n\n\npar(mfrow = c(1, 2))\nplot(pve_2, xlab = \"Principal Component\",\nylab = \"Proportion of Variance Explained\", ylim = c(0, 1),\ntype = \"b\")\n\nplot(cumsum(pve_2), xlab = \"Principal Component\",\nylab = \"Cumulative Proportion of Variance Explained\", ylim = c(0, 1), type = \"b\")\n\n\n\n\n(information regarding the meaning of each type of brokerage was\nacquired from https://edis.ifas.ufl.edu/publication/WC197)\n\n\n\n",
    "preview": "posts/2022-03-29-blog-post-7-networks/blog-post-7-networks_files/figure-html5/unnamed-chunk-18-1.png",
    "last_modified": "2022-03-29T12:51:28-04:00",
    "input_file": "blog-post-7-networks.knit.md"
  },
  {
    "path": "posts/2022-03-29-tad-post-4/",
    "title": "TAD Post 4",
    "description": "In last weeks post I combined a the material for blog posts 2, 3, and began on the 4th post. As a result this will be a continuation of dictionary validation methods.",
    "author": [
      {
        "name": "Nora Jones",
        "url": {}
      }
    ],
    "date": "2022-03-29",
    "categories": [],
    "contents": "\n\n\n\nBlog Post 3 and 4:\nThird Blog Post and\nContinuation\nExplanation: This post covers material from week 5\npre-processing, week 6 representing text, and week 7 dictionary methods.\nIn this case I have slightly modified the data I am working with, this\nweek I am using 1000 new posts for a dictionary analysis rather than the\n“top” posts to see if there is a tangible difference in content.\nInital Loading and\nProcessing\nExplanation: My code below illustrates how I\ninitially got my information from reddit by scraping. In this case I\nused RedditExtractoR. The author of this packages describes it as a\nminimalist r wrapper it scrapes a limited number of posts from reddit.\nThe api on reddit itself only allows 60 requests per minute. In this\ncase I chose posts that were “new” as of march 26 2022 at 11:17 P.M.\nThis has resulted in 980 reddit posts, this subreddit is described as\n“Firearms and related articles” and much of the subreddit is\ndescriptions of, reviews, and highlights of firearms owned by users.\n\n\n# New_guns_urls <- find_thread_urls(subreddit=\"guns\", sort_by=\"new\")\n\n\nloadRData <- function(fileName){\n#loads an RData file, and returns it\n    load(fileName)\n    get(ls()[ls() != \"fileName\"])\n}\nNew_guns_urls_df <- loadRData(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/New_reddit_posts_3_26.RData\")\n\n\n\n\n\nstr(New_guns_urls_df)\n\n\n'data.frame':   980 obs. of  7 variables:\n $ date_utc : chr  \"2022-03-15\" \"2022-03-15\" \"2022-03-15\" \"2022-03-15\" ...\n $ timestamp: num  1.65e+09 1.65e+09 1.65e+09 1.65e+09 1.65e+09 ...\n $ title    : chr  \"With tiny ar15s and mcx/similar, where does a pcc fit in the arsenal now?\" \"I still don\\031t feel like this is good enough. Ha. Anyone else this anal about this stuff?\" \"Best Home Defense Round in 5.56?\" \"5\\0365*5/555&53 5\\r50535.5\\\"5-\" ...\n $ text     : chr  \"With the advent of small ar15s and similar foldy boys like the mcx, where does the pcc fit in now?\\n\\nSeems tha\"| __truncated__ \"\" \"I'm looking for some defensive rounds for my AR-15 that are reliable. Lately I've been shooting 55 Grain XTac g\"| __truncated__ \"\" ...\n $ subreddit: chr  \"guns\" \"guns\" \"guns\" \"guns\" ...\n $ comments : num  32 23 73 12 51 29 25 13 32 7 ...\n $ url      : chr  \"https://www.reddit.com/r/guns/comments/teruk1/with_tiny_ar15s_and_mcxsimilar_where_does_a_pcc/\" \"https://www.reddit.com/r/guns/comments/ter4zi/i_still_dont_feel_like_this_is_good_enough_ha/\" \"https://www.reddit.com/r/guns/comments/ter48v/best_home_defense_round_in_556/\" \"https://www.reddit.com/r/guns/comments/teqkl1/5\\0365*5/555&53_5\\r50535.5\\\"5-/\" ...\n\nConversion From Data Frame\nto Corpus\nExplanation: Below I have processed my initial\ndataframe from Reddit into a corpus and saved a summary of the resulting\ndata.\n\n\nnew_guns_urls_df<-New_guns_urls_df[,c(\"title\", \"date_utc\", \"comments\")]\n\nnew_guns_corpus<-corpus(new_guns_urls_df$title)\n\nnew_guns_documents<-new_guns_corpus[1:10,]\n\nnew_guns_corpus_summary <- summary(new_guns_corpus)\n\n\n\nBroad Characteristics\nExplanation: In order to clean the documents for\npre-processing and analysis I have removed punctuation, converted to\nlowercase and removed stopwords. Though the language of firearms is\noften associated with punctuation, such as 5.56, 3.57, and a variety of\nother calibers, which represent the diameter of the barrel required to\nfire each ammunition. However, losing the punctuation in caliber and\nfirearm titles would not reduce their comprehensibility in analysis if\nthey retain their form as 5.56 and 556 can be considered equally while\nreducing the complexity of tokens and potentially even sentences.\nConverting the documents to lowercase can also simplifies the data.\nHowever, for my inital analysis I will just being making a lowercase\ndocument feature matrix and a more edited one.\n\n\nnew_guns_corpus_dfm_tl<-tokens(new_guns_corpus) %>%  dfm(tolower=TRUE) \n\nnew_guns_corpus_dfm_punct_tl_sw <- tokens(new_guns_corpus,\n                                    remove_punct = TRUE,) %>%\n                           dfm(tolower=TRUE) %>%\n                           dfm_remove(stopwords('english'))\n\n\n\nTop features\nExplanation: Examining the top 20 features below we\nsee a fairly predictable set of response, as may be expected from the\ngun subreddit, the most used word is gun. Rifle and pistol are also in\nthe top 20. Individual letters such as “ar”, “s”, and “m” appear\nfrequently as they are commonly used designations for types of firearms\nor model names, ar-12, ar-15, m-4, m-16, m1911, 5-mm, and s559, s-12 and\nother designations. This indicates that these numbers are valuable, if\ndifficult to comprehend on their own. With no punctuation removed the\nfirst 20 features are not informative.\n\n\ntopfeatures(new_guns_corpus_dfm_tl, 20)\n\n\n    .     ?     a   the    my     ,     i    to   for   and    is \n  425   260   259   211   183   175   175   158   112   108    99 \n   in    it     /    of  with   gun  this    on first \n   93    89    84    81    80    78    76    76    60 \n\n\n\ntopfeatures(new_guns_corpus_dfm_punct_tl_sw, 20)\n\n\n     gun    first      new       22      can        s question \n      78       60       57       43       41       35       31 \n   rifle     just      amp     help   anyone       ar   pistol \n      29       29       28       27       26       26       26 \n    time     guns     know     good      got        m \n      25       24       23       22       22       22 \n\nWorld Cloud\nExplanation: Though not necessarily statistically\ninformative, the wordcloud below can give some sense of comparative\nfrequency using the limit of minimum count being 6. Reading through\nthese can give a sense of both the communal nature of the forum in\nasking for recommendations, but also the importance of the word purchase\nand other words associated with working with, and buying firearms. In\nthe case of the only lowercased data we can gather much less\ninformation.\n\n\nset.seed(123456)\n\ntextplot_wordcloud(new_guns_corpus_dfm_tl, min_count = 12, random_order = T, rotation = 0)\n\n\n\n\n\n\nset.seed(123456)\n\ntextplot_wordcloud(new_guns_corpus_dfm_punct_tl_sw, min_count = 6, random_order = T, rotation = 0)\n\n\n\n\nTypes, Tokens, and Sentances\nTypes\nExplanation: The gun corpus summary gives 3 counting\ncategories that we can interpret in order to get a sense of the\ncomplexity of the documents that we are using. Looking at the number of\ntypes on average we see a mean of 9.18 and qunantiles that indicate a\nrange of 2-45 with 50% being between 4 and 13 types.\n\n\nmean(new_guns_corpus_summary$Types)\n\n\n[1] 9.18\n\nquantile(new_guns_corpus_summary$Types)\n\n\n  0%  25%  50%  75% 100% \n 2.0  4.0  7.5 13.0 45.0 \n\nTokens\nExplanation: Tokens are relatively similar to types\nin this case. Here there is a mean of 9.72 but a range of 2-55 with the\nmiddle 50% ranging from 4-13 tokens, as was the case for types.\n\n\nmean(new_guns_corpus_summary$Tokens)\n\n\n[1] 9.72\n\nquantile(new_guns_corpus_summary$Tokens)\n\n\n  0%  25%  50%  75% 100% \n   2    4    8   13   55 \n\nSentances\nExplanation: As is indicated below, it appears that\nthe number of sentences in each post is generally one. Arroding to the\nqunatile statistics the most sentences in any post\n\n\nmean(new_guns_corpus_summary$Sentences)\n\n\n[1] 1.18\n\nquantile(new_guns_corpus_summary$Sentences)\n\n\n  0%  25%  50%  75% 100% \n   1    1    1    1    3 \n\nWord counts\nExplanation: Looking at word counts we see a similar\ntrend reflected where including stopwords and punctuation decreases the\nquality of data as little information but punctuation and stopwords are\nincluded.\n\n\nword_counts_new_1 <- as.data.frame(sort(colSums(new_guns_corpus_dfm_tl),dec=T))\n\ncolnames(word_counts_new_1) <- c(\"Frequency\")\n\nword_counts_new_1$Rank <- c(1:ncol(new_guns_corpus_dfm_tl))\n\nhead(word_counts_new_1)\n\n\n    Frequency Rank\n.         425    1\n?         260    2\na         259    3\nthe       211    4\nmy        183    5\n,         175    6\n\n\n\nword_counts_new <- as.data.frame(sort(colSums(new_guns_corpus_dfm_punct_tl_sw),dec=T))\n\ncolnames(word_counts_new) <- c(\"Frequency\")\n\nword_counts_new$Rank <- c(1:ncol(new_guns_corpus_dfm_punct_tl_sw))\n\nhead(word_counts_new)\n\n\n      Frequency Rank\ngun          78    1\nfirst        60    2\nnew          57    3\n22           43    4\ncan          41    5\ns            35    6\n\nZipf’s Law\nExplanation: As can be seen from the frequency\ngraphs below, Ziph’s Law of inverse proportion. In this case a words\nrank in freqency is inversely prorportional to the number of times it is\nobserved. Though the uncleaned dataset has far more frequency for its\nmost common words (much of which is punctuation) it appears to follow\nthe law.\n\n\nggplot(word_counts_new, mapping = aes(x = Rank, y = Frequency)) + \n  geom_point() +\n  labs(title = \"Zipf's Law\", x = \"Rank\", y = \"Frequency\") + \n  theme_bw()\n\n\n\n\n\n\nggplot(word_counts_new_1, mapping = aes(x = Rank, y = Frequency)) + \n  geom_point() +\n  labs(title = \"Zipf's Law\", x = \"Rank\", y = \"Frequency\") + \n  theme_bw()\n\n\n\n\nData Trimming\nExplanation: Much of what I do here will be\nexplained in the code and results. Many words appear with a minimum\nfrequency of 4, though non are included in 10% and only 3 words are\nincluded in 5%. At a level of 2.5% we get 4 words.\n\n\n# First I trim the data to only include words that appear at least 4 times\n\nsmaller_dfm_4_freq <- dfm_trim(new_guns_corpus_dfm_punct_tl_sw, min_termfreq = 4)\n\n# Next I will look at proportions are see if there are words that are seen in\n# More than 10% and 5% of documents\nsmaller_dfm_10_p <- dfm_trim(smaller_dfm_4_freq, min_docfreq = 0.1, docfreq_type = \"prop\")\n\nsmaller_dfm_5_p <- dfm_trim(smaller_dfm_4_freq, min_docfreq = 0.05, docfreq_type = \"prop\")\n\nsmaller_dfm_2.5_p <- dfm_trim(smaller_dfm_4_freq, min_docfreq = 0.025,  docfreq_type = \"prop\")\n\n\n\nData Readability\nExplanation: Before making general modifications to\nthe data, it is valuable to also get a sense of readability, as in week\n5. In this case we will calculate readability scores based on 3\ndifferent measures, FOG, Coleman Liau, and Flesch Kincaid. Though this\nstep will not indicate what sort of pre-processing is best, or how the\ndata should be reduced, it does give us some insight into the complexity\nof the language in our data. In this case we just observe the\nreadability based on the post number.\n\n\nreadability_new_guns <- textstat_readability(new_guns_corpus, \n                                    measure = c(\"Flesch.Kincaid\", \"FOG\", \"Coleman.Liau.grade\")) \n\n# add in a chapter number\n\nreadability_new_guns$reddit_post <- c(1:nrow(readability_new_guns))\n\n# plot results\nggplot(readability_new_guns, aes(x = reddit_post)) +\n  geom_line(aes(y = Flesch.Kincaid), color = \"black\",  alpha=0.3) + \n  geom_line(aes(y = FOG), color = \"red\", alpha=0.3) + \n  geom_line(aes(y = Coleman.Liau.grade), color = \"blue\", alpha=0.3) + \n  theme_bw()\n\n\n\n\nExplanation: In this part we will add dates to our\ndata to see how the complexity changes over time or if it was relatively\nconstant. As can be seen below, the amount of complexity in the data\nvaries more smoothly when the data are sorted by data and not\narbitrarily by their post number, in this case all 3 complexity method\nexhibit similar trends.\n\n\nreadability_new_guns$added_dates <- as.Date(New_guns_urls_df$date_utc)\n\nggplot(readability_new_guns, aes(x = added_dates)) +\n  geom_smooth(aes(y = Flesch.Kincaid), color = \"black\") + \n  geom_smooth(aes(y = FOG), color = \"red\") + \n  geom_smooth(aes(y = Coleman.Liau.grade), color = \"blue\") + \n  theme_minimal()\n\n\n\n\nExplanation: Looking at the readability of the we\nsee that all correlations between the methods of complexity measurement\nare similar except of FOG and Coleman Liau, however the graphs above do\nindicate some similarity in trend between them, though not direct\ncorrelation in their estimates potentially.\n\n\ncor(readability_new_guns$Flesch.Kincaid, readability_new_guns$FOG, use = \"complete.obs\")\n\n\n[1] 0.8321041\n\n\n\ncor(readability_new_guns$Flesch.Kincaid, readability_new_guns$Coleman.Liau.grade, use = \"complete.obs\")\n\n\n[1] 0.751674\n\n\n\ncor(readability_new_guns$FOG, readability_new_guns$Coleman.Liau.grade, use = \"complete.obs\")\n\n\n[1] 0.5859859\n\nPre-processing\nBefore Reduction and Co-Occurance\nExplanation: Next I used the\nfactorial_preprocessing() command to both use n-grams processing and use\nan infrequent term threshold. This is in order to see what techniques,\nsuch as removing punctuation, stopwords, etc lead to a pre-text score\ndevised by Denny and Spirling. This pre-text score indicatess how many\nk-pairs of terms change the most when the pre-processing strategy is\nchanged. Lower scores indicate more usual results while higher scores\nindicate more unusual results and they are between 0 and 1. Here we have\nused n-grams and set an infreqent term threshold. Because of the nature\nof our data I will use 30% of documents as\n\n\n?factorial_preprocessing\npreprocessed_documents <- factorial_preprocessing(\n    new_guns_corpus,\n    use_ngrams = TRUE,\n    infrequent_term_threshold = 0.3,\n    verbose = FALSE)\n\n\nPreprocessing 980 documents 128 different ways...\n\n\n\nnames(preprocessed_documents)\n\n\n[1] \"choices\"  \"dfm_list\" \"labels\"  \n\nExplanation: As can be seen below the possible\nchoices are coded on the first column with each subsequent column\nindicating whether or not each choice includes each of the specified\nchoices in its assessment.\n\n\nhead(preprocessed_documents$choices)\n\n\n              removePunctuation removeNumbers lowercase stem\nP-N-L-S-W-I-3              TRUE          TRUE      TRUE TRUE\nN-L-S-W-I-3               FALSE          TRUE      TRUE TRUE\nP-L-S-W-I-3                TRUE         FALSE      TRUE TRUE\nL-S-W-I-3                 FALSE         FALSE      TRUE TRUE\nP-N-S-W-I-3                TRUE          TRUE     FALSE TRUE\nN-S-W-I-3                 FALSE          TRUE     FALSE TRUE\n              removeStopwords infrequent_terms use_ngrams\nP-N-L-S-W-I-3            TRUE             TRUE       TRUE\nN-L-S-W-I-3              TRUE             TRUE       TRUE\nP-L-S-W-I-3              TRUE             TRUE       TRUE\nL-S-W-I-3                TRUE             TRUE       TRUE\nP-N-S-W-I-3              TRUE             TRUE       TRUE\nN-S-W-I-3                TRUE             TRUE       TRUE\n\nExplanation: Next preText is calculated using 50\ncomparisons and a cosine distance calculation.\n\n\n#set.seed(12366)\n#preText_results <- preText(\n#    preprocessed_documents,\n#   dataset_name = \"Gun Pretext Results\",\n#   distance_method = \"cosine\",\n#   num_comparisons = 50,\n#  verbose = TRUE)\n\n\n\n\n\n#save(preText_results, file=\"preText_results_3_27_gun_50_comp.RData\")\n\n\n\n\n\npreText_results <- loadRData(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/preText_results_3_27_gun_50_comp.RData\")\n\npreText_results\n\n\n$preText_scores\n    preText_score preprocessing_steps\n1      0.04157825       P-N-L-S-W-I-3\n2      0.04157825         N-L-S-W-I-3\n3      0.04157825         P-L-S-W-I-3\n4      0.04157825           L-S-W-I-3\n5      0.04157825         P-N-S-W-I-3\n6      0.04157825           N-S-W-I-3\n7      0.04157825           P-S-W-I-3\n8      0.04157825             S-W-I-3\n9      0.04157825         P-N-L-W-I-3\n10     0.04157825           N-L-W-I-3\n11     0.04157825           P-L-W-I-3\n12     0.04157825             L-W-I-3\n13     0.04157825           P-N-W-I-3\n14     0.04157825             N-W-I-3\n15     0.04157825             P-W-I-3\n16     0.04157825               W-I-3\n17     0.04157825         P-N-L-S-I-3\n18     0.04157825           N-L-S-I-3\n19     0.04157825           P-L-S-I-3\n20     0.04157825             L-S-I-3\n21     0.04157825           P-N-S-I-3\n22     0.04157825             N-S-I-3\n23     0.04157825             P-S-I-3\n24     0.04157825               S-I-3\n25     0.04157825           P-N-L-I-3\n26     0.04157825             N-L-I-3\n27     0.04157825             P-L-I-3\n28     0.04157825               L-I-3\n29     0.04157825             P-N-I-3\n30     0.04157825               N-I-3\n31     0.04157825               P-I-3\n32     0.04157825                 I-3\n33     0.04815400         P-N-L-S-W-3\n34     0.08058534           N-L-S-W-3\n35     0.04403401           P-L-S-W-3\n36     0.08427290             L-S-W-3\n37     0.04864279           P-N-S-W-3\n38     0.07599185             N-S-W-3\n39     0.04470926             P-S-W-3\n40     0.08186325               S-W-3\n41     0.02723578           P-N-L-W-3\n42     0.02629912             N-L-W-3\n43     0.02280946             P-L-W-3\n44     0.02777490               L-W-3\n45     0.02723578             P-N-W-3\n46     0.02629912               N-W-3\n47     0.02280946               P-W-3\n48     0.02777490                 W-3\n49     0.02647384           P-N-L-S-3\n50     0.08304385             N-L-S-3\n51     0.02334213             P-L-S-3\n52     0.07225887               L-S-3\n53     0.02706859             P-N-S-3\n54     0.08697989               N-S-3\n55     0.02384351               P-S-3\n56     0.07767802                 S-3\n57     0.02647384             P-N-L-3\n58     0.02777727               N-L-3\n59     0.02334213               P-L-3\n60     0.01888648                 L-3\n61     0.02647384               P-N-3\n62     0.02777727                 N-3\n63     0.02334213                 P-3\n64     0.01888648                   3\n65     0.04157825         P-N-L-S-W-I\n66     0.04157825           N-L-S-W-I\n67     0.04157825           P-L-S-W-I\n68     0.04157825             L-S-W-I\n69     0.04157825           P-N-S-W-I\n70     0.04157825             N-S-W-I\n71     0.04157825             P-S-W-I\n72     0.04157825               S-W-I\n73     0.04157825           P-N-L-W-I\n74     0.04157825             N-L-W-I\n75     0.04157825             P-L-W-I\n76     0.04157825               L-W-I\n77     0.04157825             P-N-W-I\n78     0.04157825               N-W-I\n79     0.04157825               P-W-I\n80     0.04157825                 W-I\n81     0.04157825           P-N-L-S-I\n82     0.04157825             N-L-S-I\n83     0.04157825             P-L-S-I\n84     0.04157825               L-S-I\n85     0.04157825             P-N-S-I\n86     0.04157825               N-S-I\n87     0.04157825               P-S-I\n88     0.04157825                 S-I\n89     0.04157825             P-N-L-I\n90     0.04157825               N-L-I\n91     0.04157825               P-L-I\n92     0.04157825                 L-I\n93     0.04157825               P-N-I\n94     0.04157825                 N-I\n95     0.04157825                 P-I\n96     0.04157825                   I\n97     0.07109905           P-N-L-S-W\n98     0.16021925             N-L-S-W\n99     0.06767075             P-L-S-W\n100    0.17095529               L-S-W\n101    0.07098343             P-N-S-W\n102    0.15776534               N-S-W\n103    0.06802275               P-S-W\n104    0.16850136                 S-W\n105    0.02723578             P-N-L-W\n106    0.02344916               N-L-W\n107    0.02280946               P-L-W\n108    0.02739263                 L-W\n109    0.02723578               P-N-W\n110    0.02344916                 N-W\n111    0.02280946                 P-W\n112    0.02739263                   W\n113    0.05006683             P-N-L-S\n114    0.22396649               N-L-S\n115    0.04704759               P-L-S\n116    0.23717659                 L-S\n117    0.05064631               P-N-S\n118    0.21022978                 N-S\n119    0.04752479                 P-S\n120    0.23146371                   S\n121    0.02635536               P-N-L\n122    0.03527452                 N-L\n123    0.02342946                 P-L\n124    0.10226138                   L\n125    0.02635536                 P-N\n126    0.03527452                   N\n127    0.02342946                   P\n\n$ranked_preText_scores\n    preText_score preprocessing_steps\n1      0.23717659                 L-S\n2      0.23146371                   S\n3      0.22396649               N-L-S\n4      0.21022978                 N-S\n5      0.17095529               L-S-W\n6      0.16850136                 S-W\n7      0.16021925             N-L-S-W\n8      0.15776534               N-S-W\n9      0.10226138                   L\n10     0.08697989               N-S-3\n11     0.08427290             L-S-W-3\n12     0.08304385             N-L-S-3\n13     0.08186325               S-W-3\n14     0.08058534           N-L-S-W-3\n15     0.07767802                 S-3\n16     0.07599185             N-S-W-3\n17     0.07225887               L-S-3\n18     0.07109905           P-N-L-S-W\n19     0.07098343             P-N-S-W\n20     0.06802275               P-S-W\n21     0.06767075             P-L-S-W\n22     0.05064631               P-N-S\n23     0.05006683             P-N-L-S\n24     0.04864279           P-N-S-W-3\n25     0.04815400         P-N-L-S-W-3\n26     0.04752479                 P-S\n27     0.04704759               P-L-S\n28     0.04470926             P-S-W-3\n29     0.04403401           P-L-S-W-3\n30     0.04157825       P-N-L-S-W-I-3\n31     0.04157825         N-L-S-W-I-3\n32     0.04157825         P-L-S-W-I-3\n33     0.04157825           L-S-W-I-3\n34     0.04157825         P-N-S-W-I-3\n35     0.04157825           N-S-W-I-3\n36     0.04157825           P-S-W-I-3\n37     0.04157825             S-W-I-3\n38     0.04157825         P-N-L-W-I-3\n39     0.04157825           N-L-W-I-3\n40     0.04157825           P-L-W-I-3\n41     0.04157825             L-W-I-3\n42     0.04157825           P-N-W-I-3\n43     0.04157825             N-W-I-3\n44     0.04157825             P-W-I-3\n45     0.04157825               W-I-3\n46     0.04157825         P-N-L-S-I-3\n47     0.04157825           N-L-S-I-3\n48     0.04157825           P-L-S-I-3\n49     0.04157825             L-S-I-3\n50     0.04157825           P-N-S-I-3\n51     0.04157825             N-S-I-3\n52     0.04157825             P-S-I-3\n53     0.04157825               S-I-3\n54     0.04157825           P-N-L-I-3\n55     0.04157825             N-L-I-3\n56     0.04157825             P-L-I-3\n57     0.04157825               L-I-3\n58     0.04157825             P-N-I-3\n59     0.04157825               N-I-3\n60     0.04157825               P-I-3\n61     0.04157825                 I-3\n62     0.04157825         P-N-L-S-W-I\n63     0.04157825           N-L-S-W-I\n64     0.04157825           P-L-S-W-I\n65     0.04157825             L-S-W-I\n66     0.04157825           P-N-S-W-I\n67     0.04157825             N-S-W-I\n68     0.04157825             P-S-W-I\n69     0.04157825               S-W-I\n70     0.04157825           P-N-L-W-I\n71     0.04157825             N-L-W-I\n72     0.04157825             P-L-W-I\n73     0.04157825               L-W-I\n74     0.04157825             P-N-W-I\n75     0.04157825               N-W-I\n76     0.04157825               P-W-I\n77     0.04157825                 W-I\n78     0.04157825           P-N-L-S-I\n79     0.04157825             N-L-S-I\n80     0.04157825             P-L-S-I\n81     0.04157825               L-S-I\n82     0.04157825             P-N-S-I\n83     0.04157825               N-S-I\n84     0.04157825               P-S-I\n85     0.04157825                 S-I\n86     0.04157825             P-N-L-I\n87     0.04157825               N-L-I\n88     0.04157825               P-L-I\n89     0.04157825                 L-I\n90     0.04157825               P-N-I\n91     0.04157825                 N-I\n92     0.04157825                 P-I\n93     0.04157825                   I\n94     0.03527452                 N-L\n95     0.03527452                   N\n96     0.02777727               N-L-3\n97     0.02777727                 N-3\n98     0.02777490               L-W-3\n99     0.02777490                 W-3\n100    0.02739263                 L-W\n101    0.02739263                   W\n102    0.02723578           P-N-L-W-3\n103    0.02723578             P-N-W-3\n104    0.02723578             P-N-L-W\n105    0.02723578               P-N-W\n106    0.02706859             P-N-S-3\n107    0.02647384           P-N-L-S-3\n108    0.02647384             P-N-L-3\n109    0.02647384               P-N-3\n110    0.02635536               P-N-L\n111    0.02635536                 P-N\n112    0.02629912             N-L-W-3\n113    0.02629912               N-W-3\n114    0.02384351               P-S-3\n115    0.02344916               N-L-W\n116    0.02344916                 N-W\n117    0.02342946                 P-L\n118    0.02342946                   P\n119    0.02334213             P-L-S-3\n120    0.02334213               P-L-3\n121    0.02334213                 P-3\n122    0.02280946             P-L-W-3\n123    0.02280946               P-W-3\n124    0.02280946               P-L-W\n125    0.02280946                 P-W\n126    0.01888648                 L-3\n127    0.01888648                   3\n\n$choices\n              removePunctuation removeNumbers lowercase  stem\nP-N-L-S-W-I-3              TRUE          TRUE      TRUE  TRUE\nN-L-S-W-I-3               FALSE          TRUE      TRUE  TRUE\nP-L-S-W-I-3                TRUE         FALSE      TRUE  TRUE\nL-S-W-I-3                 FALSE         FALSE      TRUE  TRUE\nP-N-S-W-I-3                TRUE          TRUE     FALSE  TRUE\nN-S-W-I-3                 FALSE          TRUE     FALSE  TRUE\nP-S-W-I-3                  TRUE         FALSE     FALSE  TRUE\nS-W-I-3                   FALSE         FALSE     FALSE  TRUE\nP-N-L-W-I-3                TRUE          TRUE      TRUE FALSE\nN-L-W-I-3                 FALSE          TRUE      TRUE FALSE\nP-L-W-I-3                  TRUE         FALSE      TRUE FALSE\nL-W-I-3                   FALSE         FALSE      TRUE FALSE\nP-N-W-I-3                  TRUE          TRUE     FALSE FALSE\nN-W-I-3                   FALSE          TRUE     FALSE FALSE\nP-W-I-3                    TRUE         FALSE     FALSE FALSE\nW-I-3                     FALSE         FALSE     FALSE FALSE\nP-N-L-S-I-3                TRUE          TRUE      TRUE  TRUE\nN-L-S-I-3                 FALSE          TRUE      TRUE  TRUE\nP-L-S-I-3                  TRUE         FALSE      TRUE  TRUE\nL-S-I-3                   FALSE         FALSE      TRUE  TRUE\nP-N-S-I-3                  TRUE          TRUE     FALSE  TRUE\nN-S-I-3                   FALSE          TRUE     FALSE  TRUE\nP-S-I-3                    TRUE         FALSE     FALSE  TRUE\nS-I-3                     FALSE         FALSE     FALSE  TRUE\nP-N-L-I-3                  TRUE          TRUE      TRUE FALSE\nN-L-I-3                   FALSE          TRUE      TRUE FALSE\nP-L-I-3                    TRUE         FALSE      TRUE FALSE\nL-I-3                     FALSE         FALSE      TRUE FALSE\nP-N-I-3                    TRUE          TRUE     FALSE FALSE\nN-I-3                     FALSE          TRUE     FALSE FALSE\nP-I-3                      TRUE         FALSE     FALSE FALSE\nI-3                       FALSE         FALSE     FALSE FALSE\nP-N-L-S-W-3                TRUE          TRUE      TRUE  TRUE\nN-L-S-W-3                 FALSE          TRUE      TRUE  TRUE\nP-L-S-W-3                  TRUE         FALSE      TRUE  TRUE\nL-S-W-3                   FALSE         FALSE      TRUE  TRUE\nP-N-S-W-3                  TRUE          TRUE     FALSE  TRUE\nN-S-W-3                   FALSE          TRUE     FALSE  TRUE\nP-S-W-3                    TRUE         FALSE     FALSE  TRUE\nS-W-3                     FALSE         FALSE     FALSE  TRUE\nP-N-L-W-3                  TRUE          TRUE      TRUE FALSE\nN-L-W-3                   FALSE          TRUE      TRUE FALSE\nP-L-W-3                    TRUE         FALSE      TRUE FALSE\nL-W-3                     FALSE         FALSE      TRUE FALSE\nP-N-W-3                    TRUE          TRUE     FALSE FALSE\nN-W-3                     FALSE          TRUE     FALSE FALSE\nP-W-3                      TRUE         FALSE     FALSE FALSE\nW-3                       FALSE         FALSE     FALSE FALSE\nP-N-L-S-3                  TRUE          TRUE      TRUE  TRUE\nN-L-S-3                   FALSE          TRUE      TRUE  TRUE\nP-L-S-3                    TRUE         FALSE      TRUE  TRUE\nL-S-3                     FALSE         FALSE      TRUE  TRUE\nP-N-S-3                    TRUE          TRUE     FALSE  TRUE\nN-S-3                     FALSE          TRUE     FALSE  TRUE\nP-S-3                      TRUE         FALSE     FALSE  TRUE\nS-3                       FALSE         FALSE     FALSE  TRUE\nP-N-L-3                    TRUE          TRUE      TRUE FALSE\nN-L-3                     FALSE          TRUE      TRUE FALSE\nP-L-3                      TRUE         FALSE      TRUE FALSE\nL-3                       FALSE         FALSE      TRUE FALSE\nP-N-3                      TRUE          TRUE     FALSE FALSE\nN-3                       FALSE          TRUE     FALSE FALSE\nP-3                        TRUE         FALSE     FALSE FALSE\n3                         FALSE         FALSE     FALSE FALSE\nP-N-L-S-W-I                TRUE          TRUE      TRUE  TRUE\nN-L-S-W-I                 FALSE          TRUE      TRUE  TRUE\nP-L-S-W-I                  TRUE         FALSE      TRUE  TRUE\nL-S-W-I                   FALSE         FALSE      TRUE  TRUE\nP-N-S-W-I                  TRUE          TRUE     FALSE  TRUE\nN-S-W-I                   FALSE          TRUE     FALSE  TRUE\nP-S-W-I                    TRUE         FALSE     FALSE  TRUE\nS-W-I                     FALSE         FALSE     FALSE  TRUE\nP-N-L-W-I                  TRUE          TRUE      TRUE FALSE\nN-L-W-I                   FALSE          TRUE      TRUE FALSE\nP-L-W-I                    TRUE         FALSE      TRUE FALSE\nL-W-I                     FALSE         FALSE      TRUE FALSE\nP-N-W-I                    TRUE          TRUE     FALSE FALSE\nN-W-I                     FALSE          TRUE     FALSE FALSE\nP-W-I                      TRUE         FALSE     FALSE FALSE\nW-I                       FALSE         FALSE     FALSE FALSE\nP-N-L-S-I                  TRUE          TRUE      TRUE  TRUE\nN-L-S-I                   FALSE          TRUE      TRUE  TRUE\nP-L-S-I                    TRUE         FALSE      TRUE  TRUE\nL-S-I                     FALSE         FALSE      TRUE  TRUE\nP-N-S-I                    TRUE          TRUE     FALSE  TRUE\nN-S-I                     FALSE          TRUE     FALSE  TRUE\nP-S-I                      TRUE         FALSE     FALSE  TRUE\nS-I                       FALSE         FALSE     FALSE  TRUE\nP-N-L-I                    TRUE          TRUE      TRUE FALSE\nN-L-I                     FALSE          TRUE      TRUE FALSE\nP-L-I                      TRUE         FALSE      TRUE FALSE\nL-I                       FALSE         FALSE      TRUE FALSE\nP-N-I                      TRUE          TRUE     FALSE FALSE\nN-I                       FALSE          TRUE     FALSE FALSE\nP-I                        TRUE         FALSE     FALSE FALSE\nI                         FALSE         FALSE     FALSE FALSE\nP-N-L-S-W                  TRUE          TRUE      TRUE  TRUE\nN-L-S-W                   FALSE          TRUE      TRUE  TRUE\nP-L-S-W                    TRUE         FALSE      TRUE  TRUE\nL-S-W                     FALSE         FALSE      TRUE  TRUE\nP-N-S-W                    TRUE          TRUE     FALSE  TRUE\nN-S-W                     FALSE          TRUE     FALSE  TRUE\nP-S-W                      TRUE         FALSE     FALSE  TRUE\nS-W                       FALSE         FALSE     FALSE  TRUE\nP-N-L-W                    TRUE          TRUE      TRUE FALSE\nN-L-W                     FALSE          TRUE      TRUE FALSE\nP-L-W                      TRUE         FALSE      TRUE FALSE\nL-W                       FALSE         FALSE      TRUE FALSE\nP-N-W                      TRUE          TRUE     FALSE FALSE\nN-W                       FALSE          TRUE     FALSE FALSE\nP-W                        TRUE         FALSE     FALSE FALSE\nW                         FALSE         FALSE     FALSE FALSE\nP-N-L-S                    TRUE          TRUE      TRUE  TRUE\nN-L-S                     FALSE          TRUE      TRUE  TRUE\nP-L-S                      TRUE         FALSE      TRUE  TRUE\nL-S                       FALSE         FALSE      TRUE  TRUE\nP-N-S                      TRUE          TRUE     FALSE  TRUE\nN-S                       FALSE          TRUE     FALSE  TRUE\nP-S                        TRUE         FALSE     FALSE  TRUE\nS                         FALSE         FALSE     FALSE  TRUE\nP-N-L                      TRUE          TRUE      TRUE FALSE\nN-L                       FALSE          TRUE      TRUE FALSE\nP-L                        TRUE         FALSE      TRUE FALSE\nL                         FALSE         FALSE      TRUE FALSE\nP-N                        TRUE          TRUE     FALSE FALSE\nN                         FALSE          TRUE     FALSE FALSE\nP                          TRUE         FALSE     FALSE FALSE\n                          FALSE         FALSE     FALSE FALSE\n              removeStopwords infrequent_terms use_ngrams\nP-N-L-S-W-I-3            TRUE             TRUE       TRUE\nN-L-S-W-I-3              TRUE             TRUE       TRUE\nP-L-S-W-I-3              TRUE             TRUE       TRUE\nL-S-W-I-3                TRUE             TRUE       TRUE\nP-N-S-W-I-3              TRUE             TRUE       TRUE\nN-S-W-I-3                TRUE             TRUE       TRUE\nP-S-W-I-3                TRUE             TRUE       TRUE\nS-W-I-3                  TRUE             TRUE       TRUE\nP-N-L-W-I-3              TRUE             TRUE       TRUE\nN-L-W-I-3                TRUE             TRUE       TRUE\nP-L-W-I-3                TRUE             TRUE       TRUE\nL-W-I-3                  TRUE             TRUE       TRUE\nP-N-W-I-3                TRUE             TRUE       TRUE\nN-W-I-3                  TRUE             TRUE       TRUE\nP-W-I-3                  TRUE             TRUE       TRUE\nW-I-3                    TRUE             TRUE       TRUE\nP-N-L-S-I-3             FALSE             TRUE       TRUE\nN-L-S-I-3               FALSE             TRUE       TRUE\nP-L-S-I-3               FALSE             TRUE       TRUE\nL-S-I-3                 FALSE             TRUE       TRUE\nP-N-S-I-3               FALSE             TRUE       TRUE\nN-S-I-3                 FALSE             TRUE       TRUE\nP-S-I-3                 FALSE             TRUE       TRUE\nS-I-3                   FALSE             TRUE       TRUE\nP-N-L-I-3               FALSE             TRUE       TRUE\nN-L-I-3                 FALSE             TRUE       TRUE\nP-L-I-3                 FALSE             TRUE       TRUE\nL-I-3                   FALSE             TRUE       TRUE\nP-N-I-3                 FALSE             TRUE       TRUE\nN-I-3                   FALSE             TRUE       TRUE\nP-I-3                   FALSE             TRUE       TRUE\nI-3                     FALSE             TRUE       TRUE\nP-N-L-S-W-3              TRUE            FALSE       TRUE\nN-L-S-W-3                TRUE            FALSE       TRUE\nP-L-S-W-3                TRUE            FALSE       TRUE\nL-S-W-3                  TRUE            FALSE       TRUE\nP-N-S-W-3                TRUE            FALSE       TRUE\nN-S-W-3                  TRUE            FALSE       TRUE\nP-S-W-3                  TRUE            FALSE       TRUE\nS-W-3                    TRUE            FALSE       TRUE\nP-N-L-W-3                TRUE            FALSE       TRUE\nN-L-W-3                  TRUE            FALSE       TRUE\nP-L-W-3                  TRUE            FALSE       TRUE\nL-W-3                    TRUE            FALSE       TRUE\nP-N-W-3                  TRUE            FALSE       TRUE\nN-W-3                    TRUE            FALSE       TRUE\nP-W-3                    TRUE            FALSE       TRUE\nW-3                      TRUE            FALSE       TRUE\nP-N-L-S-3               FALSE            FALSE       TRUE\nN-L-S-3                 FALSE            FALSE       TRUE\nP-L-S-3                 FALSE            FALSE       TRUE\nL-S-3                   FALSE            FALSE       TRUE\nP-N-S-3                 FALSE            FALSE       TRUE\nN-S-3                   FALSE            FALSE       TRUE\nP-S-3                   FALSE            FALSE       TRUE\nS-3                     FALSE            FALSE       TRUE\nP-N-L-3                 FALSE            FALSE       TRUE\nN-L-3                   FALSE            FALSE       TRUE\nP-L-3                   FALSE            FALSE       TRUE\nL-3                     FALSE            FALSE       TRUE\nP-N-3                   FALSE            FALSE       TRUE\nN-3                     FALSE            FALSE       TRUE\nP-3                     FALSE            FALSE       TRUE\n3                       FALSE            FALSE       TRUE\nP-N-L-S-W-I              TRUE             TRUE      FALSE\nN-L-S-W-I                TRUE             TRUE      FALSE\nP-L-S-W-I                TRUE             TRUE      FALSE\nL-S-W-I                  TRUE             TRUE      FALSE\nP-N-S-W-I                TRUE             TRUE      FALSE\nN-S-W-I                  TRUE             TRUE      FALSE\nP-S-W-I                  TRUE             TRUE      FALSE\nS-W-I                    TRUE             TRUE      FALSE\nP-N-L-W-I                TRUE             TRUE      FALSE\nN-L-W-I                  TRUE             TRUE      FALSE\nP-L-W-I                  TRUE             TRUE      FALSE\nL-W-I                    TRUE             TRUE      FALSE\nP-N-W-I                  TRUE             TRUE      FALSE\nN-W-I                    TRUE             TRUE      FALSE\nP-W-I                    TRUE             TRUE      FALSE\nW-I                      TRUE             TRUE      FALSE\nP-N-L-S-I               FALSE             TRUE      FALSE\nN-L-S-I                 FALSE             TRUE      FALSE\nP-L-S-I                 FALSE             TRUE      FALSE\nL-S-I                   FALSE             TRUE      FALSE\nP-N-S-I                 FALSE             TRUE      FALSE\nN-S-I                   FALSE             TRUE      FALSE\nP-S-I                   FALSE             TRUE      FALSE\nS-I                     FALSE             TRUE      FALSE\nP-N-L-I                 FALSE             TRUE      FALSE\nN-L-I                   FALSE             TRUE      FALSE\nP-L-I                   FALSE             TRUE      FALSE\nL-I                     FALSE             TRUE      FALSE\nP-N-I                   FALSE             TRUE      FALSE\nN-I                     FALSE             TRUE      FALSE\nP-I                     FALSE             TRUE      FALSE\nI                       FALSE             TRUE      FALSE\nP-N-L-S-W                TRUE            FALSE      FALSE\nN-L-S-W                  TRUE            FALSE      FALSE\nP-L-S-W                  TRUE            FALSE      FALSE\nL-S-W                    TRUE            FALSE      FALSE\nP-N-S-W                  TRUE            FALSE      FALSE\nN-S-W                    TRUE            FALSE      FALSE\nP-S-W                    TRUE            FALSE      FALSE\nS-W                      TRUE            FALSE      FALSE\nP-N-L-W                  TRUE            FALSE      FALSE\nN-L-W                    TRUE            FALSE      FALSE\nP-L-W                    TRUE            FALSE      FALSE\nL-W                      TRUE            FALSE      FALSE\nP-N-W                    TRUE            FALSE      FALSE\nN-W                      TRUE            FALSE      FALSE\nP-W                      TRUE            FALSE      FALSE\nW                        TRUE            FALSE      FALSE\nP-N-L-S                 FALSE            FALSE      FALSE\nN-L-S                   FALSE            FALSE      FALSE\nP-L-S                   FALSE            FALSE      FALSE\nL-S                     FALSE            FALSE      FALSE\nP-N-S                   FALSE            FALSE      FALSE\nN-S                     FALSE            FALSE      FALSE\nP-S                     FALSE            FALSE      FALSE\nS                       FALSE            FALSE      FALSE\nP-N-L                   FALSE            FALSE      FALSE\nN-L                     FALSE            FALSE      FALSE\nP-L                     FALSE            FALSE      FALSE\nL                       FALSE            FALSE      FALSE\nP-N                     FALSE            FALSE      FALSE\nN                       FALSE            FALSE      FALSE\nP                       FALSE            FALSE      FALSE\n                        FALSE            FALSE      FALSE\n\n$regression_results\n    Coefficient          SE                Variable\n1  0.0692437554 0.008765654               Intercept\n2 -0.0248352010 0.006026387      Remove Punctuation\n3 -0.0012912608 0.006026387          Remove Numbers\n4  0.0008042152 0.006026387               Lowercase\n5  0.0315263364 0.006026387                Stemming\n6 -0.0031236074 0.006026387        Remove Stopwords\n7 -0.0194667091 0.006026387 Remove Infrequent Terms\n8 -0.0194780796 0.006026387              Use NGrams\n                Model\n1 Gun Pretext Results\n2 Gun Pretext Results\n3 Gun Pretext Results\n4 Gun Pretext Results\n5 Gun Pretext Results\n6 Gun Pretext Results\n7 Gun Pretext Results\n8 Gun Pretext Results\n\n\n\n#load(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/preText_results_gun_50_comps.RData\")\n\npreText_score_plot(preText_results)\n\n\n\n\nExplanation: After plotting we access the pretext\nscore with the minimum score, which is least unusual. This is the row\nwith the pre-processing steps refered to as “3” in the data. In addition\nL-3 results in the same preText Score.\n\n\nscores_new_pretext<-preText_results$preText_score \n\n# head(sort(scores_new_pretext))\n\n\n\nExplanation: Looking at the choices below I see that\n“3” does not do anything but use n-grams. L-3 does use lowercase and\nn-grams.\n\n\n# preprocessed_documents$choices\n\n\n\nExplanation Continued: Looking at the regression\ncoefficients we see negative scores as usual results and positive\ncoefficients as unusual ones. In this case removing puncuation,\nstopwords, and n-grams would not lead to a great deal of abnormality.\nThe scores below indicate that stemming would result in the most\nabnormality while all others but lowercase is the only other that has a\nnon-negative coefficinet.\n\n\nregression_coefficient_plot(preText_results,\n                            remove_intercept = TRUE)\n\n\n\n\nFeature Co-occurance Matrix\nExplanation: The feature co-occurance matrix can\ngive us a sense of which words in the dataset are occurring together\n\n\n# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of chapters\nsmaller_dfm <- dfm_trim(new_guns_corpus_dfm_punct_tl_sw, min_termfreq = 5)\n\n#smaller_dfm <- dfm_trim(smaller_dfm, min_docfreq = .3, docfreq_type = \"prop\")\n\n# create fcm from dfm\nsmaller_fcm <- fcm(smaller_dfm)\n\n# check the dimensions (i.e., the number of rows and the number of columnns)\n# of the matrix we created\ndim(smaller_fcm)\n\n\n[1] 226 226\n\n\n\n# pull the top features\nmyFeatures <- names(topfeatures(smaller_fcm, 40))\n\n# retain only those top features as part of our matrix\neven_smaller_fcm <- fcm_select(smaller_fcm, pattern = myFeatures, selection = \"keep\")\n\n# check dimensions\ndim(even_smaller_fcm)\n\n\n[1] 40 40\n\n# compute size weight for vertices in network\nsize <- log(colSums(even_smaller_fcm))\n\n# create plot\ntextplot_network(even_smaller_fcm, vertex_size = size / max(size) * 3)\n\n\n\n\nSentiment Results Using NRC\n\n\n# get_sentiments(\"nrc\")\n# get_sentiments(\"bing\")\n# get_sentiments(\"afinn\")\n\n\n\n\n\nsentimetnsdf <- read_csv(\"/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/sentimetnsdf.csv\")\n\n\n\n\n\nnew_guns_urls_df_2<-new_guns_urls_df\n\nnew_guns_urls_df_2$text<- seq(1, 980, by=1)\n\nnrc_joy <- sentimetnsdf %>% \n  filter(sentiment == \"joy\")\n\ntidy_posts_for_guns <- new_guns_urls_df_2 %>%\n  unnest_tokens(word, title) \n\ntidy_posts_for_guns %>%\n  inner_join(nrc_joy) %>%\n  count(word, sort = TRUE) %>% head() %>% kable()\n\n\nword\nn\ngood\n22\nlove\n12\nfinally\n11\nsafe\n10\nfun\n7\nfavorite\n6\n\n\n\ntidy_posts_for_guns_sentiment <- tidy_posts_for_guns %>%\n  inner_join(sentimetnsdf) %>%\n  count(text, sentiment) %>%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% \n  mutate(sentiment = positive - negative)\n\n\n\n\n\nnrc_sentiment <- get_sentiments(\"nrc\")\n\n\nnrc_guns_word_counts <- tidy_posts_for_guns %>%\n  inner_join(nrc_sentiment) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  ungroup()\n\nnrc_guns_word_counts %>%\n  group_by(sentiment) %>%\n  slice_max(n, n = 10) %>% \n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL)\n\n\n\n\n\n\nBing_sentiments<-get_sentiments(\"bing\")\n\ntidy_posts_for_guns_sentiment <- tidy_posts_for_guns %>%\n  inner_join(Bing_sentiments) %>%\n  count(text, sentiment) %>%\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% \n  mutate(sentiment = positive - negative)\n\nbing_word_counts <- tidy_posts_for_guns %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  ungroup()\n\nbing_word_counts %>%\n  group_by(sentiment) %>%\n  slice_max(n, n = 10) %>% \n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL)\n\n\n\n\n\n\ntidy_posts_for_guns$added_dates <- as.Date(tidy_posts_for_guns$date_utc)\n\n\nafinn <- tidy_posts_for_guns %>% \n  inner_join(get_sentiments(\"afinn\")) %>% \n  group_by(index = added_dates) %>% \n  summarise(sentiment = sum(value)) %>% \n  mutate(method = \"AFINN\")\nafinn\n\n\n# A tibble: 14 × 3\n   index      sentiment method\n   <date>         <dbl> <chr> \n 1 2022-03-14         9 AFINN \n 2 2022-03-15        17 AFINN \n 3 2022-03-16         0 AFINN \n 4 2022-03-17        23 AFINN \n 5 2022-03-18         7 AFINN \n 6 2022-03-19        30 AFINN \n 7 2022-03-20        13 AFINN \n 8 2022-03-21        11 AFINN \n 9 2022-03-22        19 AFINN \n10 2022-03-23         8 AFINN \n11 2022-03-24        32 AFINN \n12 2022-03-25         7 AFINN \n13 2022-03-26         6 AFINN \n14 2022-03-27         0 AFINN \n\n\n\nafinn %>%\n  ggplot(aes(index, sentiment, fill = method)) +\n  geom_col(show.legend = FALSE,   width = 0.7)  + \n  geom_smooth(aes(y = sentiment), color = \"black\")+\nfacet_wrap(~method, ncol = 1, scales = \"free_y\")+\n  theme_minimal()\n\n\n\n\nSentiment Results Using BING\nExplanation: Using nrc appears to have had some\nunintended effects that may require an analysis of the specific words\nused to describe sentiment. One difficult part of the data being used is\nthat firearms, and the words used to describe them, are percieved\n\n\nlibrary(methods)\n\ntoo_gun_dfm<- quanteda::dfm(new_guns_corpus, verbose = FALSE)\n\ntoo_gun_dfm\n\n\n\n\n\nlibrary(topicmodels)\n\ngun_dfm_lda <- LDA(too_gun_dfm, k = 2, control = list(seed = 777))\ngun_dfm_lda\n\n\n\n\n\ngun_dfm_lda_topics <- tidy(gun_dfm_lda, matrix = \"beta\")\ngun_dfm_lda_topics\n\n\n\n\n\ngun_top_terms <- gun_dfm_lda_topics %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>% \n  ungroup() %>%\n  arrange(topic, -beta)\n\ngun_top_terms %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\nbeta_wide <- gun_dfm_lda_topics %>%\n  mutate(topic = paste0(\"topic\", topic)) %>%\n  pivot_wider(names_from = topic, values_from = beta) %>% \n  filter(topic1 > .001 | topic2 > .001) %>%\n  mutate(log_ratio = log2(topic2 / topic1))\n\nbeta_wide\n\n\n\n\n\nbeta_wide %>%\n  group_by(direction = log_ratio > 0) %>%\n  slice_max(abs(log_ratio), n = 10) %>% \n  ungroup() %>%\n  mutate(term = reorder(term, log_ratio)) %>%\n  ggplot(aes(log_ratio, term)) +\n  geom_col() +\n  labs(x = \"Log2 ratio of beta in topic 2 / topic 1\", y = NULL)\n\n\n\nTopic Modeling analysis\nResponse: As can be seen above topic modeling may\nbenefit from some data reduction, removing punctuation and stop words\nwould likely be beneficial as can be seen above where a number of the\ndifferences between topics are modeled as punctuation and stop\nwords.\n\n\ngun_tokens_stopwords_and_punct_removed <- tokens_remove(tokens(new_guns_corpus, remove_punct = TRUE), c(stopwords(\"english\")))\n\ngun_corpus_stopwords_and_punct_removed <- corpus(sapply(gun_tokens_stopwords_and_punct_removed, paste, collapse=\" \")\n)\n\ngun_corpus_stopwords_and_punct_removed\n\n\n\n\n\nlibrary(methods)\n\ntoo_gun_dfm_no_punct_stopwords<- quanteda::dfm(tokens(gun_corpus_stopwords_and_punct_removed), verbose = FALSE)\n\ntoo_gun_dfm_no_punct_stopwords\n\n\n\n\n\nlibrary(topicmodels)\n\ngun_dfm_lda_nopunct_stop <- LDA(too_gun_dfm_no_punct_stopwords, k = 2, control = list(seed = 777))\n\ngun_dfm_lda_nopunct_stop\n\n\n\n\n\ngun_dfm_lda_topics_nopunct_stop <- tidy(gun_dfm_lda_nopunct_stop, matrix = \"beta\")\n\ngun_dfm_lda_topics_nopunct_stop\n\n\n\n\n\ngun_top_terms_no_punct_or_stop<- gun_dfm_lda_topics_nopunct_stop %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>% \n  ungroup() %>%\n  arrange(topic, -beta)\n\ngun_top_terms_no_punct_or_stop %>%\n  mutate(term = reorder_within(term, beta, topic)) %>%\n  ggplot(aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_y_reordered()\n\n\n\n\n\nbeta_wide_no_punct_stop <- gun_dfm_lda_topics_nopunct_stop %>%\n  mutate(topic = paste0(\"topic\", topic)) %>%\n  pivot_wider(names_from = topic, values_from = beta) %>% \n  filter(topic1 > .001 | topic2 > .001) %>%\n  mutate(log_ratio = log2(topic2 / topic1))\n\n\n\n\n\nbeta_wide_no_punct_stop %>%\n  group_by(direction = log_ratio > 0) %>%\n  slice_max(abs(log_ratio), n = 10) %>% \n  ungroup() %>%\n  mutate(term = reorder(term, log_ratio)) %>%\n  ggplot(aes(log_ratio, term)) +\n  geom_col() +\n  labs(x = \"Log2 ratio of beta in topic 2 / topic 1\", y = NULL)\n\n\n\nTokens and Corpus Work\n\n\ntop_guns_tokens <- tokens(new_guns_corpus)\n\nprint(top_guns_tokens)\n\n\nTokens consisting of 980 documents.\ntext1 :\n [1] \"With\"    \"tiny\"    \"ar15s\"   \"and\"     \"mcx\"     \"/\"      \n [7] \"similar\" \",\"       \"where\"   \"does\"    \"a\"       \"pcc\"    \n[ ... and 6 more ]\n\ntext2 :\n [1] \"I\"      \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"this\"  \n [8] \"is\"     \"good\"   \"enough\" \".\"      \"Ha\"    \n[ ... and 9 more ]\n\ntext3 :\n[1] \"Best\"    \"Home\"    \"Defense\" \"Round\"   \"in\"      \"5.56\"   \n[7] \"?\"      \n\ntext4 :\n [1] \"5\"       \"5\"       \"*\"       \"5\"       \"/\"       \"555\"    \n [7] \"&\"       \"53\"      \"5\"       \"50535.5\" \"\\\"\"      \"5-\"     \n\ntext5 :\n [1] \"My\"         \"suppressed\" \"12.5\"       \"AR\"         \"is\"        \n [6] \"the\"        \"same\"       \"size\"       \"as\"         \"my\"        \n[11] \"14.5\"       \"\\\"\"        \n[ ... and 1 more ]\n\ntext6 :\n [1] \"Springfield\" \"Hellcat\"     \"RDP\"         \"with\"       \n [5] \"SilencerCo\"  \"Omega\"       \"9k\"          \"and\"        \n [9] \"Streamlight\" \"TLR-7\"       \".\"          \n\n[ reached max_ndoc ... 974 more documents ]\n\ntop_guns_tokens_no_punct <- tokens(new_guns_corpus, \n    remove_punct = T)\n\nprint(top_guns_tokens_no_punct)\n\n\nTokens consisting of 980 documents.\ntext1 :\n [1] \"With\"    \"tiny\"    \"ar15s\"   \"and\"     \"mcx\"     \"similar\"\n [7] \"where\"   \"does\"    \"a\"       \"pcc\"     \"fit\"     \"in\"     \n[ ... and 3 more ]\n\ntext2 :\n [1] \"I\"      \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"this\"  \n [8] \"is\"     \"good\"   \"enough\" \"Ha\"     \"Anyone\"\n[ ... and 6 more ]\n\ntext3 :\n[1] \"Best\"    \"Home\"    \"Defense\" \"Round\"   \"in\"      \"5.56\"   \n\ntext4 :\n[1] \"5\"       \"5\"       \"5\"       \"555\"     \"53\"      \"5\"      \n[7] \"50535.5\" \"5-\"     \n\ntext5 :\n [1] \"My\"         \"suppressed\" \"12.5\"       \"AR\"         \"is\"        \n [6] \"the\"        \"same\"       \"size\"       \"as\"         \"my\"        \n[11] \"14.5\"       \"AR\"        \n\ntext6 :\n [1] \"Springfield\" \"Hellcat\"     \"RDP\"         \"with\"       \n [5] \"SilencerCo\"  \"Omega\"       \"9k\"          \"and\"        \n [9] \"Streamlight\" \"TLR-7\"      \n\n[ reached max_ndoc ... 974 more documents ]\n\n\n\ntop_guns_tokens_no_punct_no_upper <- tokens_tolower(top_guns_tokens_no_punct)\n\nprint(top_guns_tokens_no_punct_no_upper)\n\n\nTokens consisting of 980 documents.\ntext1 :\n [1] \"with\"    \"tiny\"    \"ar15s\"   \"and\"     \"mcx\"     \"similar\"\n [7] \"where\"   \"does\"    \"a\"       \"pcc\"     \"fit\"     \"in\"     \n[ ... and 3 more ]\n\ntext2 :\n [1] \"i\"      \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"this\"  \n [8] \"is\"     \"good\"   \"enough\" \"ha\"     \"anyone\"\n[ ... and 6 more ]\n\ntext3 :\n[1] \"best\"    \"home\"    \"defense\" \"round\"   \"in\"      \"5.56\"   \n\ntext4 :\n[1] \"5\"       \"5\"       \"5\"       \"555\"     \"53\"      \"5\"      \n[7] \"50535.5\" \"5-\"     \n\ntext5 :\n [1] \"my\"         \"suppressed\" \"12.5\"       \"ar\"         \"is\"        \n [6] \"the\"        \"same\"       \"size\"       \"as\"         \"my\"        \n[11] \"14.5\"       \"ar\"        \n\ntext6 :\n [1] \"springfield\" \"hellcat\"     \"rdp\"         \"with\"       \n [5] \"silencerco\"  \"omega\"       \"9k\"          \"and\"        \n [9] \"streamlight\" \"tlr-7\"      \n\n[ reached max_ndoc ... 974 more documents ]\n\n\n\ntop_guns_tokens_no_punct_no_upper_no_stop <-    tokens_select(top_guns_tokens_no_punct_no_upper, pattern = stopwords(\"en\"), selection = \"remove\")\n\nlength(top_guns_tokens_no_punct_no_upper_no_stop)\n\n\n[1] 980\n\nprint(top_guns_tokens_no_punct_no_upper_no_stop)\n\n\nTokens consisting of 980 documents.\ntext1 :\n[1] \"tiny\"    \"ar15s\"   \"mcx\"     \"similar\" \"pcc\"     \"fit\"    \n[7] \"arsenal\" \"now\"    \n\ntext2 :\n [1] \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"good\"   \"enough\"\n [8] \"ha\"     \"anyone\" \"else\"   \"anal\"   \"stuff\" \n\ntext3 :\n[1] \"best\"    \"home\"    \"defense\" \"round\"   \"5.56\"   \n\ntext4 :\n[1] \"5\"       \"5\"       \"5\"       \"555\"     \"53\"      \"5\"      \n[7] \"50535.5\" \"5-\"     \n\ntext5 :\n[1] \"suppressed\" \"12.5\"       \"ar\"         \"size\"       \"14.5\"      \n[6] \"ar\"        \n\ntext6 :\n[1] \"springfield\" \"hellcat\"     \"rdp\"         \"silencerco\" \n[5] \"omega\"       \"9k\"          \"streamlight\" \"tlr-7\"      \n\n[ reached max_ndoc ... 974 more documents ]\n\n\n\ntop_guns_corpus_tokens <- tokens(new_guns_corpus)\n\nprint(top_guns_corpus_tokens)\n\n\nTokens consisting of 980 documents.\ntext1 :\n [1] \"With\"    \"tiny\"    \"ar15s\"   \"and\"     \"mcx\"     \"/\"      \n [7] \"similar\" \",\"       \"where\"   \"does\"    \"a\"       \"pcc\"    \n[ ... and 6 more ]\n\ntext2 :\n [1] \"I\"      \"still\"  \"don\"    \"t\"      \"feel\"   \"like\"   \"this\"  \n [8] \"is\"     \"good\"   \"enough\" \".\"      \"Ha\"    \n[ ... and 9 more ]\n\ntext3 :\n[1] \"Best\"    \"Home\"    \"Defense\" \"Round\"   \"in\"      \"5.56\"   \n[7] \"?\"      \n\ntext4 :\n [1] \"5\"       \"5\"       \"*\"       \"5\"       \"/\"       \"555\"    \n [7] \"&\"       \"53\"      \"5\"       \"50535.5\" \"\\\"\"      \"5-\"     \n\ntext5 :\n [1] \"My\"         \"suppressed\" \"12.5\"       \"AR\"         \"is\"        \n [6] \"the\"        \"same\"       \"size\"       \"as\"         \"my\"        \n[11] \"14.5\"       \"\\\"\"        \n[ ... and 1 more ]\n\ntext6 :\n [1] \"Springfield\" \"Hellcat\"     \"RDP\"         \"with\"       \n [5] \"SilencerCo\"  \"Omega\"       \"9k\"          \"and\"        \n [9] \"Streamlight\" \"TLR-7\"       \".\"          \n\n[ reached max_ndoc ... 974 more documents ]\n\n\n\nhead(annotated.guns_corpus$token)\n\n\n# A tibble: 6 × 11\n  doc_id   sid tid   token token_with_ws lemma upos  xpos  feats      \n   <int> <int> <chr> <chr> <chr>         <chr> <chr> <chr> <chr>      \n1      1     1 1     With  \"With \"       with  ADP   IN    <NA>       \n2      1     1 2     tiny  \"tiny \"       tiny  ADJ   JJ    Degree=Pos \n3      1     1 3     ar    \"ar\"          ar    NOUN  NN    Number=Sing\n4      1     1 4     15s   \"15s \"        15s   NOUN  NNS   Number=Plur\n5      1     1 5     and   \"and \"        and   CCONJ CC    <NA>       \n6      1     1 6     mcx/  \"mcx/\"        mcx/  SYM   NFP   <NA>       \n# … with 2 more variables: tid_source <chr>, relation <chr>\n\nhead(annotated.guns_corpus$document)\n\n\n  doc_id\n1      1\n2      2\n3      3\n4      4\n5      5\n6      6\n\ndoc_id_guns<-annotated.guns_corpus$document\n\ndoc_id_guns$date<-new_guns_urls_df$date_utc\n\nannoData <- left_join(doc_id_guns, annotated.guns_corpus$token, by = \"doc_id\")\n\nannoData$date<-as.Date(annoData$date)\n\n\n\n\n\nannoData %>% \n  group_by(date) %>% \n  summarize(Sentences = max(sid)) %>%\n  ggplot(aes(date, Sentences)) +\n    geom_line() +\n    geom_smooth() +\n    theme_bw()\n\n\n\n\n\n\n#sentimetnsdf<-get_sentiments(\"nrc\")\n\n#write.csv(sentimetnsdf, file = \"sentimetnsdf.csv\")\n\n#save(sentimetnsdf, file=\"sentimetnsdf_2\")\n\n\n\n`\nTopic\nModeling analysis with stopwords and punctuation removed\nResponse: As can be seen from the results above,\nremoving stopwords and punctuation removes a good deal of the unwanted\nlanguage from the corpus and does a slightly more comprehensible job in\ndisplaying the information. However, any kind of stemming or reduction\nwill be difficult with posts about firearms for a number of reasons.\nFirstly the language surrounding firearms involves numbers for model\nnumbers, ammunition calibers and the capacity of magazines and other\ndevices that hold bullets. This results in difficulty removing both\npunctuation and numbers from the data as they give a sense of what sort\nof each of the aforementioned items people are interesting in talking\nabout. As a results removing the punctuation is difficult because it\nallows for more comprehensible data by reducing the usage of unneeded\npunctuation like exclamaintion points and questions marks that are\ncommon on a forum of this nature but not useful in analyzing the common\ntopics and language.\n\n\n\n",
    "preview": "posts/2022-03-29-tad-post-4/tad-post-4_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2022-03-29T13:20:33-04:00",
    "input_file": "tad-post-4.knit.md"
  },
  {
    "path": "posts/2022-03-29-ml-homework-2/",
    "title": "ML Homework 2",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Noah Milstein",
        "url": {}
      }
    ],
    "date": "2022-03-29",
    "categories": [],
    "contents": "\n\n\n\nCh. 4, Exercise 16\nQuestion: Using the Boston data set, fit\nclassification models in order to predict whether a given census tract\nhas a crime rate above or below the median. Explore logistic regression,\nLDA, naive Bayes, and KNN models using various subsets of the\npredictors. Describe your findings.\nLooking at the Boston Data\nData Exploration and\nRevue of Homework 1\nAnswer: In homework 1 we looked at predictors for\nlinear regression individual and with all in the same equation however\nwe did not look at logistic regression. As can be seen from the\ncorrelation graph above and prior correlation calculations a handful of\npredictors seem to exhibit some correlation with crime that I will be\nusing, these will be rad, tax dis medv, and lstat.\nHere we want to predict if crime is above the median so first we will\nmake a variable that reflects this.\n\n\nmedian_crime_rate <- median(Boston$crim)\n\nmedian_crime_rate %>% kable()\n\n\nx\n0.25651\n\nAnswer Median Crime: In order to create the median\ncrime rate dummy variable I calculate the median crime rate in the\nBoston dataset and use the mutate function to create a new column with\nthis data.\n\n\nBoston_over_median <- Boston %>% mutate(median_crime_in_tract = case_when(\n  crim < 0.25651 ~ 0,\n    crim >= 0.25651 ~ 1))\n\n\n\nAnswer Pairs After this I look at pair to attempt to\ndistinguish if some variables seem particularly correlated with median\ncrime rate visually.\n\n\n\nBuilding a training and test set: Though it is\nmentioned more in later chapters the authors do use a split in their\ndata in Smarket to act as their training. In this case I will do\nsomething similar, however, I will use a sample of 70% of the data.\n\n\nset.seed(222)\n\nin_training<- sample(1:nrow(Boston_over_median),  nrow(Boston) * 0.7 )\n\ntraining_boston <- Boston_over_median[in_training,]\n\ntest_boston <- Boston_over_median[-in_training,]\n\n\n\nModel Choice Logistic 1: Firstly I will be using all\nof the variables in my logistic regression and using both p-values and\nstandard errors along with the predict function to evaluate the\nmodel\n\n\nset.seed(222)\n\nlogsitic_Boston_1<-  glm(median_crime_in_tract ~ zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + lstat + medv, data = Boston_over_median, family = binomial, subset=in_training)\n\nsummary(logsitic_Boston_1)\n\n\n\nCall:\nglm(formula = median_crime_in_tract ~ zn + indus + chas + nox + \n    rm + age + dis + rad + tax + ptratio + lstat + medv, family = binomial, \n    data = Boston_over_median, subset = in_training)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.0510  -0.1337  -0.0003   0.0035   3.4664  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -48.641048   7.865389  -6.184 6.24e-10 ***\nzn           -0.092755   0.042744  -2.170  0.03000 *  \nindus        -0.074307   0.050366  -1.475  0.14012    \nchas          0.468421   0.798887   0.586  0.55765    \nnox          51.914808   9.097957   5.706 1.16e-08 ***\nrm            0.444596   0.867999   0.512  0.60851    \nage           0.014141   0.013691   1.033  0.30167    \ndis           0.784864   0.256550   3.059  0.00222 ** \nrad           0.631667   0.196382   3.217  0.00130 ** \ntax          -0.005575   0.003062  -1.821  0.06861 .  \nptratio       0.442991   0.153263   2.890  0.00385 ** \nlstat         0.141092   0.058371   2.417  0.01564 *  \nmedv          0.166757   0.086194   1.935  0.05303 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 490.75  on 353  degrees of freedom\nResidual deviance: 146.76  on 341  degrees of freedom\nAIC: 172.76\n\nNumber of Fisher Scoring iterations: 9\n\n\n\nset.seed(222)\n\nlogsitic_1_Boston_probs <- predict(logsitic_Boston_1, test_boston,\ntype = \"response\")\n\nlog_preds_1<-ifelse(logsitic_1_Boston_probs >= 0.5, 1, 0)\n\nprediction_1_logs <-mean(log_preds_1 == test_boston$median_crime_in_tract)\n\nprediction_1_logs %>% kable()\n\n\nx\n0.8618421\n\nResults Logistic 1: As can be seen above the first\nlogistic model appears to correctly evaluate whether or not a census\ntract in our test set is above or below the median correctly 0.8618421\nor 86.2% of the time.\nModel Choice Logistic 2: For my second function I\nwill be using only the variables that my glm output as statistically\nsignificant based on the glm p-values. This would suggests that most of\nthese coefficients are unlikely to be zero.\n\n\nset.seed(222)\n\nlogsitic_Boston_2 <-  glm(median_crime_in_tract ~ zn + nox + dis + rad + \n                            \nptratio + medv , data = Boston_over_median, family = binomial, \n\nsubset=in_training)\n\nsummary(logsitic_Boston_2)\n\n\n\nCall:\nglm(formula = median_crime_in_tract ~ zn + nox + dis + rad + \n    ptratio + medv, family = binomial, data = Boston_over_median, \n    subset = in_training)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.07337  -0.24571  -0.00187   0.00753   3.13797  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -35.02931    5.78928  -6.051 1.44e-09 ***\nzn           -0.07337    0.03319  -2.210 0.027089 *  \nnox          41.29154    6.64906   6.210 5.29e-10 ***\ndis           0.60332    0.21526   2.803 0.005068 ** \nrad           0.48104    0.13704   3.510 0.000448 ***\nptratio       0.32090    0.12317   2.605 0.009175 ** \nmedv          0.11077    0.03732   2.968 0.002993 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 490.75  on 353  degrees of freedom\nResidual deviance: 168.50  on 347  degrees of freedom\nAIC: 182.5\n\nNumber of Fisher Scoring iterations: 8\n\nResults Logistic 2: As can be seen above this has\nlead to a very slightly reduction in the error rate from about 86% to\nabout 89% of tracts being correctly identified.\nModel Choice Logistic 3: Next, I will remove\nvariables that changed in their significance, I will remove ones that\nwent down in significance in my third iteration and include those that\nstayed the same or improved in their “statistical significance so here\nwe just remove zn.\n\n\nset.seed(727)\n\nlogsitic_Boston_3 <-  glm(median_crime_in_tract ~ nox + dis + rad + \n                            \nmedv , data = Boston_over_median, family = binomial, subset=in_training)\n\nsummary(logsitic_Boston_3)\n\n\n\nCall:\nglm(formula = median_crime_in_tract ~ nox + dis + rad + medv, \n    family = binomial, data = Boston_over_median, subset = in_training)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.96251  -0.34660  -0.03326   0.01130   2.61492  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -23.82357    4.00139  -5.954 2.62e-09 ***\nnox          36.77822    6.01008   6.119 9.39e-10 ***\ndis           0.30796    0.16164   1.905 0.056747 .  \nrad           0.43698    0.12121   3.605 0.000312 ***\nmedv          0.03308    0.02884   1.147 0.251420    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 490.75  on 353  degrees of freedom\nResidual deviance: 187.91  on 349  degrees of freedom\nAIC: 197.91\n\nNumber of Fisher Scoring iterations: 8\n\nResults Logistic 3: As can be seen by the error rate\nabove, removing the single variable did not serve to improve the fit as\nit is still guessed about 89% of the tracts correctly.\nModel Choice LDA 1: My first fit for LDA will again\nstart by using all of the variables in the dataset.\n\n\nset.seed(292)\n\nlda.fit_1 <- lda(median_crime_in_tract ~ zn + indus + chas + nox + rm + \nage + dis + rad + tax + ptratio + lstat + medv, \ndata = Boston_over_median, subset=in_training)\n\nlda.fit_1\n\n\nCall:\nlda(median_crime_in_tract ~ zn + indus + chas + nox + rm + age + \n    dis + rad + tax + ptratio + lstat + medv, data = Boston_over_median, \n    subset = in_training)\n\nPrior probabilities of groups:\n  0   1 \n0.5 0.5 \n\nGroup means:\n         zn     indus       chas       nox       rm      age      dis\n0 23.161017  6.824407 0.06214689 0.4684520 6.407825 49.71808 5.085899\n1  1.265537 15.204181 0.10169492 0.6392147 6.174492 86.04802 2.537347\n       rad      tax  ptratio     lstat     medv\n0  4.19209 307.8192 17.85706  9.061356 25.25424\n1 13.92655 496.0226 18.95932 16.165819 20.23842\n\nCoefficients of linear discriminants:\n                  LD1\nzn      -0.0075446955\nindus    0.0111437527\nchas    -0.2075615996\nnox      9.1062150722\nrm       0.2401656098\nage      0.0131358309\ndis      0.1330540845\nrad      0.0584337654\ntax     -0.0006712969\nptratio  0.1157332806\nlstat    0.0409740539\nmedv     0.0509953338\n\nResults LDA 1: As can be seen above the LDA output\nsuggests that pihat1 and pihat2 are both 0.5 so half of the tracts are\nbelow and half are above the median as should be expected.\n\n\nset.seed(292)\n\nlda.pred <- predict(lda.fit_1, test_boston)\n\nlda.class <- lda.pred$class\n\ntable(lda.class, test_boston$median_crime_in_tract) %>% kable()\n\n\n\n0\n1\n0\n67\n17\n1\n9\n59\n\nResults LDA 1 Continued: As can be seen above the\nfirst LDA model with all of the predictors included predicts the test\nmedian crime rate dummy variable correctly about 82.9% of the time, in\ntotal it predicts the splits above correctly and the total percent as\nindicated below.\n\n\nmean(lda.class == test_boston$median_crime_in_tract) %>% kable()\n\n\nx\n0.8289474\n\nResults LDA 1 Continued: Below we see that with a\n50% threshold we acquire the two predictions below.\n\n\nsum(lda.pred$posterior[, 1] >= .5) %>% kable()\n\n\nx\n84\n\nsum(lda.pred$posterior[, 1] < .5) %>% kable()\n\n\nx\n68\n\nResults LDA 1 Continued: The plot of the fit is as\nplotted below.\n\n\nplot(lda.fit_1)\n\n\n\n\nModel Choice LDA 2: Since LDA does not give an\nindication of the performance of individual predictors I will model my\nsecond one using the predictors that logistic regression indicated as\nstatistically significant, or likely to be non-zero.\n\n\nset.seed(292)\n\nlda.fit_2 <- lda(median_crime_in_tract ~  zn + nox + dis + rad + ptratio \n                 + medv , data = Boston_over_median, subset=in_training)\n\nlda.fit_2 \n\n\nCall:\nlda(median_crime_in_tract ~ zn + nox + dis + rad + ptratio + \n    medv, data = Boston_over_median, subset = in_training)\n\nPrior probabilities of groups:\n  0   1 \n0.5 0.5 \n\nGroup means:\n         zn       nox      dis      rad  ptratio     medv\n0 23.161017 0.4684520 5.085899  4.19209 17.85706 25.25424\n1  1.265537 0.6392147 2.537347 13.92655 18.95932 20.23842\n\nCoefficients of linear discriminants:\n                 LD1\nzn      -0.007133417\nnox     10.155028179\ndis     -0.006666860\nrad      0.051446121\nptratio  0.116810250\nmedv     0.036270047\n\nResults LDA 2: As can be seen above the LDA output\nsuggests that pihat1 and pihat2 are both 0.5 so half of the tracts are\nbelow and half are above the median as should be expected as in the\nfirst example.\n\n\nset.seed(292)\n\nlda.pred_2 <- predict(lda.fit_2, test_boston)\n\nlda_2.class <- lda.pred_2$class\n\ntable(lda_2.class, test_boston$median_crime_in_tract) %>% kable()\n\n\n\n0\n1\n0\n65\n15\n1\n11\n61\n\nResults LDA 2 continued: As can be seen below this\nLDA performs incredibly similarly to the first 0.8289474 with both\nhaving identical correct classification rates.\n\n\nmean(lda_2.class == test_boston$median_crime_in_tract) %>% kable()\n\n\nx\n0.8289474\n\n\n\nsum(lda.pred_2$posterior[, 1] >= .5) %>% kable()\n\n\nx\n80\n\nsum(lda.pred_2$posterior[, 1] < .5) %>% kable()\n\n\nx\n72\n\n\n\nplot(lda.fit_2)\n\n\n\n\nModel Choice LDA 3: For my third LDA model choice I\nused all of the dummy variables and nox to see how using a number of\ncategorical variables effects the outcome of LDA.\n\n\nset.seed(7209)\n\nlda.fit_3<- lda(median_crime_in_tract ~ age + rad + chas\n + nox , data = Boston_over_median, subset=in_training)\n\nlda.fit_3\n\n\nCall:\nlda(median_crime_in_tract ~ age + rad + chas + nox, data = Boston_over_median, \n    subset = in_training)\n\nPrior probabilities of groups:\n  0   1 \n0.5 0.5 \n\nGroup means:\n       age      rad       chas       nox\n0 49.71808  4.19209 0.06214689 0.4684520\n1 86.04802 13.92655 0.10169492 0.6392147\n\nCoefficients of linear discriminants:\n            LD1\nage  0.01539699\nrad  0.06022015\nchas 0.01643176\nnox  6.98239859\n\n\n\nset.seed(7209)\n\nlda.pred_3 <- predict(lda.fit_3, test_boston)\n\nlda.class_3 <- lda.pred_3$class\n\ntable(lda.class_3, test_boston$median_crime_in_tract)\n\n\n           \nlda.class_3  0  1\n          0 70 16\n          1  6 60\n\nResults Model Choice LDA 3: As seen above this\nsubset of predictors has decreased the misclassification error rate as\ncompared to the previous models using just the most “statistically\nsignificant” predictors in LDA. The correct classification rate is\naround 85% here which is a slight increase from previous models.\n\n\nmean(lda.class_3 == test_boston$median_crime_in_tract) %>% kable()\n\n\nx\n0.8552632\n\n\n\nsum(lda.pred_3$posterior[, 1] >= .5)%>% kable()\n\n\nx\n86\n\nsum(lda.pred_3$posterior[, 1] < .5) %>% kable()\n\n\nx\n66\n\n\n\nplot(lda.fit_3)\n\n\n\n\nModel Choice Naive Bayes: My first model for naive\nbayes uses an, nox, rad, and taxes to predict median crime rate.\n\n\nset.seed(2680)\n\nnb.fit_1 <- naiveBayes(median_crime_in_tract ~ zn + nox +rad +\n                         tax , data = Boston_over_median, in_training)\n\nnb.fit_1\n\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n  0   1 \n0.5 0.5 \n\nConditional probabilities:\n   zn\nY        [,1]      [,2]\n  0 21.525692 29.319808\n  1  1.201581  4.798611\n\n   nox\nY        [,1]       [,2]\n  0 0.4709711 0.05559789\n  1 0.6384190 0.09870365\n\n   rad\nY        [,1]     [,2]\n  0  4.158103 1.659121\n  1 14.940711 9.529843\n\n   tax\nY       [,1]     [,2]\n  0 305.7431  87.4837\n  1 510.7312 167.8553\n\nResults 1 Naive Bayes: As can be seen from the error\nrates below the first naive bayes model correctly classifies the\npredictions 83% of the time whihc is similar to the results of the\nprevious LDA models.\n\n\nset.seed(2680)\n\nnb.class_1 <- predict(nb.fit_1, test_boston)\n\ntable(nb.class_1, test_boston$median_crime_in_tract) %>% kable()\n\n\n\n0\n1\n0\n65\n15\n1\n11\n61\n\n\n\nmean(nb.class_1 ==  test_boston$median_crime_in_tract) %>% kable()\n\n\nx\n0.8289474\n\nModel Choice 2 Naive Bayes: As can be seen below the\nnext model choice uses the same statistically significant predictors\naccording to their p-values in logsitic regression to see if this will\nimprove the prediction of Naive Bayes.\n\n\nset.seed(382)\n\nnb.fit_2 <- naiveBayes(median_crime_in_tract ~ zn + nox + dis + rad + \n              ptratio + medv , data = Boston_over_median, subset = in_training)\n\nnb.fit_2\n\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n  0   1 \n0.5 0.5 \n\nConditional probabilities:\n   zn\nY        [,1]     [,2]\n  0 23.161017 30.10405\n  1  1.265537  4.93395\n\n   nox\nY        [,1]       [,2]\n  0 0.4684520 0.05410754\n  1 0.6392147 0.10448031\n\n   dis\nY       [,1]     [,2]\n  0 5.085899 2.084295\n  1 2.537347 1.180086\n\n   rad\nY       [,1]     [,2]\n  0  4.19209 1.626256\n  1 13.92655 9.545955\n\n   ptratio\nY       [,1]     [,2]\n  0 17.85706 1.836210\n  1 18.95932 2.414571\n\n   medv\nY       [,1]      [,2]\n  0 25.25424  6.828791\n  1 20.23842 10.349862\n\nNaive Bayes model 2 Results: The inclusion of 6\nvariables, as opposed to 4 slightly improves the correct classification\nrate of our model, in this case, about a percent.\n\n\nset.seed(2680)\n\nnb.class_2 <- predict(nb.fit_2, test_boston)\n\ntable(nb.class_2, test_boston$median_crime_in_tract) %>% kable()\n\n\n\n0\n1\n0\n61\n9\n1\n15\n67\n\n\n\nmean(nb.class_2 ==  test_boston$median_crime_in_tract) %>% kable()\n\n\nx\n0.8421053\n\ntraining_boston <- Boston_over_median[in_training,]\ntest_boston <- Boston_over_median[-in_training,]\nModel Choice 3 naive Bayes: For this naive bayes I\nwill be using the categorical variables in addition to nox and number of\nrooms to see the impact of these two categorical variables with the two\nindicators of density and pollution.\n\n\nset.seed(282)\n\nnb.fit_3 <- naiveBayes(median_crime_in_tract ~ nox + chas +rad + \n                         rm , data = Boston_over_median, subset = in_training)\n\nnb.fit_3\n\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n  0   1 \n0.5 0.5 \n\nConditional probabilities:\n   nox\nY        [,1]       [,2]\n  0 0.4684520 0.05410754\n  1 0.6392147 0.10448031\n\n   chas\nY         [,1]      [,2]\n  0 0.06214689 0.2421070\n  1 0.10169492 0.3031041\n\n   rad\nY       [,1]     [,2]\n  0  4.19209 1.626256\n  1 13.92655 9.545955\n\n   rm\nY       [,1]      [,2]\n  0 6.407825 0.5381020\n  1 6.174492 0.8282674\n\nModel Evaluation Naive Bayes 3: As can be seen from\nthe results below the naive Bayes with these variables included performs\nvery well improving the number of correctly identified cross validated\npoints to about 88%.\n\n\nset.seed(235)\n\nnb.class_3 <- predict(nb.fit_3, test_boston)\n\ntable(nb.class_3, test_boston$median_crime_in_tract) %>% kable()\n\n\n\n0\n1\n0\n73\n14\n1\n3\n62\n\n\n\nmean(nb.class_3 ==  test_boston$median_crime_in_tract) %>% kable()\n\n\nx\n0.8881579\n\nModel Choice 1 KNN: Using nox and rad as our\nvariables below we see that 2 nearest neighbors results in a 98% rate of\ncorrect identifications.\n\n\nset.seed(777)\n\ntrain.X <- cbind(Boston_over_median$nox, Boston_over_median$rad)[in_training, ]\n\ntest.X <- cbind(Boston_over_median$nox, Boston_over_median$rad)[-in_training, ]\n\ntrain.median <- Boston_over_median$median_crime_in_tract[in_training]\n\nknn_pred_1 <- knn(train.X, test.X, train.median, k=2)\n\ntable(knn_pred_1, test_boston$median_crime_in_tract) %>% kable()\n\n\n\n0\n1\n0\n74\n1\n1\n2\n75\n\n\n\nmean(knn_pred_1 == test_boston$median_crime_in_tract) %>% kable()\n\n\nx\n0.9802632\n\nModel Choice 2 KNN: As can be seen below using these\ntwo variables and 3 nearest neighbors decreases the number of correctly\nidentified points in the test set slightly. Continuing to 4 nearest\nneighbors afterwards we see this trend continue.\n\n\nset.seed(377)\n\nknn_pred_2 <- knn(train.X, test.X, train.median, k=3)\n\ntable(knn_pred_2, test_boston$median_crime_in_tract)\n\n\n          \nknn_pred_2  0  1\n         0 72  1\n         1  4 75\n\n\n\nmean(knn_pred_2 == test_boston$median_crime_in_tract) %>% kable()\n\n\nx\n0.9671053\n\nModel Choice 3 KNN:\n\n\nset.seed(377)\n\nknn_pred_3 <- knn(train.X, test.X, train.median, k=4)\n\ntable(knn_pred_3, test_boston$median_crime_in_tract)\n\n\n          \nknn_pred_3  0  1\n         0 70  1\n         1  6 75\n\n\n\nmean(knn_pred_3 == test_boston$median_crime_in_tract) %>% kable()\n\n\nx\n0.9539474\n\nKNN-Results: According to the KNN’s above the\nresults of suggests that 2 nearest neighbors have the lowest amount of\nmisclassifications for the nox and rad variables.\nCh. 5, Exercise 2\nbasis: We will now derive the probability that a\ngiven observation is part of a bootstrap sample. Suppose that we obtain\na bootstrap sample from a set of n observations.\n(a)\nQuestion: What is the probability that the first\nbootstrap observation is not the jth observation from the original\nsample? Justify your answer.\nAnswer: The probability that the first bootstrap\nobservation is the jth observation from the original sample of n\nobservations is 1/n. This means the probability of the converse, that it\nis not the jth observation is the total probability 1 -\n1/n\n(b)\nQuestion: What is the probability that the second\nbootstrap observation is not the jth observation from the original\nsample?\nAnswer: The probability the second bootstrap\nobservation is not the jth observation is the same as the first since\nbootstrapping uses replacement so the probability of each observation\nbeing drawn does not change from bootstrap to bootstrap even if the\nnumber of interest is drawn. As the authors state that since there is\nreplacement the “same observation can occur more than once in the\nbootstrap data set” (211) so the second observation has the same\nprobability as the first of being the jth observation,\n(c)\nQuestion: Argue that the probability that the jth\nobservation is not in the bootstrap sample is (1 - 1/n)^n.\nAnswer: The probability that the jth observation is\nnot in the boot is (1 - 1/n)^n. This is because a the probability for a\nsingle observation is (1-1/n), as there is replacement the probability\nof drawing the jth observation does not change from bootstrap to\nbootstrap or draw to draw. As a result the probability of jth\nobservation in an entire bootstrap sample is the product of every single\nbootstrap observation not being j, so because we have n observations to\nchoose from our probability of not having any one in particular\nobservation is the probability of not drawing jth to the power of the\nnumber of observations.\n(d)\nQuestion: When n = 5, what is the probability that\nthe jth observation is in the bootstrap sample?\nAnswer: As seen below, the probability that the jth\nobservation is in the bootstrap is 0.67232, this is because the\nprobability of not drawing an observation is (1-1/n)^n\nso the probability of drawing one must be one minus this value as seen\nbelow.\n\n\n(1-(1 - 1/5)^5) %>% kable()\n\n\nx\n0.67232\n\n(e)\nQuestion: When n = 100, what is the probability that\nthe jth observation is in the bootstrap sample?\nAnswer: In this case when n = 100 the probability\nthat the jth observation is in the bootstrap sample is 0.6339677 or\n63.39677%\n\n\n(1-(1 - 1/100)^100) %>% kable()\n\n\nx\n0.6339677\n\n(f)\nQuestion: When n = 10, 000, what is the probability\nthat the jth observation is in the bootstrap sample?\nAnswer: In this case when n = 10,000 the probability\nthat the jth observation is in the bootstrap sample is 0.632139 or\n63.2139%\n\n\n(1-(1 - 1/10000)^10000) %>% kable()\n\n\nx\n0.632139\n\n(g)\nQuestion: Create a plot that displays, for each\ninteger value of n from 1 to 100,000, the probability that the jth\nobservation is in the bootstrap sample. Comment on what you observe.\nAnswer: Below I have plotted an integer value from 0\nto 100000 using the colon command. After this I calculate the\nprobability that the observation is in the dataset using 1 - the\nprobability that it is not in the dataset which is (1-(1 -\n1/possible_integers)^possible_integers), the inverse of the prior\nproblems. For clarity I have also used the prior n-sizes of 5, 100, and\n10,000. As can be seen the data appears to have a negative exponential\nrelationship that eventually reaches a minimum probability near 0.63 no\nmatter how larger the number of bootstraps we take is.\n\n\n\n\n\n\n\n\n\n(h)\nQuestion: We will now investigate numerically the\nprobability that a bootstrap sample of size n = 100 contains the jth\nobservation. Here j = 4. We repeatedly create bootstrap samples, and\neach time we record whether or not the fourth observation is contained\nin the bootstrap sample.\nAnswer: For the loop below the authors use a\nreplication function with an empty vector, NA, this is then done 10,000\ntimes. Next this goes into a loop with i in 1 through 10000. Next this\nfunction added a sample with replacement from 1:00, sees if it equals 4,\nsums the number of samples equal to 4 are greater than zero, this is\nthen stored in the “store” which is then expressed by taking its\nmean.\n\n\nset.seed(222222)\n\nstore <- rep(NA, 10000) \n\nfor(i in 1:10000){\n   \nstore[i] <- sum(sample(1:100, rep=TRUE) == 4) > 0 \n\n}\n\nmean(store)\n\n\n[1] 0.6362\n\nQuestion: Comment on the results obtained.\nAnswer continued: The results of the function are\n0.6362, this suggests that as the number of samples increases the\nlikelihood of observing a specific jth observation in an n out of 100 in\nan increasingly large number of bootstraps with replacement approaches\n0.636.\nCh. 5, Exercise 3\nBasis: We now review k-fold cross-validation.\nPart (a)\nQuestion: Explain how k-fold cross-validation is\nimplemented.\nAnswer: A k-folds cross validation method is\nimplemented by randomly diving sets of observations in our data into\ngroups, in this case they are known as folds. The data is divided k\ntimes and the first fold, which we refer to as the validation set is\nthen removed and the k-1 folds leftover are then fit using an analysis\nmethod, we then validate the model that we got from the model with the\ndata not in the fold to see how accurately the model predicts the left\nout data. This process is then repeated for all k-folds that were not\nthe first and were used to fit the original model. Iterating through\nthis process results in a cross validation estimate in the form of\nerrors between the predicted values from the model using k-1 and the\ntrue values in the left out k-fold.\nPart (b)\nQuestion: What are the advantages and disadvantages\nof k-fold cross-validation relative to:\nPart i.\nQuestion: The validation set approach?\nAnswer: The validation set approach faces the issue\nof insufficient data being used to construct its model. Since the method\ncan only divide the data in half its estimates can do a poor job\npredicting the left out set because it contains so much data that could\nbe reflective of the relationship between predictors and responses. This\ncan result in variance in comparing the predictions from the modeling\nset to the validation set. As the authors also point out the validation\nset approach is fitting on fewer observations it is likely to\noverestimate test error rate. The disadvantage of k-folds compared to\nvalidation set approach.\nPart ii.\nQuestion: LOOCV?\nAnswer: Leave one out cross validation is\nessentially a special case of k-folds cross validation where the number\nof “k” folds is the total number of observations in the dataset, minus\none. This approach is n-1 k-folds on the dataset and as a result it has\nlittle randomness in its selection thus it has an extremely small amount\nof variance between its predictions since a vast majority of data is\nused to make predictions that can only vary significantly from one point\nthat is not used. Depending on the size of the dataset cross validating\none for every point could take a great deal of computational power.\nUnlike the validation set leave-one-out-cross-validation will greatly\nunderestimate the test error rates since its prediction only excludes a\nsingle point. However in k-folds a number of folds that is not\ncomputationally intensive or accounts for a larger difference between\nfolds that allow for randomness in the subsets that can give a better\nsense of how errors are being made as they will be validated on many\nmore points. As a result a number of k-folds can be chosen that is not 2\nor n-1, this k can be determined in such a way that it likely has more\nbias than loocv, but balances the excessively high variance from 2 folds\nor validation set, and the excessively low variance of loocv.\nCh. 5, Exercise 5\nBasis: In Chapter 4, we used logistic regression to\npredict the probability of default using income and balance on the\nDefault data set. We will now estimate the test error of this logistic\nregression model using the validation set approach. Do not forget to set\na random seed before beginning your analysis.\n(a)\nQuestion: Fit a logistic regression model that uses\nincome and balance to predict default.\nAnswer: Below is my first equation labelled\ninc_bal_logit which uses the default data and a binomial family within a\nglm to model a logistic regression predicting whether of not an\nindividual defaults based on the income and their balance of debt.\n\n\nset.seed(777)\n\n\ninc_bal_logit <- glm(default ~ income + balance,  data = Default, family = \"binomial\")\n\ninc_bal_logit\n\n\n\nCall:  glm(formula = default ~ income + balance, family = \"binomial\", \n    data = Default)\n\nCoefficients:\n(Intercept)       income      balance  \n -1.154e+01    2.081e-05    5.647e-03  \n\nDegrees of Freedom: 9999 Total (i.e. Null);  9997 Residual\nNull Deviance:      2921 \nResidual Deviance: 1579     AIC: 1585\n\n(b)\nBasis: Using the validation set approach, estimate\nthe test error of this model. In order to do this, you must perform the\nfollowing steps:\ni.\nQuestion: Split the sample set into a training set\nand a validation set.\nAnswer: First I split the sample set into training\nand validation by sampling the data using the total number of rows in\nthe dataset nrow(Default) and diving it in half by sampling the\nnrow(Default), nrow(Default)/2 times. This amounts to sampling from\n10000 rows 5000 times randomly. Next I split the data into a validation\nand training set by choosing rows equivalent to those in my training set\nand not in my training set.\n\n\nset.seed(777)\n\ntrain5_5_1 <- sample(nrow(Default), nrow(Default)/2)\n\ndefault_training <- Default[train5_5_1,]\n\ndefault_validation <- Default[-train5_5_1,]\n\n\n\nii.\nQuestion: Fit a multiple logistic regression model\nusing only the training observations.\nAnswer: Next I fit a logistic regression using only\nmy training observations which I defined above as default_training\n\n\nset.seed(777)\n\ninc_bal_logit_ii <- glm(default ~ income + balance,  family = \"binomial\", data = default_training)\n\n\n\niii.\nQuestion: Obtain a prediction of default status for\neach individual in the validation set by computing the posterior\nprobability of default for that individual, and classifying the\nindividual to the default category if the posterior probability is\ngreater than 0.5.\nAnswer: I then predict whether or not an individual\nin the validation set defaults by using the predict() function which\ntakes my function inc_bal_logit_ii and my dataset, in this case the\nvalidation set default_validation, I then choose response as this is the\ntype of prediction we are interested in rather than terms.\n\n\nset.seed(777)\n\ndefault_estimates <- predict(inc_bal_logit_ii, default_validation,  type=\"response\")\n\nclassification <- ifelse(default_estimates>0.5,\"Yes\",\"No\")\n\n\n\niv.\nQuestion: Compute the validation set error, which is\nthe fraction of the observations in the validation set that are\nmisclassified.\nAnswer: I then compute the validation set error by\ntaking the mean of the number of classification outcomes that are not\nequal to their true values in the default_validation dataset’s default\ncolumn.\n\n\nmean(classification != default_validation$default)\n\n\n[1] 0.0256\n\n(a) (Split 2)\nQuestion: Fit a logistic regression model that uses\nincome and balance to predict default.\nAnswer: In the subsequent question I repeat my model\nusing different splits acquired by changing the seed which dictates the\nrandom generation of all “random” generation in the model through the\nsampling split.\n\n\nset.seed(908)\n\ninc_bal_logit <- glm(default ~ income + balance,  data = Default, \n                     family = \"binomial\")\n\ninc_bal_logit\n\n\n\nCall:  glm(formula = default ~ income + balance, family = \"binomial\", \n    data = Default)\n\nCoefficients:\n(Intercept)       income      balance  \n -1.154e+01    2.081e-05    5.647e-03  \n\nDegrees of Freedom: 9999 Total (i.e. Null);  9997 Residual\nNull Deviance:      2921 \nResidual Deviance: 1579     AIC: 1585\n\n(b) (Split 2)\nBasis: Using the validation set approach, estimate\nthe test error of this model. In order to do this, you must perform the\nfollowing steps:\ni.\nQuestion: Split the sample set into a training set\nand a validation set.\n\n\nset.seed(908)\n\ntrain5_5_1 <- sample(nrow(Default), nrow(Default)/2)\n\ndefault_training<-Default[train5_5_1,]\n\ndefault_validation<-Default[-train5_5_1,]\n\n\n\nii.\nQuestion: Fit a multiple logistic regression model\nusing only the training observations.\n\n\nset.seed(908)\n\ninc_bal_logit_ii <- glm(default ~ income + balance,  family = \"binomial\", \n                        data = default_training)\n\n\n\niii.\nQuestion: Obtain a prediction of default status for\neach individual in the validation set by computing the posterior\nprobability of default for that individual, and classifying the\nindividual to the default category if the posterior probability is\ngreater than 0.5.\n\n\nset.seed(908)\n\ndefault_estimates<-predict(inc_bal_logit_ii, default_validation, \n                           type=\"response\")\n\nclassification<-ifelse(default_estimates>0.5,\"Yes\",\"No\")\n\n\n\niv.\nQuestion: Compute the validation set error, which is\nthe fraction of the observations in the validation set that are\nmisclassified.\n\n\nmean(classification != default_validation$default)\n\n\n[1] 0.0286\n\n(a) (Split 3)\nQuestion: Fit a logistic regression model that uses\nincome and balance to predict default.\n\n\nset.seed(2000)\n\ninc_bal_logit <- glm(default ~ income + balance,  data = Default,\n                     family = \"binomial\")\n\ninc_bal_logit\n\n\n\nCall:  glm(formula = default ~ income + balance, family = \"binomial\", \n    data = Default)\n\nCoefficients:\n(Intercept)       income      balance  \n -1.154e+01    2.081e-05    5.647e-03  \n\nDegrees of Freedom: 9999 Total (i.e. Null);  9997 Residual\nNull Deviance:      2921 \nResidual Deviance: 1579     AIC: 1585\n\n(b) (Split 3)\nBasis: Using the validation set approach, estimate\nthe test error of this model. In order to do this, you must perform the\nfollowing steps:\ni.\nQuestion: Split the sample set into a training set\nand a validation set.\n\n\nset.seed(2000)\n\ntrain5_5_1 <- sample(nrow(Default), nrow(Default)/2)\n\ndefault_training<-Default[train5_5_1,]\n\ndefault_validation<-Default[-train5_5_1,]\n\n\n\nii.\nQuestion: Fit a multiple logistic regression model\nusing only the training observations.\n\n\nset.seed(2000)\n\ninc_bal_logit_ii <- glm(default ~ income + balance,  family = \"binomial\", \n                        data = default_training)\n\n\n\niii.\nQuestion: Obtain a prediction of default status for\neach individual in the validation set by computing the posterior\nprobability of default for that individual, and classifying the\nindividual to the default category if the posterior probability is\ngreater than 0.5.\n\n\nset.seed(2000)\n\ndefault_estimates<-predict(inc_bal_logit_ii, default_validation,  \n                           type=\"response\")\n\nclassification<-ifelse(default_estimates>0.5,\"Yes\",\"No\")\n\n\n\niv.\nQuestion: Compute the validation set error, which is\nthe fraction of the observations in the validation set that are\nmisclassified.\n\n\nmean(classification != default_validation$default)\n\n\n[1] 0.0294\n\n(a) (Split 4)\nQuestion: Fit a logistic regression model that uses\nincome and balance to predict default.\n\n\nset.seed(2)\n\ninc_bal_logit <- glm(default ~ income + balance,  data = Default,\n                     family = \"binomial\")\n\ninc_bal_logit\n\n\n\nCall:  glm(formula = default ~ income + balance, family = \"binomial\", \n    data = Default)\n\nCoefficients:\n(Intercept)       income      balance  \n -1.154e+01    2.081e-05    5.647e-03  \n\nDegrees of Freedom: 9999 Total (i.e. Null);  9997 Residual\nNull Deviance:      2921 \nResidual Deviance: 1579     AIC: 1585\n\n(b) (Split 4)\nBasis: Using the validation set approach, estimate\nthe test error of this model. In order to do this, you must perform the\nfollowing steps:\ni.\nSplit the sample set into a training set and a validation set.\n\n\nset.seed(2)\n\ntrain5_5_1 <- sample(nrow(Default), nrow(Default)/2)\n\ndefault_training<-Default[train5_5_1,]\n\ndefault_validation<-Default[-train5_5_1,]\n\n\n\nii.\nFit a multiple logistic regression model using only the training\nobservations.\n\n\nset.seed(2)\n\ninc_bal_logit_ii <- glm(default ~ income + balance, \n                        family = \"binomial\", data = default_training)\n\n\n\niii.\nObtain a prediction of default status for each individual in\nthe validation set by computing the posterior probability of default\nfor that individual, and classifying the individual to the default\ncategory if the posterior probability is greater than 0.5.\n\n\nset.seed(2)\n\ndefault_estimates<-predict(inc_bal_logit_ii, default_validation,  \n                           type=\"response\")\n\nclassification<-ifelse(default_estimates>0.5,\"Yes\",\"No\")\n\n\n\niv.\nCompute the validation set error, which is the fraction of the\nobservations in the validation set that are misclassified.\n\n\nmean(classification != default_validation$default)\n\n\n[1] 0.0238\n\n(d)\nQuestion: Now consider a logistic regression model\nthat predicts the probability of default using income, balance, and a\ndummy variable for student. Estimate the test error for this model using\nthe validation set approach. Comment on whether or not including a dummy\nvariable for student leads to a reduction in the test error rate.\n\n\nset.seed(222)\n\n\nstudent_bal_logit <- glm(default ~ income + balance + student,  \n                         data = Default, family = \"binomial\")\n\nstudent_bal_logit\n\n\n\nCall:  glm(formula = default ~ income + balance + student, family = \"binomial\", \n    data = Default)\n\nCoefficients:\n(Intercept)       income      balance   studentYes  \n -1.087e+01    3.033e-06    5.737e-03   -6.468e-01  \n\nDegrees of Freedom: 9999 Total (i.e. Null);  9996 Residual\nNull Deviance:      2921 \nResidual Deviance: 1572     AIC: 1580\n\n(b)\nBasis: Using the validation set approach, estimate\nthe test error of this model. In order to do this, you must perform the\nfollowing steps:\ni.\nSplit the sample set into a training set and a validation set.\n\n\nset.seed(222)\n\ntrain5_5_1 <- sample(nrow(Default), nrow(Default)/2)\n\ndefault_training<-Default[train5_5_1,]\n\ndefault_validation<-Default[-train5_5_1,]\n\n\n\nii.\nFit a multiple logistic regression model using only the training\nobservations.\n\n\nset.seed(222)\n\nstudent_bal_logit_ii <- glm(default ~ income + balance,  \n                            family = \"binomial\", data = default_training)\n\n\n\niii.\nObtain a prediction of default status for each individual in\nthe validation set by computing the posterior probability of default\nfor that individual, and classifying the individual to the default\ncategory if the posterior probability is greater than 0.5.\n\n\nset.seed(222)\n\ndefault_estimates_student<-predict(student_bal_logit_ii, default_validation,  \n                                   type=\"response\")\n\nclassification_students<-ifelse(default_estimates_student>0.5,\"Yes\",\"No\")\n\n\n\niv.\nCompute the validation set error, which is the fraction of the\nobservations in the validation set that are misclassified.\n\n\nmean(classification != default_validation$default)\n\n\n[1] 0.0492\n\n(d)\nAnswer: It appears that including the dummy variable\nfor student increases the missclassification error rate in the dataset,\nin this way the inclusion of the variable is not beneficial in reducing\nthe error rate of the function as it increases from under 0.03, or less\nthan 3% to 0.0468 using my seed which is 4.68% which thus decreases the\naccuracy of the model. This could suggest that student variable leads to\nsome calculations in the model that are more likely to mis-classify.\nStudents take on a great volume of debt at a young age, however they do\nso as an investment for later earnings, as a result students with high\ndebt may be misclassified by the regression due to their likely high\ndebt.\nCh. 5, Exercise 9\nBasis: We will now consider the Boston housing data\nset, from the ISLR2 library.\n(a)\nQuestion: Based on this data set, provide an\nestimate for the population mean of medv. Call this estimate mu.\nAnswer: My calculation for the population mean is\nfound my taking the mean of the entire column of medv from the Boston\ndataset which is stored as mu\n\n\nmu <- mean(Boston$medv)\n\nmu %>% kable()\n\n\nx\n22.53281\n\n(b)\nQuestion: Provide an estimate of the standard error\nof mu. Interpret this result.\nHint: We can compute the standard error of the sample mean by\ndividing the sample standard deviation by the square root of the number\nof observations.\nAnswer: Using the hint I label the standard error of\nmu as the standard deviation of the Boston$medv column and diving it by\nthe number of observations nrow in the Boston dataset which has its\nsquare root taken before deiving the standard deviation, this is then\nstored in se_mu\n\n\nse_mu<- sd(Boston$medv)/sqrt(nrow(Boston))\n\nse_mu %>% kable()\n\n\nx\n0.4088611\n\n(c)\nQuestion: Now estimate the standard error of mu\nusing the bootstrap. How does this compare to your answer from (b)?\nAnswer: In order to estimate the standard error of\nmu using a bootstrap I must first create a function that can be used by\nboot, which will return our statistic of interest. In this case the data\nis Boston and its column medv Boston$medv which are placed in the first\nobservation of the bootstrap. Next I need a statistic writing the\nfunction I needed it to specify the dataframe and column index as\ndescribed in the chapter 5 lab. Next I need this function to return the\nmean of this dataframe’s specified index. Following the chapter 5 lab I\nuse 1000 replicates of the bootstrap.\n\n\n?boot\nset.seed(547)\n\nmu_function <- function(data, index) {\n mu_for_function <- mean(data[index])\n return(mu_for_function)\n}\n\n\nboot(Boston$medv, mu_function , R = 1000)\n\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Boston$medv, statistic = mu_function, R = 1000)\n\n\nBootstrap Statistics :\n    original     bias    std. error\nt1* 22.53281 0.01013577   0.4090362\n\n(d)\nQuestion: Based on your bootstrap estimate from (c),\nprovide a 95 % confidence interval for the mean of medv. Compare it to\nthe results obtained using t.test(Boston$medv).\nHint: You can approximate a 95 % confidence interval\nusing the formula [mu - 2SE(mu), mu + 2SEmu].\nAnswer: I create the 95% confidence interval from my\nbootstrap using my mean estimate plus and minus 2 standard deviations,\nthis estimate is 22.53281 as the mean I then use the hint formula addin\nor subtracting 2 standard errors.\n\n\n(sd(Boston$medv)/sqrt(nrow(Boston)) )%>% kable()\n\n\nx\n0.4088611\n\nci_95_boot <- c(\n (22.53281-2*0.4132074),(22.53281+2*0.4132074)\n   )\nci_95_boot %>% kable()\n\n\nx\n21.70640\n23.35922\n\n\n\nt.test(Boston$medv)\n\n\n\n    One Sample t-test\n\ndata:  Boston$medv\nt = 55.111, df = 505, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 21.72953 23.33608\nsample estimates:\nmean of x \n 22.53281 \n\nAnswer Continued: The t-test estimates (21.72953\n23.33608) and our bootstrap estimates (21.70640 23.35922). The\ndifferences in the lower tail are below:\n\n\n21.72953-21.70640\n\n\n[1] 0.02313\n\nAnswer Continued: The differences in the upper tail\nare below:\n\n\n23.33608 - 23.35922\n\n\n[1] -0.02314\n\nAnswer Continued: The difference is similar in both\ndirections on the lower tail the t-test estimates a slightly higher\nupper tail and a slightly higher on the lower tail.\n(e)\nQuestion: Based on this data set, provide an\nestimate, mu_med, for the median value of medv in the population.\nAnswer: I calculate this median value as the median\nof the Boston dataset and medv column using the median function which is\nsaved as mu_med.\n\n\nmu_med <- median(Boston$medv)\n\nmu_med %>% kable()\n\n\nx\n21.2\n\n(f)\nQuestion: We now would like to estimate the standard\nerror of mu_med.Unfortunately, there is no simple formula for computing\nthe standard error of the median. Instead, estimate the standard error\nof the median using the bootstrap. Comment on your findings.\nAnswer: Below I create a function to take the median\nof a dataset and its column index that works the same way as the prior\nmean function.\n\n\nset.seed(8282)\n\nmedian_function <- function(data, index) {\n median_for_function <- median(data[index])\n return(median_for_function)\n}\n\nboot(Boston$medv, median_function, R = 1000)\n\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Boston$medv, statistic = median_function, R = 1000)\n\n\nBootstrap Statistics :\n    original   bias    std. error\nt1*     21.2 -0.01325    0.381356\n\nAnswer Continued: The resulting standard error from\nthis bootstrap is 0.381356.\n(g)\nQuestion: Based on this data set, provide an\nestimate for the tenth percentile of medv in Boston census tracts. Call\nthis quantity mu_0.1. (You can use the quantile() function.)\nAnswer: Below I calculate the 10th percentile using\nthe Boston dataset and the medv column and the quantile function, then\nspecifying the quantile between [0 and 1] I use 0.1 as this is the 10th\npercentile\n\n\nmu_0.1 <- quantile(Boston$medv, 0.1)\n\nmu_0.1 %>% kable()\n\n\n\nx\n10%\n12.75\n\n(h)\nQuestion: Use the bootstrap to estimate the standard\nerror of mu_0.1. Comment on your findings.\n\n\nset.seed(8282)\n\n\ntenth_quantile_function <- function(data, index) {\n tenth_for_function <- quantile(data[index], 0.1)\n return(tenth_for_function)\n}\n\nboot(Boston$medv, tenth_quantile_function, R = 1000)\n\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = Boston$medv, statistic = tenth_quantile_function, \n    R = 1000)\n\n\nBootstrap Statistics :\n    original  bias    std. error\nt1*    12.75  -0.004   0.5122122\n\nAnswer: In this case I estimate the average quantile\nestimate of medv in the 10th percentile as 12.75 in the bootstrap as\ncompared to 12.75 in the dataset. It appears that both function capture\nthe same exact medv point as the 10th percentile in the dataset.\nCh. 6, Exercise 2\nBasis: For parts (a) through (c), indicate which of\ni. through iv. is correct. Justify your answer.\n(a)\nQuestion: The lasso, relative to least squares,\nis:\ni.\nMore flexible and hence will give improved prediction accuracy when its\nincrease in bias is less than its decrease in variance.\nAnswer: Compared to least squares the lasso\nregression is less flexible since it will perform variable selection,\nthis is meant to reduce overfitting which least squares will often do.\nIn this case the answer is correct because lasso’s are not more flexible\nto least squares since they do not fit as many variables and thus will\nnot fit the data as closely.\nii.\nMore flexible and hence will give improved prediction accuracy when its\nincrease in variance is less than its decrease in bias.\nAnswer: This answer is is incorrect as in the case\nof the first one because a\niii.\nLess flexible and hence will give improved prediction accuracy when its\nincrease in bias is less than its decrease in variance.\nAnswer: This answer is correct, as established lasso\nis less flexible as it will reduce non-influential parameters to zero\ndepending on its tuning parameters but will always be less flexible than\nthe regression it is based on if the tuning parameter is non-zero.\nHowever as the authors in ISLR explain “the lasso solution can yield a\nreduction in variance at the expense of a small increase in bias,” in\ncases when least squares is over fit and results in inaccurate\npredictions,” which this case follows.\niv.\nLess flexible and hence will give improved prediction accuracy when its\nincrease in variance is less than its decrease in bias.\nAnswer: This answer is incorrect as lasso regression\nis formulated in order to reduce overfitting and thus variance on fitted\ndata. As a result, and relative to least squares, lasso is not used to\nreduce bias, but instead variance in estimates.\n(b)\nQuestion: ridge regression relative to least\nsquares.\ni.\nMore flexible and hence will give improved prediction accuracy when its\nincrease in bias is less than its decrease in variance.\nAnswer: As in the first response the ridge\nregression also is less flexible than least squares because it removes\nvariables, or at least most their coefficients closer to zero, however,\nunlike lasso, it keeps the p-varaibles within the model. In this case\nmaking variables smaller does not allow the ridge regression to follow\nthe data as closely as least squares as increasing the shrinkage\ncoefficient leads to a decrease in flexibility by making the fit closer,\nthe second part is correct however as the ridge regression will decrease\nvariance as overfitting is also reduced..\nii.\nMore flexible and hence will give improved prediction accuracy when its\nincrease in variance is less than its decrease in bias.\nAnswer: This response is also incorrect as if the\nshrinkage coefficient is sufficiently small the function is less\nflexible, the second part is also incorrect as the function is valuable\nwhen it gives improved prediction accuracy when its increase in\nbias is sufficiently less than its decrease in\nvariance.\niii.\nLess flexible and hence will give improved prediction accuracy when its\nincrease in bias is less than its decrease in variance.\nAnswer: This response is again correct, here like\nlasso, we reduce the influence of non-influential parameters by moving\nthem towards, or to zero, as a result these variables will lose\ninfluence thus not being as likely to overfit as a result, the function\nmay not be able to avoid bias as much as linear regression but can\ngreatly reduce the variance due to overfitting.\niv.\nLess flexible and hence will give improved prediction accuracy when its\nincrease in variance is less than its decrease in bias.\nAnswer: This is again incorrect as it will decrease\nvariance rather than bias in its efforts not to overfit.\n(c)\nQuestion: non-linear methods relative to least\nsquares.\ni.\nMore flexible and hence will give improved prediction accuracy when its\nincrease in bias is less than its decrease in variance.\nAnswer: Non-linear methods are more flexible than\nleast squares, however, in most cases it is useful when its increase in\nbias is less than its decrease in\nii.\nMore flexible and hence will give improved prediction accuracy when its\nincrease in variance is less than its decrease in bias.\nAnswer: Non-linear methods are more flexible as they\nwill fit predictions more closely, they are also able to afford more\npredictive accuracy when their increase in variance is less than their\nincrease in bias, since bias is the difference between a parameters and\nthe expected value of a statistic the non-linear model will give better\npredictions when its increase in variance is less than its increase in\nbias. This is due to the bias variance trade-off, here out non-linear\nmodel fits data more closely thus it has the potential to have higher\nvariance if it is overfit than linear regression, as a result if that\nincrease in variance is sufficiency less than its decrease in bias due\nto its closer fit than we consider it a stronger model for the scenario\nthus this answer is correct.\niii.\nLess flexible and hence will give improved prediction accuracy when its\nincrease in bias is less than its decrease in variance.\nAnswer: This answer is incorrect as non-linear\nmodels are more flexible than linear ones.\niv.\nLess flexible and hence will give improved prediction accuracy when its\nincrease in variance is less than its decrease in bias.\nAnswer: This answer is incorrect as non-linear\nmodels are more flexible than linear ones.\nISLR Ch. 6, Exercise 9\nBasis: In this exercise, we will predict the number\nof applications received using the other variables in the College data\nset.\n(a)\nQuestion: Split the data set into a training set and\na test set.\nAnswer: I split the data into training and test set\nusing 1 through the number of rows in the dataset and sampling it\nnrow(College)*0.8 times which works out to taking 621.6 row samples with\nreplacement equals FALSE, this equates to and 80%, 20% split in the\ntraining and test sets. I specify the college_training as the 80% of\nCollege data I specified as my training. Next I find the test set using\nthe converse of the training rows.\n\n\nset.seed(9292)\n\ntrain_College_numbers <- sample(1:nrow(College), nrow(College)*0.8,\n                                replace = FALSE)\n\ncollege_training <- College[train_College_numbers,]\n\ncollege_test <- College[-train_College_numbers,]\n\n\n\n(b)\nQuestion: Fit a linear model using least squares on\nthe training set, and report the test error obtained.\nAnswer: I fit a least squares on my 80% training set\nbelow using all variables since we will be using a ridge regression that\nlater account for the number of variables we should use as the ridge\nmoves their coefficients towards zero.\n\n\nset.seed(9292)\n\nleast_squares_college <- glm(Grad.Rate ~. , data=college_training)\n\n\n\nAnswer: I then use the predict function using my\nleast squares function and my test data to make prediction for my error\nrate. I express the test error as the mean square error, which is\ncalculated as 142.0359\n\n\nset.seed(9292)\n\npred_college_for_mse <- predict(least_squares_college, newdata=college_test)\n\nMSE_college_test <- mean((college_test$Grad.Rate-pred_college_for_mse)^2)\n\nMSE_college_test %>% kable()\n\n\nx\n190.8233\n\n(c)\nQuestion: Fit a ridge regression model on the\ntraining set, with lambda chosen by cross-validation. Report the test\nerror obtained.\nAnswer: Below I make a grid of values that is a\nsequence of 10 to -2 of length 100 which goes from 10^10 to 10^(-2)\ncreates a set of selected values. In order to fit a ridge model using\ncross validation I split the model into training and test set. Using the\nglmnet() function I then plug in my x-ridge model matrix and the y-ridge\nbeing the true values from the graduation rate column of the college\ndataset. This results in a dimension of 18 by 100.\n\n\nset.seed(9292)\n\nx_ridge <- model.matrix(Grad.Rate ~ ., College)[, -1] \n\ny_ridge <- College$Grad.Rate\n\ngrid <- 10^seq(10, -2, length = 100)\n\nridge.mod <- glmnet(x_ridge, y_ridge, alpha = 0, lambda = grid)\n\ndim(coef(ridge.mod))\n\n\n[1]  18 100\n\nAnswer: Below I then split my data into an 80% 20%\nsplit between training and test sets. These values are stored in the\ntrain_ridge and test_ridge variables.\n\n\nset.seed(9292)\n\ntrain_ridge <- sample(1:nrow(x_ridge), nrow(x_ridge)*0.8 ) \n\ntest_ridge <- (-train_ridge)\n\ny.test_ridge <- y_ridge[test_ridge]\n\n\n\nAnswer: I then use the glmnet() function with an\nalpha of 0 which is a ridge penalty in this case. In this scenario the\nfunction is named ridge.mod, this is then put into predict() where it\npredicts on the test section of the data.\n\n\nset.seed(9292)\n\nridge.mod <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], \n                    alpha = 0, lambda = grid, thresh = 1e-12)\n\nridge.pred <- predict(ridge.mod, s = 4, newx = x_ridge[test_ridge,])\n\nmean((ridge.pred - y.test_ridge)^2) %>% kable()\n\n\nx\n191.9619\n\nAnswer: Afterwards we fit the model again using the\nglmnet() function ridge.mod using a new x from x_ridge and our x and y\nfrom training. We then access the 18 coefficients below, as can be seen\nthe ridge moves the coefficients it decides have little impact towns\nzero.\n\n\nset.seed(9292)\n\nridge.pred <- predict(ridge.mod, s = 0, newx = x_ridge[test_ridge, ], \n                      exact = T, x = x_ridge[train_ridge, ], y = y_ridge[train_ridge])\n\npredict(ridge.mod, s = 0, exact = T, type = \"coefficients\", \n        x = x_ridge[train_ridge, ], y = y_ridge[train_ridge])[1:18, ]\n\n\n  (Intercept)    PrivateYes          Apps        Accept        Enroll \n28.8970053130  4.6799076483  0.0012568714 -0.0008668106  0.0023593568 \n    Top10perc     Top25perc   F.Undergrad   P.Undergrad      Outstate \n 0.0468337629  0.1437342449 -0.0004323463 -0.0013350746  0.0008084910 \n   Room.Board         Books      Personal           PhD      Terminal \n 0.0021795777  0.0004206020 -0.0017882831  0.1379432456 -0.0702662338 \n    S.F.Ratio   perc.alumni        Expend \n 0.0406692785  0.2997380403 -0.0004995716 \n\nAnswer continued: Next after beginning these tests\nwe use cross validation to get the optimal tuning parameters from a\nnumber of repetitions using cv.glmnet() in this case it takes our\ntraining values and an alpha of zero to indicate it is a ridge and uses\na number of values of lambda to determine which minimizes mean squared\nerrors. This is plotted below\n\n\nset.seed(9292)\n\ncv.out <- cv.glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], alpha = 0) \n\nplot(cv.out)\n\n\n\n\nAnswer continued: The cv.out we calculated above\noutputs a 1se and min lambda for its mean square errors which I access\nbelow to determine the cross validated tuning parameter.\n\n\nset.seed(9292)\n\nbestlam <- cv.out$lambda.min\n\nbestlam\n\n\n[1] 1.876765\n\nAnswer continued: Finally we use out predict\nfunction to see what our mean squared error will be for our tuning\nparameters than minimizes MSE, in this case it is 191.0022 as seen below\ncompared to the grid values which yielded a 191.9619 at best.\n\n\nset.seed(9292)\n\nridge.pred <- predict(cv.out, s = bestlam, newx = x_ridge[test_ridge,])\n\nmean((ridge.pred - y.test_ridge)^2) %>% kable()\n\n\nx\n191.0022\n\nAnswer continued: The coefficient outputs for the\nlambda that minimizes MSE are then outputted below. Here Private school,\nper.alumni, Top10perc, and top25perc, along with Phd having coefficients\nover 0.05 in predicting the graduation rate.\n\n\nset.seed(9292)\n\nout <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], alpha = 0)\n\npredict(out, type = \"coefficients\", s = bestlam)[1:18, ]\n\n\n  (Intercept)    PrivateYes          Apps        Accept        Enroll \n30.4028107340  4.3857666072  0.0005579653  0.0002846755  0.0002715066 \n    Top10perc     Top25perc   F.Undergrad   P.Undergrad      Outstate \n 0.0898592282  0.1170937551 -0.0001524760 -0.0012353250  0.0006590921 \n   Room.Board         Books      Personal           PhD      Terminal \n 0.0020270611  0.0001479914 -0.0018704177  0.0918979855 -0.0206644006 \n    S.F.Ratio   perc.alumni        Expend \n 0.0319300362  0.2726551009 -0.0002998044 \n\n(d)\nQuestion: Fit a lasso model on the training set,\nwith lambda chosen by cross validation. Report the test error obtained,\nalong with the number of non-zero coefficient estimates.\nAnswer: The process of creating the lasso is largely\nsimilar to that of the ridge though here our inital glmnet() takes an\nalpha of 1. The plot below this indicates that depending on our choise\nof tuning parameter lambda, certain coefficients will become zero.\n\n\nset.seed(9292)\n\nlasso.mod <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], \n                    alpha = 1, lambda = grid)\n\nplot(lasso.mod)\n\n\n\n\nAnswer continued: We continue this analysis by cross\nvalidation using cv.glmnet() with an alpha of 1 to find the coefficients\nthat again, minimize the MSE.\n\n\nset.seed(9292)\n\ncv.out_2 <- cv.glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], alpha = 1) \n\nplot(cv.out_2)\n\n\n\n\nAnswer continued: After this cross validation has\nbeen calculated we then access the lambda that minimizes MSE. We then\nuse the predict function to calculate predictions for our lasso using\nthe lambda that minimizes MSE this gets a MSE of 190.8808.\n\n\nset.seed(9292)\n\nbestlam <- cv.out_2$lambda.min\n\nlasso.pred <- predict(cv.out_2, s = bestlam, newx = x_ridge[test_ridge,])\n\nmean((lasso.pred - y.test_ridge)^2) %>% kable()\n\n\nx\n190.8808\n\nAnswer continued: Finally we calculate and show the\ncoefficients for our equation with the lasso reducing our number of\nparameters by moving coefficients towards zero that have low impact.\nHere we remove, Accept, Enroll, F.Undergrad, Books, Terminal, and\nS.F.Ratio with other values still being reflected in the equation\ncoefficients.\n\n\nset.seed(9292)\n\nout <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], \n              alpha = 1, lambda = grid)\n\nlasso.coef <- predict(out, type = \"coefficients\", s = bestlam)[1:18, ]\n\nlasso.coef\n\n\n  (Intercept)    PrivateYes          Apps        Accept        Enroll \n29.8146208578  4.2178703808  0.0006714817  0.0000000000  0.0000000000 \n    Top10perc     Top25perc   F.Undergrad   P.Undergrad      Outstate \n 0.0508305488  0.1371884865  0.0000000000 -0.0013060619  0.0007140270 \n   Room.Board         Books      Personal           PhD      Terminal \n 0.0020119407  0.0000000000 -0.0016927269  0.0683671684  0.0000000000 \n    S.F.Ratio   perc.alumni        Expend \n 0.0000000000  0.2909171513 -0.0002954747 \n\nCh. 8, Exercise 4\nUses Plots in Figure 8.14.\nPart (a)\nQuestion: Sketch the tree corresponding to the\npartition of the predictor space illustrated in the left-hand panel of\nFigure 8.14. The numbers inside the boxes indicate the mean of Y within\neach region.\nAnswer: Possible Partitions in 8.14 Parent\n(x1<1), if not below return 5. All else if x1 is below 1, next\n(x2<1, or x2>1) if x2 greater than 1 return 15, if x1<0 less\nthan 0 return 3, if it is more than 0, but x2 is also less than zero\nreturn 10\n\n\nlibrary(treemap)\nlibrary(DiagrammeR)\n\n\n\n\n\nfig8_1_4 <- Node$new(\"figure 8.14\")\n    top_node <- fig8_1_4$AddChild(\"If x1<1\")\n    return_5 <- top_node$AddChild(\"5\")\n    second_node <- top_node$AddChild(\"If x2<1\")\n      return_15 <- second_node$AddChild(\"15\")\n      third_node <- second_node$AddChild(\"If x1<0\")\n        return_3 <- third_node$AddChild(\"3\")\n          fourth_node <-  third_node$AddChild(\"x2<0\")\n              fifth_node <-  fourth_node$AddChild(\"10\")\n              fifth_node <-  fourth_node$AddChild(\"0\")\n\nplot(fig8_1_4)\n\n\n\n{\"x\":{\"diagram\":\"digraph {\\n\\n\\n\\n\\n  \\\"1\\\" [label = \\\"figure 8.14\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"2\\\" [label = \\\"If x1<1\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"3\\\" [label = \\\"5\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"4\\\" [label = \\\"If x2<1\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"5\\\" [label = \\\"15\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"6\\\" [label = \\\"If x1<0\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"7\\\" [label = \\\"3\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"8\\\" [label = \\\"x2<0\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"9\\\" [label = \\\"10\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"10\\\" [label = \\\"0\\\", fillcolor = \\\"#FFFFFF\\\", fontcolor = \\\"#000000\\\"] \\n  \\\"1\\\"->\\\"2\\\" \\n  \\\"2\\\"->\\\"3\\\" \\n  \\\"2\\\"->\\\"4\\\" \\n  \\\"4\\\"->\\\"5\\\" \\n  \\\"4\\\"->\\\"6\\\" \\n  \\\"6\\\"->\\\"7\\\" \\n  \\\"6\\\"->\\\"8\\\" \\n  \\\"8\\\"->\\\"9\\\" \\n  \\\"8\\\"->\\\"10\\\" \\n}\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\nPart (b)\nQuestion: Create a diagram similar to the left-hand\npanel of Figure 8.14, using the tree illustrated in the right-hand panel\nof the same figure. You should divide up the predictor space into the\ncorrect regions, and indicate the mean for each region.\n\n\n?plot\n\n\nHelp on topic 'plot' was found in the following packages:\n\n  Package               Library\n  base                  /Library/Frameworks/R.framework/Resources/library\n  graphics              /Library/Frameworks/R.framework/Versions/4.1/Resources/library\n\n\nUsing the first match ...\n\nplot(NA, NA, type = \"n\", xlim = c(0,3), \n     ylim = c(-1, 1.25), xlab = \"x-2\", ylab = \"x-1\")\n\nlines(x = c(1, 1), y = c(-1.0, 1.20))\n\nlines(x = c(2, 2), y = c(-1.0, 1.20))\n\nlines(x = c(0, 1), y = c(1, 1))\n\nlines(x = c(1, 2), y = c(0, 0))\n\ntext(0.5, 1.2 , \"0.63\") \n\ntext(0.5, 0.0 , \"-1.80\") \n\ntext(2.5, -0.0, \"-2.49\") \n\ntext(1.5, -0.5, \"-1.06\") \n\ntext(1.5, 0.5, \"0.21\") \n\n\n\n\nCh. 8, Exercise 7\nQuestion: In the lab, we applied random forests to\nthe Boston data using mtry = 6 and using ntree = 25 and ntree = 500.\nCreate a plot displaying the test error resulting from random forests on\nthis data set for a more comprehensive range of values for mtry and\nntree. You can model your plot after Figure 8.10. Describe the results\nobtained.\nAnswer: I will be modeling my response from that in\nfigure 8.10, in this case they use 3 m-values being m=p, m=p/2, and\nm=sqrt(p), in addition to this I will vary the number of trees in each\ncase. As seen below, I am using 13, 6.5 rounded up to 7 predictions, and\n3.6 rounded up to 4 for the square root of p.\n\n\n(ncol(Boston)-1)  %>% kable()\n\n\nx\n13\n\n(13/2) %>% kable()\n\n\nx\n6.5\n\n(sqrt(13)) %>% kable()\n\n\nx\n3.605551\n\nAnswer Continued: As in the lab, I will divide my\ndata into testing and training set, I will use 70% of the data in my\ntraining set for the model\n\n\nset.seed(888) \n\ntrain_boston_set <- sample(1:nrow(Boston), nrow(Boston)*0.7)\n\nRf_train_boston <- Boston[train_boston_set,]\n\nRf_test_boston <- Boston[-train_boston_set,]\n\nBoston_rf_mtry_13_ntree_25 <- randomForest(medv~. ,subset = train_boston_set, data = Boston, mtry = 13, ntree = 25)\n\nBoston_rf_mtry_7_ntree_25 <- randomForest(medv~.,subset = train_boston_set, data = Boston, mtry = 7, ntree = 25)\n\nBoston_rf_mtry_4_ntree_25 <- randomForest(medv~.,subset = train_boston_set,  data = Boston, mtry = 4, ntree = 25)\n\nBoston_rf_mtry_13_ntree_100 <- randomForest(medv~. ,subset = train_boston_set, data = Boston, mtry = 13, ntree = 100)\n\nBoston_rf_mtry_7_ntree_100 <- randomForest(medv~.,subset = train_boston_set, data = Boston, mtry = 7, ntree = 100)\n\nBoston_rf_mtry_4_ntree_100 <-randomForest(medv~.,subset = train_boston_set,  data = Boston, mtry = 4, ntree = 100)\n\nBoston_rf_mtry_13_ntree_500 <- randomForest(medv~. ,subset = train_boston_set, data = Boston, mtry = 13, ntree = 500)\n\nBoston_rf_mtry_7_ntree_500<-randomForest(medv~.,subset = train_boston_set, data = Boston, mtry = 7, ntree = 500)\n\nBoston_rf_mtry_4_ntree_500<-randomForest(medv~.,subset = train_boston_set,  data = Boston, mtry = 4, ntree = 500)\n\n\n\n\n\n\nAnswer continued: The plot above indicates the\naverage errors plotted against the number of trees used in the\nderivation, in this case I have included a legend as in 8.10 to indicate\nboth the number of trees used in addition to my comparison between m and\np used in my random forest. In this case it is apparent that trees in\nthe 500s initially have the lowest mean squared errors when compared to\n25 trees, and to a lesser extent, 100 trees. Below I will plot graphs\ncontaining only the 25 trees, 100 trees, and 500 trees.\n\n\n\nAnswer continued: Comparing the 25 trees to each\nother and 100 trees, it does firstly appear that 100 trees decreases the\nerror rate across the differing m and p values.\n\n\n\nAnswer continued: Comparing the 100 trees and 500s\ntrees it also appears that the higher number of trees decreases the\nerror rate in the data, with all plotted against each other as in the\nfirst graph it does appear that the larger number of trees decreases the\nerror rate more rapidly as the number of trees increases.\n\n\n\nCh. 8, Exercise 9\nBasis: This problem involves the OJ data set which\nis part of the ISLR2 package.\nPart (a)\nQuestion: Create a training set containing a random\nsample of 800 observations, and a test set containing the remaining\nobservations.\nAnswer: In order to take a sample 800 observations\nfrom OJ I use the sample function to take 800 from 1 through the total\nnumber of rows in the dataset. The test is then taken as the rows not\nincluded in this set of sampled rows.\n\n\nset.seed(727)\n\ntrain_orange_juice <- sample(1:nrow(OJ), 800, replace = FALSE)\n\nOJ_train <- OJ[train_orange_juice,]\n\nOJ_test <- OJ[-train_orange_juice,]\n\n\n\nPart (b)\nQuestion: Fit a tree to the training data, with\nPurchase as the response and the other variables as predictors. Use the\nsummary() function to produce summary statistics about the tree, and\ndescribe the results obtained. What is the training error rate? How many\nterminal nodes does the tree have?\nAnswer: Using the training data below I fit a tree\nusing the tree function on the training subset. The training error rate\nfrom the summary is 0.1475 and there are 8 terminal nodes.\n\n\nset.seed(727)\n\ntree.OJ_train <- tree(Purchase ~ ., OJ, subset = train_orange_juice)\n\nsummary(tree.OJ_train)\n\n\n\nClassification tree:\ntree(formula = Purchase ~ ., data = OJ, subset = train_orange_juice)\nVariables actually used in tree construction:\n[1] \"LoyalCH\"       \"PriceDiff\"     \"ListPriceDiff\" \"PctDiscMM\"    \nNumber of terminal nodes:  8 \nResidual mean deviance:  0.7145 = 565.9 / 792 \nMisclassification error rate: 0.1475 = 118 / 800 \n\nPart (c)\nQuestion: Type in the name of the tree object in\norder to get a detailed text output. Pick one of the terminal nodes, and\ninterpret the information displayed.\n\n\ntree.OJ_train\n\n\nnode), split, n, deviance, yval, (yprob)\n      * denotes terminal node\n\n 1) root 800 1065.00 CH ( 0.61625 0.38375 )  \n   2) LoyalCH < 0.5036 344  407.30 MM ( 0.27907 0.72093 )  \n     4) LoyalCH < 0.276142 172  131.50 MM ( 0.12791 0.87209 )  \n       8) LoyalCH < 0.051325 62   10.24 MM ( 0.01613 0.98387 ) *\n       9) LoyalCH > 0.051325 110  107.30 MM ( 0.19091 0.80909 ) *\n     5) LoyalCH > 0.276142 172  235.10 MM ( 0.43023 0.56977 )  \n      10) PriceDiff < 0.05 69   60.54 MM ( 0.15942 0.84058 ) *\n      11) PriceDiff > 0.05 103  137.60 CH ( 0.61165 0.38835 ) *\n   3) LoyalCH > 0.5036 456  351.30 CH ( 0.87061 0.12939 )  \n     6) LoyalCH < 0.764572 188  215.70 CH ( 0.73936 0.26064 )  \n      12) ListPriceDiff < 0.235 74  102.60 MM ( 0.50000 0.50000 )  \n        24) PctDiscMM < 0.196196 54   71.19 CH ( 0.62963 0.37037 ) *\n        25) PctDiscMM > 0.196196 20   16.91 MM ( 0.15000 0.85000 ) *\n      13) ListPriceDiff > 0.235 114   76.72 CH ( 0.89474 0.10526 ) *\n     7) LoyalCH > 0.764572 268   85.39 CH ( 0.96269 0.03731 ) *\n\nAnswer: One of the terminal nodes is the 7th one\nindicated, “7) LoyalCH > 0.764572 268, 85.39 CH ( 0.96269 0.03731 )”\nthis node indicates 6 attributes of the node the first, node), is\n7) in this case. The next attribute is the split in the\ndata that it represents in this case split, which is when the variable\nLoyalCH > 0.764572 next is the n, or number of observations that\nfulfill this attribute. Following this is the deviance of this\nobservation denoted as 85.39, yval is 0.96269, and finally (yprob) is\n0.03731.\nPart (d)\nQuestion: Create a plot of the tree, and interpret\nthe results.\n\n\n\nAnswer: The tree above indicates predicted\nclassification into 2 categories, either purchasing citrus hill or\nminute maid using class proportions in our training data, each split\nrepresents a distinction between observations in each predictor category\nas determined by prior steps to eliminate the most classification error.\nFirst Loyal CH, or customer brand loyalty for citrus hill is observed\nand split to be above or below a loyalty of 0.5036, this split then,\nagain distinguishes loyal CH. On the 2nd node on the left loyal ch is\nbelow 0.5036, it is then tested whether or not loyalty to CH is above or\nbelow 0.276, if it is below this, it is then test on the third level\nnode on the far left whether or not loyalty CH is below or above\n0.051325, in this case whether loyalty is below this or above it\n(indicating it is between 0.051325 and 0.276) the customers will always\nbe classified as being fans of minute main. Continuing from the 2nd node\non the left this time we test LoyalCH above 0.276, this leads us to the\nthird node PriceDiff < 0.05, in this case if the price difference is\nbetween the two and CH loyalty is above 0.276, the customer is\ncategorized as buying CH, however if the loyalty to CH is 0.276, but the\nprice difference is less than 0.05 then the customers is categorized as\nchoosing MM. Going to the second node on the right, LoyalCH < 0.764,\nwe categorize a customer as buying CH if their loyalty to the brand is\ngreater than 0.764. However if that node LoyalCH < 0.764 is less than\nthe stated value then we look to ListPriceDiff, in this case LoyalCH is\nbetween 0.50 and 0.764, this third node then predicts a customer as CH\nif the listed price difference is above 0.235, if the list price\ndifference is less we move to PctDiscMM or the percentage discount for\nMM, if the discount is above 0.196 the customer will choose MM, if it is\nbelow they will be categorized as choosing CH.\nPart (e)\nQuestion: Predict the response on the test data, and\nproduce a confusion matrix comparing the test labels to the predicted\ntest labels. What is the test error rate?\n\n\nset.seed(727)\n\ntest.pred <- predict(tree.OJ_train, OJ_test, type = \"class\")\n\ntable(test.pred, OJ_test$Purchase)  %>% kable()\n\n\n\nCH\nMM\nCH\n144\n40\nMM\n16\n70\n\nAnswer: The error rate is the number of incorrectly\nidentified purchasers of orange juice in terms of their actual purcahse\ncompared to whether or not they were predicted to purchase it by the\nmodel. It is calculated here as 0.2074074, or 20.7%, this is calculated\nbelow as one minus the difference between correctly identified buyers\nand incorrectly identified buyers.\n\n\n(1-(144+70)/(144+70+40+16) )  %>% kable()\n\n\nx\n0.2074074\n\nPart (f)\nQuestion: Apply the cv.tree() function to the\ntraining set in order to determine the optimal tree size.\nAnswer: The dev, or deviation represents the error\nrate. In this case it is lowest when size=8 and dev is 646.6050.\n\n\nset.seed(727)\n\ncv.OJ <- cv.tree(tree.OJ_train)\n\ncv.OJ \n\n\n$size\n[1] 8 7 6 5 4 3 2 1\n\n$dev\n[1]  646.6050  674.2620  673.7721  766.0959  770.2143  751.8258\n[7]  772.1636 1066.2872\n\n$k\n[1]      -Inf  14.04513  14.48895  36.41322  36.93239  40.72147\n[7]  50.20841 306.72764\n\n$method\n[1] \"deviance\"\n\nattr(,\"class\")\n[1] \"prune\"         \"tree.sequence\"\n\nPart (g)\nQuestion: Produce a plot with tree size on the\nx-axis and cross-validated classification error rate on the y-axis.\nAnswer: Below I have plotted the tree size compared\nto the classification error rate. As can be seen 8 is much lower than\nthe tree sizes that come before them in terms of error rate.\n\n\nset.seed(727)\n\nplot(cv.OJ$size, cv.OJ$dev, type = \"b\", xlab = \"Tree Size\", ylab=\"Cross Validated Misclass\") \n\n\n\n\nPart (h)\nQuestion: Which tree size corresponds to the lowest\ncross-validated classification error rate?\nAnswer: It appears that the tree with the lowest\npossible error is 8 in this case.\nPart (i)\nQuestion: Produce a pruned tree corresponding to the\noptimal tree size obtained using cross-validation. If cross-validation\ndoes not lead to selection of a pruned tree, then create a pruned tree\nwith five terminal nodes.\n\n\nset.seed(727)\n\nprune.OJ <- prune.misclass(tree.OJ_train, best = 8)\n\nsummary(prune.OJ)\n\n\n\nClassification tree:\ntree(formula = Purchase ~ ., data = OJ, subset = train_orange_juice)\nVariables actually used in tree construction:\n[1] \"LoyalCH\"       \"PriceDiff\"     \"ListPriceDiff\" \"PctDiscMM\"    \nNumber of terminal nodes:  8 \nResidual mean deviance:  0.7145 = 565.9 / 792 \nMisclassification error rate: 0.1475 = 118 / 800 \n\nAnswer: In this case the misclassification error\nrate is 0.1475 as indicated by the summary above.\nPart (j)\nQuestion: Compare the training error rates between\nthe pruned and un-pruned trees. Which is higher?\nAnswer: The training error rate in the pruned tree\nis 0.1475, this is far lower than the unpruned rate of 0.2074074.\nPart (k)\nQuestion: Compare the test error rates between the\npruned and unpruned trees. Which is higher?\n\n\nset.seed(727)\n\nprediction_pruned <- predict(prune.OJ, OJ_test,\ntype = \"class\")\n\ntable(prediction_pruned, OJ_test$Purchase) %>% kable()\n\n\n\nCH\nMM\nCH\n144\n40\nMM\n16\n70\n\nAnswer: Here we see the misclassification error rate\nis 0.2074074, or about 20.7% this is the same as the prior rate.\n\n\n(1-(144+70)/(144+40+16+70))  %>% kable()\n\n\nx\n0.2074074\n\nPossible Data Set Final\nQuestion:\nChosen Dataset: For my final project I wanted to\ncombine machine learning techniques with a networks dataset to see if\ncertain models could be used on it. I am using data on conflict which I\nam treating as a network with node and edge attributes, using these\nattributes I think it would be interesting to see if it is possible to\nuse classification techniques like KNN, LDA, and logistic regression to\npredict centrality and see what node and edge attributes contribute to\ncentrality ranking.\n\n\n\n",
    "preview": "posts/2022-03-29-ml-homework-2/ml-homework-2_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-03-29T13:27:06-04:00",
    "input_file": "ml-homework-2.knit.md"
  }
]
