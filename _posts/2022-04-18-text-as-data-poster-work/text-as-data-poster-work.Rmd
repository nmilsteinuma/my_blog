---
title: "Text as Data Poster Work"
description: |
  Work on corpus, document-term-matrix and visualizations for my Text-as-Data Poster.
author:
  - name: Noah Milstein
    url: {}
date: 2022-04-18
output:
  distill::distill_article:
    self_contained: false
---

```{r, echo=FALSE}
library(rmarkdown)
library(RedditExtractoR)
library(jsonlite)
library(tidyverse)
library(stringr)
library(dplyr)
library(httr)
library(tm)
library(corpus)
library(quanteda)
library(textclean)
library(knitr)
library(lubridate)
library(cleanNLP)
library(quanteda.textstats)
library(quanteda.textplots)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(stopwords)
library(tidyverse)
library(rvest)
library(tidytext)
library(text2vec)
library(preText)
library(ggplot2)
library(pals)
library(reshape2)
library(lda)
library(ldatuning)
# install klippy for copy-to-clipboard button in code chunks
remotes::install_github("rlesur/klippy")

```


```{r}
set.seed(836)
# First I use a fucntion to load an RData file as R has no default 

loadRData <- function(fileName){
#loads an RData file, and returns it
    load(fileName)
    get(ls()[ls() != "fileName"])
}

# My first data frame comprises new posts as of march 26 2022 which are taken 
# from the 15th-26th of march primarily. 

new_guns_urls <- loadRData("/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/New_reddit_posts_3_26.RData")

# Next I extract the 4 columns that I will need, title, date, comments
# and non-title text

new_guns_urls_df<-new_guns_urls[,c("title", "date_utc", "comments", "text")]

# Next I load the top posts of all time on the thread which go back to 2017 
# With observations to march 2022 when it was scraped.

top_guns_urls <- loadRData("/Users/noahmilstein/Desktop/Spring 2022/Textasdata/text_as_data_work/top_guns_urls.RData")

top_guns_urls_df<-top_guns_urls[,c("title", "date_utc", "comments",  "text")]

#hot_guns_urls <- find_thread_urls(subreddit="guns", sort_by="hot")
# save(hot_guns_urls, file="hot_guns_urls.RData")

# Next "hot" posts are loaded which are also primarily from march 2022

hot_guns_urls<-
  loadRData("/Users/noahmilstein/Desktop/my_blog/_posts/2022-03-29-tad-post-4/hot_guns_urls.RData")

hot_guns_urls_df<-hot_guns_urls[,c("title", "date_utc", "comments", "text")]
#hot_guns_urls_df

```

```{r}

set.seed(836)

#First I add rownames from 1 to 844 to the hot guns data frame

rownames(hot_guns_urls_df) <- seq(1, 844, 1)

#Next I add rownames from 1 to 980 to the new guns data frame

rownames(new_guns_urls_df) <- seq(1, 980, 1)

#Next I add the two row named data framed together by row.

hg_and_ng<-rbind(hot_guns_urls_df, new_guns_urls_df)

#After this I add rownames from 1 to 996 to the top guns data frame

rownames(top_guns_urls_df) <- seq(1, 996, 1)

# Finally I bind together the new and hot guns combined data frame with the 
# top guns data frame by row

whole_data_frame <- rbind(hg_and_ng, top_guns_urls_df)

```

Since all of the posts come from the same sub Reddit I have to ensure that all posts are unique as to avoid bias towards any particular set of words. This is because top posts, hot posts, and recent posts could all include the same post if it were to qualify for any of the categories by the reddit algorithm.

```{r}

set.seed(836)

# First I remove all non-unique rows from the data frame

whole_data_frame_unique <- unique(whole_data_frame)

# Next I add together the title and text of each reddit post to 
# ensure I am using all of the linguistic data in each post.

whole_data_frame_unique$all_text<-
  paste(whole_data_frame_unique$title, whole_data_frame_unique$text)

# Next I remove all line breaks from the posts to ensure aesthetic 
# specifications such as /r and /n are not included in the corpus

whole_data_frame_unique$"all_text" <- sapply(whole_data_frame_unique$"all_text",
                                    function(x) { gsub("[\r\n]", "", x)  }) 

whole_data_frame_unique$"all_text" <- sapply(whole_data_frame_unique$"all_text",
                                    function(x) { gsub("[\031\034\035\024]", "`", x) })

```

Next I must convert the character vector of dates into an actual date object using the as.Date() function.

```{r,cache = TRUE}

set.seed(836)

# As can be seen below I replace the previous date_utc column which was not 
# a date object with date_utc as a date object.

whole_data_frame_unique$date_utc <- as.Date(whole_data_frame_unique$date_utc)

```

```{r}
library(stm)
```

```{r}

set.seed(836)

myDfm <- dfm(whole_data_frame_unique$all_text,
  tolower=TRUE,
  remove = stopwords('en'), 
  remove_punct = TRUE
  )

dim(myDfm)

```

```{r}

library(topicmodels)

```

```{r}

set.seed(836)


# load stopwords
english_stopwords <- readLines("https://slcladal.github.io/resources/stopwords_en.txt", encoding = "UTF-8")

# create corpus object
textdata <- data.frame(doc_id=row.names(whole_data_frame_unique),
                       text=whole_data_frame_unique$all_text, date=whole_data_frame_unique$date_utc)

corpus <- VCorpus(DataframeSource(textdata))

# Preprocessing chain

processedCorpus_guns <- tm_map(corpus, content_transformer(tolower))

processedCorpus_guns <- tm_map(processedCorpus_guns, removeWords, english_stopwords)

processedCorpus_guns <- tm_map(processedCorpus_guns, removePunctuation)



```

```{r}

library(tokenizers)

```

```{r}

set.seed(836)

NLP_tokenizer <- function(x) {
  unlist(lapply(ngrams(words(x), 1:2), paste, collapse = "_"), use.names = FALSE)
}

DTM <- DocumentTermMatrix(
  processedCorpus_guns, 
  control = list(tokenize = NLP_tokenizer))

removeCommonTerms <- function (x, pct) 
{
    stopifnot(inherits(x, c("DocumentTermMatrix", "TermDocumentMatrix")), 
        is.numeric(pct), pct > 0, pct < 1)
    m <- if (inherits(x, "DocumentTermMatrix")) 
        t(x)
    else x
    t <- table(m$i) < m$ncol * (pct)
    termIndex <- as.numeric(names(t[t]))
    if (inherits(x, "DocumentTermMatrix")) 
        x[, termIndex]
    else x[termIndex, ]
}


DTM <- removeCommonTerms(DTM, 0.8)

```

```{r}

set.seed(836)

dim(DTM)

```

```{r}

set.seed(836)

sel_idx <- slam::row_sums(DTM) > 0
DTM <- DTM[sel_idx, ]

textdata <- textdata[sel_idx, ]

```

```{r}

set.seed(836)

result <- ldatuning::FindTopicsNumber(
  DTM,
  topics = seq(from = 2, to = 20, by = 1),
  metrics = c("CaoJuan2009",  "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  verbose = TRUE
)

```

```{r}
set.seed(836)


FindTopicsNumber_plot(result)

```

```{r}

set.seed(836)

# number of topics
K <- 10
# set random number generator seed

# compute the LDA model, inference via 1000 iterations of Gibbs sampling
topicModel <- LDA(DTM, K, method="Gibbs", control=list(iter = 500, verbose = 25))

```

```{r}

set.seed(836)

# have a look a some of the results (posterior distributions)
tmResult <- posterior(topicModel)
# format of the resulting object
attributes(tmResult)
```

```{r}

set.seed(836)


nTerms(DTM)              # lengthOfVocab

```

```{r,cache = TRUE}

set.seed(836)

# topics are probability distribtions over the entire vocabulary
beta <- tmResult$terms   # get beta from results
dim(beta)                # K distributions over nTerms(DTM) terms

```

```{r}

set.seed(836)

rowSums(beta) 
```

```{r}

set.seed(836)

nDocs(DTM)               # size of collection

```

```{r}

set.seed(836)

theta <- tmResult$topics 

dim(theta) 

```

```{r,cache = TRUE}

set.seed(836)

terms(topicModel, 10)

```




```{r}

set.seed(836)

top5termsPerTopic <- terms(topicModel, 5)

topicNames <- apply(top5termsPerTopic, 2, paste, collapse=" ")

```

The 4 topics of interest are 10, 4, 3, 1

```{r}
# visualize topics as word cloud

topicToViz <- 1 # change for your own topic of interest
topicToViz <- grep(topicNames[1] , topicNames)[1]
# Or select a topic by a term contained in its name
# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
words <- names(top40terms)
# extract the probabilites of each of the 40 terms
probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)
```

```{r}
# visualize topics as word cloud

topicToViz <- 3 # change for your own topic of interest
topicToViz <- grep(topicNames[3] , topicNames)[1] # Or select a topic by a term contained in its name
# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
words <- names(top40terms)
# extract the probabilites of each of the 40 terms
probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)
```

```{r}
# visualize topics as word cloud

topicToViz <- 4 # change for your own topic of interest
topicToViz <- grep(topicNames[4], topicNames)[1] # Or select a topic by a term contained in its name
# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
words <- names(top40terms)
# extract the probabilites of each of the 40 terms
probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
wordcloud(words, probabilities, random.order = FALSE, color = mycolors)
```

```{r}
# visualize topics as word cloud

topicToViz <- 10 # change for your own topic of interest
topicToViz <- grep(topicNames[10], topicNames)[1] # Or select a topic by a term contained in its name
# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
words <- names(top40terms)
# extract the probabilites of each of the 40 terms
probabilities <- sort(tmResult$terms[topicToViz,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
mycolors <- brewer.pal(8, "Dark2")
wordcloud(words, probabilities, random.order = TRUE, color = mycolors)

```

```{r}
set.seed(836)

topicNames <- apply(lda::top.topic.words(beta, 5, by.score = T), 2, paste, collapse = " ")
set.seed(836)

# What are the most probable topics in the entire collection?
topicProportions <- colSums(theta) / nDocs(DTM) 
# mean probablities over all paragraphs

names(topicProportions) <- topicNames     

# assign the topic names we created before

```

```{r}
set.seed(836)

soP <- sort(topicProportions, decreasing = TRUE)

paste(round(soP, 5), ":", names(soP))
```

```{r}

countsOfPrimaryTopics <- rep(0, K)
names(countsOfPrimaryTopics) <- topicNames
for (i in 1:nDocs(DTM)) {
  topicsPerDoc <- theta[i, ] # select topic distribution for document i
  # get first element position from ordered list
  primaryTopic <- order(topicsPerDoc, decreasing = TRUE)[1] 
  countsOfPrimaryTopics[primaryTopic] <- countsOfPrimaryTopics[primaryTopic] + 1
}
sort(countsOfPrimaryTopics, decreasing = TRUE)

```

```{r}
set.seed(836)

so <- sort(countsOfPrimaryTopics, decreasing = TRUE)

paste(so, ":", names(so))
```

```{r}
set.seed(291)
n <- 10
qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))
pie(rep(1,n), col=sample(col_vector, n))
```

```{r}
set.seed(1234)

n <- 4
qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]

col_vector_2 = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))

pie(rep(1,n), col=sample(col_vector_2, n))
```

```{r}

set.seed(1234)

df_for_visualization<-as.data.frame(countsOfPrimaryTopics)
names(so)<-c("Concealed Carry", "Purchase Advice", "Hunting","Range and Recreation", "Firearm Accessories", "Experience and Review", "Pistol Review", "Rifle Review", "Loading Issues",  "Home Defense")

df_for_visualization$group_names <- names(so)

df_for_visualization$group_numbers <- seq(1, 10, 1)

ggplot(df_for_visualization, aes(x=group_names, y=countsOfPrimaryTopics,  
                                 fill=group_names)) + 
  geom_bar(stat = "identity" )+  theme_classic() +
   theme(axis.text.x = element_text(angle = 90, hjust = 1, size=3),legend.key.size = unit(0.3, 'cm'), #change legend key size
        legend.key.height = unit(0.4, 'cm'), #change legend key height
        legend.key.width = unit(0.4, 'cm'),) +  
scale_fill_manual(values = sample(col_vector_2, 10))+ 
  xlab("Topic Groups") +
  ylab("Number of Posts in Group") + 
  guides(fill=guide_legend(title="Topic Groups"))  +
  coord_polar()+ theme_void()

```


```{r}
set.seed(1234)

df_for_visualization<-as.data.frame(countsOfPrimaryTopics)

df_for_visualization_relevant <- as.data.frame(df_for_visualization[c(1 ,4, 8,9),])

df_for_visualization_relevant$group_names <- names(so)[c(1 ,3, 4,10)]

df_for_visualization_relevant$group_names <-c("Concealed Carry", "Hunting", "Range and Recreation", "Home Defense")

df_for_visualization_relevant$group_numbers <- c(1 ,4, 8,9)

df_for_visualization_relevant$frequency <- df_for_visualization_relevant$`df_for_visualization[c(1, 4, 8, 9), ]`

df_for_visualization_relevant$proportion<-df_for_visualization_relevant$frequency/sum(df_for_visualization_relevant$frequency)

ggplot(df_for_visualization_relevant, aes(x=group_names, y=proportion,  fill=group_names)) + 
  geom_bar(stat = "identity" ) +    
  scale_fill_manual(values = sample(col_vector_2, 4)) + 
   theme(axis.text.x = element_text(size = 6))+
  scale_x_discrete(guide = guide_axis(n.dodge = 2)) + xlab("Topic Groups") +
  ylab("Number of Posts in Group") + guides(fill=guide_legend(title="Topic Groups"))+
  coord_polar()+
  theme_void()

```
```{r}

terms(topicModel, 10)

```

```{r}

set.seed(1234)

topic_proportion_per_decade <- aggregate(theta, by = list(decade = textdata$date), mean)

names(topic_proportion_per_decade)[names(topic_proportion_per_decade)==c("decade", "1", "2", "3","4","5","6","7","8","9","10")]<- c("day", "Concealed Carry", "Purchase Advice", "Hunting","Range and Recreation", "Firearm Accessories", "Experience and Review", "Pistol Review", "Rifle Review", "Loading Issues",  "Home Defense")
                                     #c("decade", "pistol carry buy state people","good question find guns 9mm","rifle ruger pretty bolt long", "range ammo time shooting shoot", "red sight found sights dot", "gun safe post feel dad", "glock amp love trigger great","barrel bought lower steel recently", "back round slide mag rounds","gun home guns defense firearm")] <- c("day", "Concealed Carry", "Purchase Advice", "Hunting","Range and Recreation", "Firearm Accessories", "Experience and Review", "Pistol Review", "Rifle Review", "Loading Issues",  "Home Defense")
 
topicNames <-c("Concealed Carry", "Purchase Advice", "Hunting","Range and Recreation", "Firearm Accessories", "Experience and Review", "Pistol Review", "Rifle Review", "Loading Issues",  "Home Defense")

colnames(topic_proportion_per_decade)[2:(K+1)] <- topicNames
# reshape data frame
vizDataFrame <- melt(topic_proportion_per_decade, id.vars = "day")
# plot topic proportions per decade as bar plot
ggplot(vizDataFrame, aes(x=day, y=value, fill=variable)) + 
  geom_bar(stat = "identity") + ylab("proportion") + 
  scale_fill_manual(values = sample(col_vector_2, 10), name = "day") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  theme_minimal()
```
```{r}


dat2 <- whole_data_frame_unique %>% 
  group_by(date_utc) %>% 
  summarise(n = n()) %>% 
  mutate(prop = n/lag(n))
#> `summarise()` ungrouping output (override with `.groups` argument)

ggplot(dat2, aes(x = date_utc, y = n)) +
  geom_col() +theme_minimal()

```

