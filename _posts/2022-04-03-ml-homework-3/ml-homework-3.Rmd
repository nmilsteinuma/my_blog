---
title: "ML homework 3"
description: |
  ISLR DACSS Machine Learning Homework 3.
author:
  - name: Noah Milstein
    url: {}
date: 2022-04-03
output:
  distill::distill_article:
    self_contained: false
---


```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(dplyr)

library(ISLR2)

library(tidyverse)

library(knitr)

library(randomForest)

library(tree)

library(glmnet)

library(boot)

library(markdown)


```

## Problem Set 3

### Ch. 6, Exercise 9

**Basis:**  In this exercise, we will predict the number of applications received using the other variables in the College data set. During my original calculations for homework 2 I did the prediction on the graduation rate rather than applications received so I will be modifying my calculations for parts a through e to reflect this and enable me to interpret part g correctly.

#### Part (a) 

**Question:** Split the data set into a training set and a test set.

**Answer:** I split the data into training and test set using 1 through the number of rows in the dataset and sampling it  nrow(College)*0.8 times which works out to taking 621.6 row samples with replacement equals FALSE, this equates to and 80%, 20% split in the training and test sets. I specify the college_training as the 80% of College data I specified as my training. Next I find the test set using the converse of the training rows.

```{r}

set.seed(9292)

train_College_numbers <- sample(1:nrow(College), nrow(College)*0.8,
                                replace = FALSE)

college_training <- College[train_College_numbers,]

college_test <- College[-train_College_numbers,]


```

#### Part (b) 

**Question:** Fit a linear model using least squares on the training set, and report the test error obtained.

**Fitting Linear Model on training set:** I fit a least squares on my 80% training set below using all variables since we will be using a ridge regression that later account for the number of variables we should use as the ridge moves their coefficients towards zero. In this first regression "least_squares_college" all will be included

```{r}

set.seed(9292)

least_squares_college <- glm(Apps ~. , data=college_training)

summary(least_squares_college)

```

**Obtaining Test MSE:** In order to obtain test error I use the predict function and the least_squares_college linear model in my previous code and apply it to my test data. I report test error as Mean Squared error which is calculated below as the difference between the true value of the number of applications minus the predicted number, squared. I obtain a result of 1,094,646 as my MSE.

```{r}

set.seed(9292)

pred_college_for_mse <- predict(least_squares_college, newdata=college_test)

MSE_college_test <- mean((college_test$Apps-pred_college_for_mse)^2)

MSE_college_test %>% kable()

```

#### Part (c) 

**Question:** Fit a ridge regression model on the training set, with lambda chosen by cross-validation. Report the test error obtained.

**Answer:** Below I make a grid of values that is a sequence of 10 to -2 of length 100 which goes from 10^10 to 10^(-2) and creates a set of selected values. In order to fit a ridge model using cross validation I split the model into training and test set. Using the glmnet() function I then plug in my x-ridge model matrix and the y-ridge being the true values from the graduation rate column of the college dataset. This results in a dimension of 18 by 100.

```{r}

set.seed(9292)

x_ridge <- model.matrix(Apps ~ ., College)[, -1] 

y_ridge <- College$Apps

grid <- 10^seq(10, -2, length = 100)

ridge.mod <- glmnet(x_ridge, y_ridge, alpha = 0, lambda = grid)

dim(coef(ridge.mod)) %>% kable()
 
```

**Answer:** Below I then split my data into an 80% 20% split between training and test sets. These values are stored in the train_ridge and test_ridge variables. 

```{r}

set.seed(9292)

train_ridge <- sample(1:nrow(x_ridge), nrow(x_ridge)*0.8 ) 

test_ridge <- (-train_ridge)

y.test_ridge <- y_ridge[test_ridge]

```

**Answer:** I then use the glmnet() function with an alpha of 0 which is a ridge penalty in this case. In this scenario the function is named ridge.mod, this is then put into predict() where it predicts on the test section of the data. The resulting mean squared error is 1,082,823 which is a 11,823 unit improvement over the original GLM with no variable coefficinets moved towards zero.

```{r}

set.seed(9292)

ridge.mod <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], 
                    alpha = 0, lambda = grid, thresh = 1e-12)

ridge.pred <- predict(ridge.mod, s = 4, newx = x_ridge[test_ridge,])

mean((ridge.pred - y.test_ridge)^2) %>% kable()

```

**Answer:** Afterwards we fit the model again using the glmnet() function ridge.mod using a new x from x_ridge and our x and y from training. We then access the 18 coefficients below, as can be seen the ridge moves the coefficients it decides have little impact towards zero.

```{r}

set.seed(9292)

ridge.pred <- predict(ridge.mod, s = 0, newx = x_ridge[test_ridge, ], 
                      exact = T, x = x_ridge[train_ridge, ], 
                      y = y_ridge[train_ridge])

predict(ridge.mod, s = 0, exact = T, type = "coefficients", 
        x = x_ridge[train_ridge, ], y = y_ridge[train_ridge])[1:18, ] %>% kable()
```

**Answer continued:** Next after beginning these tests we use cross validation to get the optimal tuning parameters from a number of repetitions using cv.glmnet() in this case it takes our training values and an alpha of zero to indicate it is a ridge and uses a number of values of lambda to determine which minimizes mean squared errors. This is plotted below:

```{r}

set.seed(9292)

cv.out <- cv.glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], alpha = 0) 

plot(cv.out)

```

**Answer continued:** The cv.out we calculated above outputs a 1se and min lambda for its mean square errors which I access below to determine the cross validated tuning parameter.

```{r}

set.seed(9292)

bestlam <- cv.out$lambda.min

bestlam %>% kable()

```

**Answer continued:** Finally we use out predict function to see what our mean squared error will be for our tuning parameters than minimizes MSE, in this case it is *924,234.4* as seen below compared to the grid values which yielded a *1,082,823* at best.

```{r}

set.seed(9292)

ridge.pred <- predict(cv.out, s = bestlam, newx = x_ridge[test_ridge,])

mean((ridge.pred - y.test_ridge)^2) %>% kable()

```

**Answer continued:** The coefficient outputs for the lambda that minimizes MSE are then expressed below. Here a number of coefficients remain in the function when predicting the number of applications received using a ridge regression.

```{r}
set.seed(9292)

out <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], alpha = 0)

predict(out, type = "coefficients", s = bestlam)[1:18, ] %>% kable()

```

#### Part (d) 

**Question:** Fit a lasso model on the training set, with lambda chosen by cross validation. Report the test error obtained, along with the number of non-zero coefficient estimates.

**Answer:** The process of creating the lasso is largely similar to that of the ridge though here our initial glmnet() takes an alpha of 1. The plot below this indicates that depending on our choice of tuning parameter lambda, certain coefficients will become zero.

```{r}
set.seed(9292)

lasso.mod <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], 
                    alpha = 1, lambda = grid)

plot(lasso.mod)

```


**Answer continued:**  We continue this analysis by cross validation using cv.glmnet() with an alpha of 1 to find the coefficients that again, minimize the MSE.

```{r}

set.seed(9292)

cv.out_2 <- cv.glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], alpha = 1) 

plot(cv.out_2)

```

**Answer continued:** After this cross validation has been calculated we then access the lambda that minimizes MSE. We then use the predict function to calculate predictions for our lasso using the lambda that minimizes MSE this gets a MSE of *1,082,717*.

```{r}

set.seed(9292)

bestlam <- cv.out_2$lambda.min

lasso.pred <- predict(cv.out_2, s = bestlam, newx = x_ridge[test_ridge,])

mean((lasso.pred - y.test_ridge)^2) %>% kable()

```

**Answer continued:** Finally we calculate and show the coefficients for our equation with the lasso reducing our number of parameters by moving coefficients towards zero that have low impacts.

```{r}

set.seed(9292)

out <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], 
              alpha = 1, lambda = grid)

lasso.coef <- predict(out, type = "coefficients", s = bestlam)[1:18, ]

lasso.coef %>% kable()


```

### Part (g) 

**Basis:** Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?

**Answer:** The results are relatively similar across the 3 models. Mean squared errors are close to a million with some methods improving by about 10,000 though still making quite high MSE. No method appears to significantly alter the results of the regression with lasso not appearing to move any coefficients towards zero and drop the impact of any particular variaable.

### Ch. 12, Exercise 8

**Basis:** In Section 12.2.3, a formula for calculating PVE was given in Equation 12.10. We also saw that the PVE can be obtained using the sdev output of the prcomp() function. On the USArrests data, calculate PVE in two ways:

#### Part (a) 

**Basis:** Using the sdev output of the prcomp() function, as was done in Section 12.2.3.

**Answer:** Using sdev and prcomp() on USA arrests on USArrests as in 12.2.3 we first start by using our principal component function prcomp(), inside of this we add USArrests and scale our variables for consistency. These princpal components are represented by pr.out

```{r} 

pr.out <- prcomp(USArrests, scale = TRUE)

pr.out 

```

**Answer 2:** Next we see 4 names in the output of our principal component function results, one of which is sdev which will need to calculate our PVE.

```{r} 

names(pr.out) %>% kable()

```

```{r}

dim(pr.out$x) %>% kable()

```

```{r}

biplot(pr.out, scale = 0)

```

**Answer 3:** We see our 4 sdev, or standard deviations for Murder, Assault, UrbanPop and Rape.

```{r}

pr.out$sdev %>% kable()

```

**Answer 4:** First we square our sdevs to acquire our variance.

```{r}

pr.var <- pr.out$sdev^2

pr.var %>% kable()

```

**Answer 5:** Next we divide these values of our variance by their sum in the denominator.

```{r}

pve <- pr.var / sum(pr.var)

pve %>% kable()

```

#### Part (b) 

**Basis:** By applying Equation 12.10 directly. That is, use the prcomp() function to compute the principal component loadings. Then, use those loadings in Equation 12.10 to obtain the PVE.

**Answer:** In order to obtain PVE directly we must first obtain our loadings using prcomp, which I caclualted earlier as pr.out <- prcomp(USArrests, scale = TRUE). The loading's are represented as the rotations of the principal component. I then multiply these by the matrix of scaled USArrests observations which have each state represented with each of the 4 variables, Using matrix multiplication on the USArrests data and loading's we have the first part of the top of 12.10, this is then squared and summed. This is then divided by the sum of the previously calculated squared matrix multipliation. This results in identical PVE's to those obtained by code alone.

```{r}

USArrests_scaled <- scale(USArrests)

#dim(USArrests_scaled)

loadings <- pr.out$rotation

# dim(loadings)


loadings_times_var <- (USArrests_scaled %*% loadings)

sum_squares <- apply(loadings_times_var^2, 2, sum)

sum_squares / sum(sum_squares)

```


### Ch. 12, Exercise 9

**Basis:** Consider the USArrests data. We will now perform hierarchical clustering on the states.

#### Part (a) 

**Basis:** Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.

**Answer:** Below I use hierarchical clustering on the US states, this is done using the dist() function between states in the USArrests matrix, by default this uses eucliean distance, no diagonal or upper portion of the data which would repeat the values below the matrix diagonal. Next I use hclust() which performs hierarchical clustering with complete linkage.

```{r}

# First dist() is used on USArrests to compute euclidean 
# distance between the observations in each state.

USArrests.dist <- dist(USArrests)

# Next I acquire the labels from the dendrogram graph using the labels
# From USArrests
USArrest.labs <- rownames(USArrests)

plot(hclust(USArrests.dist, method="complete"), xlab = "", 
     sub = "", ylab = "", main = "Complete Linkage")

```

#### Part (b) 

**Basis:** Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?

**Answer:** Below I have used the function cutree, which cuts a tree specified in an hclust() into a spcified number of groups, in this case 3. After running this function I manually extract the rows from the table that represent state membership into each group with having a 1, as opposed to a zero in that group representing that the observation belongs to it. 

```{r}


set.seed(8437)

# First I use clust() on our USArrests euclidean distance with complete linkage
# Next I use cutree() to cut the USArrests.dist data into 3 clusters.

hc.clusters <- cutree(hclust(USArrests.dist, method="complete"), 3)

# I then add these to a table with each observations repective label

table_with_3_clusters <- table(hc.clusters, USArrest.labs)

# I then take all 3 rows on their own

table_with_3_clusters_r1 <- table_with_3_clusters[1,]

table_with_3_clusters_r2 <- table_with_3_clusters[2,]

table_with_3_clusters_r3 <- table_with_3_clusters[3,]

# I finally take these rows and find when they are eqivalent to one since 
# each row is a group and 1 indicates the state belongs to it.

table_with_3_clusters_r1[table_with_3_clusters_r1 == 1] %>% kable()

table_with_3_clusters_r2[table_with_3_clusters_r2 == 1] %>% kable()

table_with_3_clusters_r3[table_with_3_clusters_r3 == 1] %>% kable()


```


**Answer 2:** I then plot the results below using a red line at a level which creates 3 groups.

```{r}

plot(hclust(USArrests.dist, method="complete"), labels = USArrest.labs) 
abline(h = 105, col = "red")
```

#### Part (c) 

**Basis:** Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.

**Answer:** In this problem I follow largely the same as before with the exception of the scaling of the USArrests data to have a mean of 0 and a standard deviation of 1. This is first done with the scale() function.

```{r}

set.seed(213)

# First I use the scale function to scale the observations to the std of 1 
# and mean of zero this is done with scale()

USArrests_scaled_c <- scale(USArrests)
 
# Next I compute the distance with dist() specifying that it will be using
# The euclidean method, though this is the default.

USArrests_scaled_c.dist <- dist(USArrests_scaled_c, method = "euclidean" )

# Next I find labels using the rownames from the dataset.

USArrests_scaled.labs <- rownames(USArrests_scaled_c)

# Finally I use the hclust() function to apply hierarchical clustering to the 
# data, in this case using complete clustering.

hclust_USArrests_scaled_c <- hclust(USArrests_scaled_c.dist, method="complete")

plot(hclust_USArrests_scaled_c, xlab = "", sub = "", ylab = "",
labels = USArrests_scaled.labs, main = "Complete Linkage")

```

#### (d) 

**Basis:** What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.

**Answer:** Scaling the variables breaks them into far more small clusters and likely distinguishes them more closely at more levels. This is likely as distinguishing between groups will be based on a common scale that reduces possible scale differences within the dataset. As a result of this I would argue that scaling is beneficial, though the data is per 100,000 residents so it is scaled to some extent, certain states have far higher density and population so the meaning of per-100,000 can have differing impact. Though it likely helps to have all variables on the same scale it is not as useful as it would be in a scenario with variables using a different basis of measurement from one another, such as total deaths and deaths per-100,000 residents, or percent of violent crimes in the state. I


### Ch. 12, Exercise 10

**Basis:** In this problem, you will generate simulated data, and then perform PCA and K-means clustering on the data.

#### Part (a) 

**Basis:** Generate a simulated data set with 20 observations in each of three classes (i.e. 60 observations total), and 50 variables.

Hint: There are a number of functions in R that you can use to generate data. One example is the rnorm() function; runif() is another option. Be sure to add a mean shift to the observations in each class so that there are three distinct classes.

**Answer:** In order to generate 20 observations of 3 classes I used the code and random generation functions below.

```{r}

set.seed(280)


# First I take an rnorm with 20 observation and 3 different means

# and 3 different standard deviations to observe the effect of this

# In generating principal components, I then use replicate to run this

# simulation for 50 variables so it is replacted 50 times

df_1_10 <- as.data.frame(replicate(50, rnorm(n=20, mean=0, sd = 2)))

colnames(df_1_10)[1] <- "V1"


# I repeat this again with different mean and sd

df_2_10<-as.data.frame(replicate(50, rnorm(n=20, mean=0.5, sd= 0.8)))

colnames(df_2_10)[1] <- "V1"

# I then combine data frams

df_2_10_combined <- rbind(df_1_10, df_2_10)

# I repeat this again with different mean and sd

df_3_10 <- as.data.frame(replicate(50, rnorm(n=20, mean=1)))

colnames(df_3_10)[1] <- "V1"

# All data frames are then combined, after which I label the rows as to 

# know which class they are in.

big_df_combined_10 <- rbind(df_2_10_combined, df_3_10)

big_df_combined_10$groups <- NA 

big_df_combined_10$groups[1:20] <- "1"

big_df_combined_10$groups[21:40] <- "2"

big_df_combined_10$groups[41:60] <- "3"

```


#### Part (b) 

**Basis:** Perform PCA on the 60 observations and plot the first two principal component score vectors. Use a different color to indicate the observations in each of the three classes. If the three classes appear separated in this plot, then continue on to part (c). If not, then return to part (a) and modify the simulation so that there is greater separation between the three classes. Do not continue to part (c) until the three classes show at least some separation in the first two principal component score vectors.

**Answer:** Next I perform PCA on the 60 randomly generated observations.

```{r}

# I perform principal components on all by column 51 as this column has the 

# Group numbers in it for later reference.

pca_10_b <- prcomp(big_df_combined_10[-51])

```

**Answer Continued:** Using the base R plot function below we see that the components have separated noticeably. The black points on the right have the highest standard deviation of 2 while the red in the middle have a smaller standard deviation of 0.8 while the green points on the far right have an standard deviation of 1.

```{r}

plot(pca_10_b$x[, 1:2], pch=17,  col = (big_df_combined_10$groups),
xlab = "P1", ylab = "P2")

```

#### Part (c) 

**Basis:** Perform K-means clustering of the observations with K = 3. How well do the clusters that you obtained in K-means clustering compare to the true class labels?

**Answer:** Though it is difficult to interpret in some ways due to class labels, it appears as if K-means put all of the correct observations in group 2 and 3 but mis-classified many in group 1, for this see it put 9 in the incorrect groups. This is understandable as this group was created in my simulated data using the highest variance so it is concieveable that if its standard deviation was lower the algorithm would classify it more accurately, however in this case it was often incorrect.

```{r}

set.seed(928)

km.out <- kmeans(big_df_combined_10[-51], centers=3) 

km.clusters <- km.out$cluster

table(km.clusters, big_df_combined_10$groups)

```

#### Part (d) 

**Basis:** Perform K-means clustering with K = 2. Describe your results. 

**Answer:** In this case using 2 cluster will force the clusters to go from 2 to 3 which we see in the km.out$cluster output being only groups one and two. I this case it assigns all points that would be in the third cluster to the first cluster which outputs results of a 3, 20, 20.

```{r}
set.seed(75903)

km.out <- kmeans(big_df_combined_10[-51], centers=2,  nstart = 20) 

km.clusters <- km.out$cluster

table(km.clusters, big_df_combined_10$groups)

```

#### (e) 

**Basis:**Now perform K-means clustering with K = 4, and describe your results.

**Answer:** Similarly to part d above the algorithm assigns points to a cluster that is not truly in the group assigning most points to what are truly groups 2 and 3 in the original km.clusters, however groups 1 and 3 in the new clusters take the remaining points and split them into 2 clusters where there was only truly one.

```{r}
set.seed(98367)

km.out <- kmeans(big_df_combined_10[-51], 4)

km.clusters <- km.out$cluster

table(km.clusters, big_df_combined_10$groups)

```

#### Part (f) 

**Basis:** Now perform K-means clustering with K = 3 on the first two principal component score vectors, rather than on the raw data. That is, perform K-means clustering on the 60 × 2 matrix of which the first column is the first principal component score vector, and the second column is the second principal component score vector. Comment on the results.

**Answer:** Using 3 clusters in this case we find that the k-means assigns all points correctly in groups 2 and 3 in the original dataset but has difficulty with group 1. This is likely becasuse the variance in this particular group is quite large having a standard deviation of 2 compared to 0.8 and 1. The groups also appear to be incorrectly labeled as in pervious cases as the k-means algorithm picks groups 1, 2, and 3 arbitrarily while we chose them manually when making the true groups.

```{r}

k.means.pcr <- kmeans(pca_10_b$x[,1:2], 3)

km.clusters <- k.means.pcr$cluster

set.seed(98367)

big_df_no_51<-big_df_combined_10[-51]

table(km.clusters, big_df_combined_10$groups)

```


#### Part (g)

**Basis:** Using the scale() function, perform K-means clustering with K = 3 on the data after scaling each variable to have standard deviation one. How do these results compare to those obtained in (b)? Explain.

**Answer:** The results in this case are similar to that of (b) in this case my first group has a large amount of variance which has resulted in a large number of observations being placed in the group nearest to it in terms of its mean. As a result the classifications rate appears to be similar to earlier problems and most of the missclasficiations from the first group on the left in b are also being attributed to the center group, and to a lesser extent, the third group.

```{r}

set.seed(756)

km.out_g <- kmeans(scale(big_df_combined_10[-51]), centers=3) 

km_g.clusters <- km.out_g$cluster 

table(km_g.clusters, big_df_combined_10$groups)

```

## Written Questions:

### Question 1:

**Question:** Based on reading for this class, what are the main risks of algorithmic models (machine learning) from a fairness and justice perspective? What forms of modeling/learning and/or sorts of data may pose particular risks of harm? What general strategies might be employed to mitigate these risks? Can you think of any methodological advances that might be pursued by computational and statistical researchers in order to help data analysts make "ethically derived" predictions?

**Answer:** Based on the readings for this class there are a great number of potential fairness and justice risks of the use of models in many areas of life. The primary ones are the reinforcement of systems that perpetuate inequality or direct harm to a group through self-reinforcing predictions and punitive or financial outcomes. As O'neal writes the collegiate arms race has inflated prices far beyond inflation while predicted outcomes of policing models ignore white collar crimes and encourage increased policing which leads to more punitive outcomes which have far reaching social and financial consequences. A number of these risks arise from the fact that machine learning models derive their data from a world that has been created on systems that are inherently unequal or were created in ways that were designed to keep certain groups unequal. The financial system for example, which is the subject of the article "Wells Fargo Rejected Half of Its Black Applicants in Mortgage refinancing Boom" by Shawn Donnan, et. al, sold mortgages to lower income people that were likely to become delinquent which resulted in the 2008 crash. Examples like Wells Fargo accepting less than half of their Black applicants are specific reflections of an incredibly inequitable system. This system will inevitably provide data to nearly all types of businesses which they will make decisions based on, and as a results, those algorithms will be polluted with the inequities within the data that comprises them. In this way the fairness and justice risks are the perpetuation of systems of inequality.

The algorithms that appear to carry the most risk are predictive algorithms in the financial and social spaces. Predicting issues such as recidivism and crime are inevitable going to reinforce systems that created the inequities that lead to higher crime. The structure of social society that results in data makes it impossible to separate algorithms from the assumptions and history of the society they predict and reflect. As a result mitigation of risk is difficult in modeling as algorithms are inherently reductive and based on the past. Because of this there are areas that they simply should not be used in, or that the results of the algorithms should be subject to far more questioning. 

I do not think there is a way to make an "ethically derived" prediction in a number of spaces. The assumptions and purposes of certain industries renders them incapable of creating ethical algorithms. For example the motives of the housing industry and financial industry will likely never be ethical, nor consumer good. Methodological advance could improve opaque nature of certain algorithms which can be "black boxes" whose predictions and algorithms can be nearly impossible to explain due to the complexity and inherent mathematical structure that they comprise. In addition modeling should be subject to a much higher degree of oversight and structural regulation, though this is unlikely to happen because proving inequities in algorithms that are complex enough is nearly impossible.

## Project Update

#### My Machine Learning Analysis so Far and Plans Moving Forward.


```{r, echo=FALSE}
library(readxl)
library(igraph)
library(statnet)
library(network)
library(tidyverse)
library(ggnetwork)
library(GGally)
library(ggplot2)
library(sna)
library(intergraph)
library(ggbiplot)
library(knitr)
library(GGally)
library(aRtsy)
```

```{r setup, include=FALSE}

wars_in_1000s <- read_excel("~/Desktop/Spring 2022/Networks/wars_in_1000s.xlsx")

wars_in_1000s$"Out-list" <- sapply(wars_in_1000s$"Out-list",
                                    function(x) { gsub("[\r\n]", "", x) })
wars_in_1000s$"In-list"<- sapply(wars_in_1000s$"In-list",
                                    function(x) { gsub("[\r\n]", "", x) })

wars_in_1100s <- read_excel("~/Desktop/Spring 2022/Networks/wars_in_1100s.xlsx")

wars_in_1100s$"Out" <- sapply(wars_in_1100s$"Out",
                                    function(x) { gsub("[\r\n]", "", x) })
wars_in_1100s$"In"<- sapply(wars_in_1100s$"In",
                                    function(x) { gsub("[\r\n]", "", x) })

wars_in_1200s <- read_excel("~/Desktop/Spring 2022/Networks/wars_in_1200s.xlsx")


wars_in_1200s$"Out" <- sapply(wars_in_1200s$"Out",
                                    function(x) { gsub("[\r\n]", "", x) })
wars_in_1200s$"In"<- sapply(wars_in_1200s$"In",  
                                    function(x) { gsub("[\r\n]", "", x) })
                            
```

#### Putting Network into Necessary Formats

```{r, echo=FALSE}

wars_in_1000s_edgelist <- as.matrix(wars_in_1000s)

wars_in_1000s_edgelist_network_edgelist <- graph.edgelist(wars_in_1000s_edgelist, directed=TRUE)

wars_in_1000s.ig<-graph_from_data_frame(wars_in_1000s)

wars_in_1000s_network <- asNetwork(wars_in_1000s.ig)

wars_in_1100s_edgelist <- as.matrix(wars_in_1100s)

wars_in_1100s_edgelist_network_edgelist <- graph.edgelist(wars_in_1100s_edgelist, directed=TRUE)

wars_in_1100s.ig<-graph_from_data_frame(wars_in_1100s)

wars_in_1100s_network <- asNetwork(wars_in_1100s.ig)

wars_in_1200s_edgelist <- as.matrix(wars_in_1200s)

wars_in_1200s_edgelist_network_edgelist <- graph.edgelist(wars_in_1200s_edgelist, directed=TRUE)

wars_in_1200s.ig<-graph_from_data_frame(wars_in_1200s)

wars_in_1200s_network <- asNetwork(wars_in_1200s.ig)

```

#### Adding Attributes

**Explanation:** My Project is an exploration of data sets involving conflicts between the year 1000 and 1200, I do a great deal of general network analysis that is not included here, including some hierarchical clustering. My primary research question is what network attributes and node attributes are most influential in state continuity. I plan to eventually add more node attributes but for the moment I have just a handful, those being state religion, centrality measures, and degree measures.

```{r,echo=FALSE}

wars_in_1000s.nodes.stat<-data.frame(name=wars_in_1000s_network%v%"vertex.names",
    totdegree=sna::degree(wars_in_1000s_network),
    indegree=sna::degree(wars_in_1000s_network, cmode="indegree"),
    outdegree=sna::degree(wars_in_1000s_network, cmode="outdegree"))

wars_in_1000s.nodes.stat$eigen<-evcent(wars_in_1000s_network)

wars_in_1000s.nodes.stat$close<-sna::closeness(wars_in_1000s_network, cmode="suminvdir")

wars_1000s<-as.matrix(as_adjacency_matrix(wars_in_1000s.ig))

#square the adjacency matrix

wars_1000s_sq<-t(wars_1000s) %*% wars_1000s

#Calculate the proportion of reflected centrality.

wars_in_1000s.nodes.stat$rc<-diag(wars_1000s_sq)/rowSums(wars_1000s_sq)

#replace missing values with 0

wars_in_1000s.nodes.stat$rc<-ifelse(is.nan(wars_in_1000s.nodes.stat$rc),0,wars_in_1000s.nodes.stat$rc)

#Calculate received eigenvalue centrality
wars_in_1000s.nodes.stat$eigen.rc<-wars_in_1000s.nodes.stat$eigen*wars_in_1000s.nodes.stat$rc

#Calculate the proportion of derived centrality.
wars_in_1000s.nodes.stat$dc<-1-diag(wars_1000s_sq)/rowSums(wars_1000s_sq)
#replace missing values with 0
wars_in_1000s.nodes.stat$dc<-ifelse(is.nan(wars_in_1000s.nodes.stat$dc),1,wars_in_1000s.nodes.stat$dc)
#Calculate received eigenvalue centrality
wars_in_1000s.nodes.stat$eigen.dc<-wars_in_1000s.nodes.stat$eigen*wars_in_1000s.nodes.stat$dc

```

#### Brokerage scores in the 1000s

```{r,echo=FALSE}

temp<-data.frame(brokerage(wars_in_1000s_network, cl = wars_in_1000s.nodes.stat$totdegree)$z.nli)

wars_in_1000s.nodes.stat_2<-wars_in_1000s.nodes.stat %>%
  mutate(broker.tot = temp$t,
         broker.coord = temp$w_I,
         broker.itin = temp$w_O,
         broker.rep = temp$b_IO,
         broker.gate = temp$b_OI,
         broker.lia = temp$b_O)

```


```{r, echo=FALSE}

wars_in_1100s.nodes.stat<-data.frame(name=wars_in_1100s_network%v%"vertex.names",
    totdegree=sna::degree(wars_in_1100s_network),
    indegree=sna::degree(wars_in_1100s_network, cmode="indegree"),
    outdegree=sna::degree(wars_in_1100s_network, cmode="outdegree"))

wars_in_1100s.nodes.stat$eigen<-evcent(wars_in_1100s_network)

wars_1100s<-as.matrix(as_adjacency_matrix(wars_in_1100s.ig))

#square the adjacency matrix

wars_1100s_sq<-t(wars_1100s) %*% wars_1100s

#Calculate the proportion of reflected centrality.

wars_in_1100s.nodes.stat$rc<-diag(wars_1100s_sq)/rowSums(wars_1100s_sq)

#replace missing values with 0

wars_in_1100s.nodes.stat$rc<-ifelse(is.nan(wars_in_1100s.nodes.stat$rc),
                                    0,wars_in_1100s.nodes.stat$rc)

#Calculate received eigenvalue centrality
wars_in_1100s.nodes.stat$eigen.rc<-
  wars_in_1100s.nodes.stat$eigen*wars_in_1100s.nodes.stat$rc

#Calculate the proportion of derived centrality.
wars_in_1100s.nodes.stat$dc<-1-diag(wars_1100s_sq)/rowSums(wars_1100s_sq)
#replace missing values with 0
wars_in_1100s.nodes.stat$dc<-ifelse(is.nan(wars_in_1100s.nodes.stat$dc),
                                    1,wars_in_1100s.nodes.stat$dc)
#Calculate received eigenvalue centrality
wars_in_1100s.nodes.stat$eigen.dc<-
  wars_in_1100s.nodes.stat$eigen*wars_in_1100s.nodes.stat$dc

```

#### Brokerage scores in the 1100s

```{r, echo=FALSE}

temp<-data.frame(brokerage(wars_in_1100s_network, 
                           cl = wars_in_1100s.nodes.stat$totdegree)$z.nli)

wars_in_1100s.nodes.stat_2<-wars_in_1100s.nodes.stat %>%
  mutate(broker.tot = temp$t,
         broker.coord = temp$w_I,
         broker.itin = temp$w_O,
         broker.rep = temp$b_IO,
         broker.gate = temp$b_OI,
         broker.lia = temp$b_O)

```

#### Brokerage scores in the 1200s

```{r, echo=FALSE}

wars_in_1200s.nodes.stat<-
  data.frame(name=wars_in_1200s_network%v%"vertex.names",
    totdegree=sna::degree(wars_in_1200s_network),
    indegree=sna::degree(wars_in_1200s_network, cmode="indegree"),
    outdegree=sna::degree(wars_in_1200s_network, cmode="outdegree"))

wars_in_1200s.nodes.stat$eigen<-evcent(wars_in_1200s_network)

```

```{r, echo=FALSE}

wars_1200s<-as.matrix(as_adjacency_matrix(wars_in_1200s.ig))

#square the adjacency matrix

wars_1200s_sq<-t(wars_1200s) %*% wars_1200s

#Calculate the proportion of reflected centrality.

wars_in_1200s.nodes.stat$rc<-diag(wars_1200s_sq)/rowSums(wars_1200s_sq)

#replace missing values with 0

wars_in_1200s.nodes.stat$rc<-ifelse(is.nan(wars_in_1200s.nodes.stat$rc),
                                    0,wars_in_1200s.nodes.stat$rc)

#Calculate received eigenvalue centrality
wars_in_1200s.nodes.stat$eigen.rc<-
  wars_in_1200s.nodes.stat$eigen*wars_in_1200s.nodes.stat$rc

#Calculate the proportion of derived centrality.
wars_in_1200s.nodes.stat$dc<-1-diag(wars_1200s_sq)/rowSums(wars_1200s_sq)
#replace missing values with 0
wars_in_1200s.nodes.stat$dc<-ifelse(is.nan(wars_in_1200s.nodes.stat$dc),
                                    1,wars_in_1200s.nodes.stat$dc)
#Calculate received eigenvalue centrality
wars_in_1200s.nodes.stat$eigen.dc<-
  wars_in_1200s.nodes.stat$eigen*wars_in_1200s.nodes.stat$dc

```

```{r, echo=FALSE}

temp<-data.frame(brokerage(wars_in_1200s_network, 
                           cl = wars_in_1200s.nodes.stat$totdegree)$z.nli)

wars_in_1200s.nodes.stat_2<-wars_in_1200s.nodes.stat %>%
  mutate(broker.tot = temp$t,
         broker.coord = temp$w_I,
         broker.itin = temp$w_O,
         broker.rep = temp$b_IO,
         broker.gate = temp$b_OI,
         broker.lia = temp$b_O)

```


```{r}

wars_in_1000s_edgelist <- as.matrix(wars_in_1000s)

wars_in_1000s_edgelist_network_edgelist <- 
  graph.edgelist(wars_in_1000s_edgelist, directed=TRUE)

wars_in_1000s.ig<-graph_from_data_frame(wars_in_1000s)

wars_in_1000s_network <- asNetwork(wars_in_1000s.ig)

```

```{r}


aspects_of_1000s_states <-
  read_excel("~/Desktop/Spring 2022/Networks/aspects_of_1000s_states.xlsx")

total_1000s <- merge(aspects_of_1000s_states,
                     wars_in_1000s.nodes.stat_2, by="name")

```

```{r}

aspects_of_1100s_states <- 
  read_excel("~/Desktop/Spring 2022/Networks/aspects_of_1100s_states.xlsx")

total_1100s <- merge(aspects_of_1100s_states, 
                     wars_in_1100s.nodes.stat_2, by="name")

```

```{r}

aspects_of_1200s_states <- 
  read_excel("~/Desktop/Spring 2022/Networks/aspects_of_1200s_states.xlsx")

total_1200s <- merge(aspects_of_1200s_states, wars_in_1200s.nodes.stat_2, 
                     by="name")

```

```{r}

total_1000s_brokerag_reg<-total_1000s

total_1000s_brokerag_reg$win_rate <- (total_1000s_brokerag_reg$outdegree/total_1000s_brokerag_reg$totdegree)

total_1000s_brokerag_reg$loss_rate <- (total_1000s_brokerag_reg$indegree/total_1000s_brokerag_reg$totdegree)

total_1000s_brokerag_reg_binom <- 
  total_1000s_brokerag_reg %>% mutate(more_win_or_loss = case_when(
  win_rate < 0.5 ~ 0,
    win_rate >= 0.5 ~ 1))

First_1000s_regression <- 
  glm(more_win_or_loss~.
      -name-totdegree-indegree-outdegree-dc-eigen.dc-win_rate-loss_rate, total_1000s_brokerag_reg_binom, family=binomial)

First_1000s_regression


```

```{r}

set.seed(292)

total_1000s_for_regression <- total_1000s[,-c(1, 20:25)]

total_1000s_for_regression$win_rate <- (total_1000s_for_regression$outdegree/total_1000s_for_regression$totdegree)

total_1000s_for_regression$loss_rate <- (total_1000s_for_regression$indegree/total_1000s_for_regression$totdegree)

total_1000s_for_regression <- 
  total_1000s_for_regression %>% mutate(more_win_or_loss = case_when(
  win_rate < 0.5 ~ 0,
    win_rate >= 0.5 ~ 1))

First_1000s_regression <- glm(more_win_or_loss~.
                  -loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, total_1000s_for_regression, family=binomial)

First_1000s_regression

```

```{r}

set.seed(6738)

in_training<- sample(1:nrow(total_1000s_for_regression),  
                     nrow(total_1000s_for_regression) * 0.7 )

training_1000s <- total_1000s_for_regression[in_training,]

test_1000s <- total_1000s_for_regression[-in_training,]

lm_1000s_binom_subset_1 <- glm(more_win_or_loss~.
            -loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, total_1000s_for_regression, family=binomial, subset = in_training )

logsitic_1_1000s_prob <- predict(lm_1000s_binom_subset_1, test_1000s,
type = "response")

log_preds_1<-ifelse(logsitic_1_1000s_prob >= 0.5, 1, 0)

prediction_1_logs <-mean(log_preds_1 == test_1000s$more_win_or_loss)

prediction_1_logs %>% kable()

```

```{r}

library(glmnet)
library(MASS)
```

```{r}


set.seed(246)

x_ridge <- model.matrix(more_win_or_loss ~ .-loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, 
                        total_1000s_for_regression)[, -1] 

y_ridge <- total_1000s_for_regression$more_win_or_loss

grid <- 10^seq(10, -2, length = 100)

ridge.mod <- glmnet(x_ridge, y_ridge, alpha = 0, lambda = grid)

dim(coef(ridge.mod))



```

```{r}
set.seed(729)
train_ridge <- sample(1:nrow(x_ridge), nrow(x_ridge)*0.8 ) 

test_ridge <- (-train_ridge)

y.test_ridge <- y_ridge[test_ridge]

```

```{r}
set.seed(9292)

ridge.mod <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], 
                    alpha = 0, lambda = grid, thresh = 1e-12)

ridge.pred <- predict(ridge.mod, s = 4, newx = x_ridge[test_ridge,])

mean((ridge.pred - y.test_ridge)^2) %>% kable()
```

```{r}
set.seed(231)
ridge.pred <- predict(ridge.mod, s = 0, newx = x_ridge[test_ridge, ], 
                      exact = T, x = x_ridge[train_ridge, ], 
                      y = y_ridge[train_ridge])

predict(ridge.mod, s = 0, exact = T, type = "coefficients", 
        x = x_ridge[train_ridge, ], y = y_ridge[train_ridge])[1:14, ]

```

```{r}

set.seed(9292)

cv.out <- cv.glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], alpha = 0) 

plot(cv.out)

```

```{r}
set.seed(9292)

bestlam <- cv.out$lambda.min

bestlam
```

```{r}
set.seed(9292)

ridge.pred <- predict(cv.out, s = bestlam, newx = x_ridge[test_ridge,])

mean((ridge.pred - y.test_ridge)^2) %>% kable()
```

```{r}

set.seed(2897)

x_lasso <- model.matrix(more_win_or_loss ~ .-loss_rate-win_rate-totdegree-indegree-outdegree-dc-eigen.dc, 
                        total_1000s_for_regression)[, -1] 

y_lasso <- total_1000s_for_regression$more_win_or_loss

grid <- 10^seq(10, -2, length = 100)

lasso.mod <- glmnet(x_lasso, y_lasso, alpha = 0, lambda = grid)

dim(coef(lasso.mod))



```

```{r}

set.seed(729)

train_lasso <- sample(1:nrow(x_ridge), nrow(x_ridge)*0.8 ) 

test_lasso <- (-train_lasso)

y.test_lasso <- y_lasso[test_lasso]
```

```{r}
set.seed(9292)

lasso.mod <- glmnet(x_lasso[train_lasso, ], y_lasso[train_lasso], 
                    alpha = 1, lambda = grid)

plot(lasso.mod)
```

```{r}

set.seed(1029)

cv.out_2 <- cv.glmnet(x_lasso[train_lasso, ], y_lasso[train_lasso], alpha = 1) 

plot(cv.out_2)

```

```{r}
set.seed(1920)

bestlam_2 <- cv.out_2$lambda.min

lasso.pred <- predict(cv.out_2, s = bestlam_2, newx = x_ridge[test_ridge,])

mean((lasso.pred - y.test_ridge)^2) %>% kable()

```

```{r}

set.seed(2739)

out <- glmnet(x_lasso[train_lasso, ], y_lasso[train_lasso], 
              alpha = 1, lambda = grid)

lasso.coef <- predict(out, type = "coefficients", s = bestlam_2)[1:14, ]

lasso.coef

```


### Community Grouping

#### Label Propagation 1000s:

The first community cluster below is done using label propagation. This results in 39 groups

```{r}
set.seed(23)
comm.lab<-label.propagation.community(wars_in_1000s.ig)
#Inspect clustering object
# igraph::groups(comm.lab)

```

```{r, echo=FALSE, warning=FALSE}

set.seed(123)

plot(comm.lab,wars_in_1000s.ig, vertex.size=0.5, edge.size=0.25, 
     edge.arrow.size=.25,  vertex.label.dist=1.5,vertex.label.cex=0.25,asp = 0)


```

#### Walktrap 1000s:

Walktrap classification as seen below results in 19 distinct communities.

```{r}

set.seed(238)
#Run clustering algorithm: fast_greedy
wars_in_1000s.wt<-walktrap.community(wars_in_1000s.ig)

#igraph::groups(wars_in_1000s.wt)

```

Adding more steps resulted in 19 groups for both 10 and 20 steps.

```{r}
#Run & inspect clustering algorithm: 10 steps
#igraph::groups(walktrap.community(wars_in_1000s.ig, steps=10)) 
#Run & inspect clustering algorithm: 20 steps
#igraph::groups(walktrap.community(wars_in_1000s.ig ,steps=20))
#Run & inspect clustering algorithm
```

```{r, echo=FALSE}

plot(wars_in_1000s.wt, wars_in_1000s.ig, vertex.size=0.5, 
     edge.size=0.25, edge.arrow.size=.25, 
     vertex.label.dist=1.5,vertex.label.cex=0.25,asp = 0)


```

### Machine Learning, Regression and Principle Components:

#### Principal Components 1000s

```{r}

total_1000s_for_PCA <- total_1000s_brokerag_reg_binom[-c(20:27)]

apply(total_1000s_for_PCA[-1], 2, mean)

```

```{r}

apply(total_1000s_for_PCA[-1], 2, var)

```

```{r}

pr.out <- prcomp(total_1000s_for_PCA[-1], scale = TRUE)

```

```{r}
names(pr.out)
```

```{r}
pr.out$center
```

```{r}

pr.out$scale

```

```{r, results=FALSE, echo=FALSE}

pr.out$rotation

```

```{r}
biplot(pr.out, scale = 0)
```

```{r}

set.seed(172)

ggbiplot(pr.out, labels =  total_1000s_for_PCA$name, labels.size  =1.5) + 
  labs(title="Principal Components 1000s")+ theme_minimal()

```


```{r}

pr.out$rotation = -pr.out$rotation 

pr.out$x = -pr.out$x

biplot(pr.out, scale = 0)

```

```{r}

pr.out$sdev

```

```{r}

pr.var <- pr.out$sdev^2

pr.var

```

```{r}

pve <- pr.var / sum(pr.var)

pve

```

```{r}

par(mfrow = c(1, 2))
plot(pve, xlab = "Principal Component",
ylab = "Proportion of Variance Explained", ylim = c(0, 1),
type = "b")

plot(cumsum(pve), xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained", 
ylim = c(0, 1), type = "b")

```

#### Label Propagation 1100s:

The first community cluster below is done using label propagation. This results in 39 groups

```{r}
set.seed(23)
comm.lab<-label.propagation.community(wars_in_1100s.ig)
#Inspect clustering object
# igraph::groups(comm.lab)

```

```{r, echo=FALSE, warning=FALSE}

set.seed(123)

plot(comm.lab,wars_in_1100s.ig, vertex.size=0.5, edge.size=0.25, 
     edge.arrow.size=.25,  vertex.label.dist=1.5,vertex.label.cex=0.25,asp = 0)


```

#### Walktrap 1000s:

Walktrap classification as seen below results in 19 distinct communities.

```{r}

set.seed(238)
#Run clustering algorithm: fast_greedy
wars_in_1100s.wt<-walktrap.community(wars_in_1100s.ig)

#igraph::groups(wars_in_1000s.wt)

```

Adding more steps resulted in 19 groups for both 10 and 20 steps.

```{r}
#Run & inspect clustering algorithm: 10 steps
#igraph::groups(walktrap.community(wars_in_1000s.ig, steps=10)) 
#Run & inspect clustering algorithm: 20 steps
#igraph::groups(walktrap.community(wars_in_1000s.ig ,steps=20))
#Run & inspect clustering algorithm
```

```{r, echo=FALSE}

plot(wars_in_1100s.wt, wars_in_1100s.ig, vertex.size=0.5, edge.size=0.25, 
     edge.arrow.size=.25,  vertex.label.dist=1.5,vertex.label.cex=0.25,asp = 0)


```

#### Principal Components 1200s

```{r}

names(total_1200s)

```

```{r}

total_1200s_brokerag_reg<-total_1200s

```

```{r}
total_1200s_brokerag_reg$win_rate <- (total_1200s_brokerag_reg$outdegree/total_1200s_brokerag_reg$totdegree)
```

```{r}
total_1200s_brokerag_reg$loss_rate <- (total_1200s_brokerag_reg$indegree/total_1200s_brokerag_reg$totdegree)
```

```{r}

total_1200s_brokerag_reg_binom <-
  total_1200s_brokerag_reg %>% mutate(more_win_or_loss = case_when(
  win_rate < 0.5 ~ 0,
    win_rate >= 0.5 ~ 1))

```

```{r}

total_1200s_for_PCA <- total_1200s_brokerag_reg_binom[-c(20:27)]


apply(total_1200s_for_PCA[-1], 2, mean)

```

```{r}

apply(total_1200s_for_PCA[-1], 2, var)

```

```{r}

# I cannot scale variables with 

total_1200s_for_PCA<-total_1200s_for_PCA[-c(8,10)]

```

```{r}

pr.out_2 <- prcomp(total_1200s_for_PCA[-1], scale = TRUE)

```

```{r}
names(pr.out_2)
```

```{r}
pr.out_2$center
```

```{r}

pr.out_2$scale

```

```{r, results=FALSE, echo=FALSE}
pr.out_2$rotation
```

```{r}
biplot(pr.out_2, scale = 0)
```

```{r}

pr.out_2$rotation = -pr.out_2$rotation 

pr.out_2$x = -pr.out_2$x

biplot(pr.out_2, scale = 0)

```
```{r}

set.seed(8192)

ggbiplot(pr.out_2, labels =  total_1200s_for_PCA$name, 
         labels.size  =1.5) + labs(title="Principal Components 1200s")+
  theme_minimal()

```

```{r}

pr.out$sdev

```

```{r}

pr.var_2 <- pr.out_2$sdev^2

pr.var_2

```

```{r}

pve_2 <- pr.var_2 / sum(pr.var_2)

pve_2

```

```{r}

par(mfrow = c(1, 2))
plot(pve_2, xlab = "Principal Component",
ylab = "Proportion of Variance Explained", ylim = c(0, 1),
type = "b")

plot(cumsum(pve_2), xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained", 
ylim = c(0, 1), type = "b")

```

#### Label Propagation 1200s:

The first community cluster below is done using label propagation. This results in 39 groups

```{r}
set.seed(23)
comm.lab<-label.propagation.community(wars_in_1200s.ig)
#Inspect clustering object
# igraph::groups(comm.lab)

```

```{r, echo=FALSE, warning=FALSE}

set.seed(123)

plot(comm.lab,wars_in_1200s.ig, vertex.size=0.5, edge.size=0.25, 
     edge.arrow.size=.25,  vertex.label.dist=1.5,vertex.label.cex=0.25,asp = 0)

```

#### Walktrap 1200s:

Walktrap classification as seen below results in 19 distinct communities.

```{r}

set.seed(238)
#Run clustering algorithm: fast_greedy
wars_in_1200s.wt<-walktrap.community(wars_in_1200s.ig)

#igraph::groups(wars_in_1000s.wt)

```

Adding more steps resulted in 19 groups for both 10 and 20 steps.

```{r}
#Run & inspect clustering algorithm: 10 steps
#igraph::groups(walktrap.community(wars_in_1000s.ig, steps=10)) 
#Run & inspect clustering algorithm: 20 steps
#igraph::groups(walktrap.community(wars_in_1000s.ig ,steps=20))
#Run & inspect clustering algorithm
```

```{r, echo=FALSE}

plot(wars_in_1200s.wt, wars_in_1200s.ig, vertex.size=0.5, 
     edge.size=0.25, edge.arrow.size=.25,  
     vertex.label.dist=1.5,vertex.label.cex=0.25,asp = 0)


```

#### Principal Components 1100s

```{r}

names(total_1100s)

```

```{r}

total_1100s_brokerag_reg<-total_1100s

```

```{r}
total_1100s_brokerag_reg$win_rate <- (total_1100s_brokerag_reg$outdegree/total_1100s_brokerag_reg$totdegree)
```

```{r}
total_1100s_brokerag_reg$loss_rate <- (total_1100s_brokerag_reg$indegree/total_1100s_brokerag_reg$totdegree)
```

```{r}

total_1100s_brokerag_reg_binom <- 
  total_1100s_brokerag_reg %>% mutate(more_win_or_loss = case_when(
  win_rate < 0.5 ~ 0,
    win_rate >= 0.5 ~ 1))

```

```{r}

total_1100s_for_PCA <- total_1100s_brokerag_reg_binom[-c(20:27)]


apply(total_1100s_for_PCA[-1], 2, mean)

```

```{r}

apply(total_1100s_for_PCA[-1], 2, var)

```

```{r}

# I cannot scale variables with 

total_1100s_for_PCA<-total_1100s_for_PCA[-c(8,10)]

```

```{r}

pr.out_2 <- prcomp(total_1100s_for_PCA[-c(1,7)], scale = TRUE)

```

```{r}
names(pr.out_2)
```

```{r}
pr.out_2$center
```

```{r}

pr.out_2$scale

```

```{r, results=FALSE, echo=FALSE}
pr.out_2$rotation
```

```{r}
biplot(pr.out_2, scale = 0)
```

```{r}

pr.out_2$rotation = -pr.out_2$rotation 

pr.out_2$x = -pr.out_2$x

biplot(pr.out_2, scale = 0)

```
```{r}

set.seed(8192)

ggbiplot(pr.out_2, labels =  
           total_1100s_for_PCA$name, labels.size  =1.5) +
  labs(title="Principal Components 1100s")+
  theme_minimal()

```

```{r}

pr.out$sdev

```

```{r}

pr.var_2 <- pr.out_2$sdev^2

pr.var_2

```

```{r}

pve_2 <- pr.var_2 / sum(pr.var_2)

pve_2

```

```{r}

par(mfrow = c(1, 2))
plot(pve_2, xlab = "Principal Component",
ylab = "Proportion of Variance Explained", ylim = c(0, 1),
type = "b")

plot(cumsum(pve_2), xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained", 
ylim = c(0, 1), type = "b")

```
