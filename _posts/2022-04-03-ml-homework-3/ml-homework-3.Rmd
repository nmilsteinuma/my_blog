---
title: "ML homework 3"
description: |
  A short description of the post.
author:
  - name: Noah Milstein
    url: {}
date: 2022-04-03
output:
  distill::distill_article:
    self_contained: false
---



ISLR Ch. 12, Exercise 9

ISLR Ch. 12, Exercise 10

```{r, echo=FALSE, message=FALSE, warning=FALSE}

library(dplyr)

library(ISLR2)

library(tidyverse)

library(knitr)

library(randomForest)

library(tree)

library(glmnet)

library(boot)

library(markdown)


```

## Problem Set 3

### Ch. 6

**Basis:**  In this exercise, we will predict the number of applications received using the other variables in the College data set. During my original calculations for homework 2 I did the prediction on the graduation rate rather than applications received so I will be modifying my calculations for parts a through e to reflect this and enable me to interpret part g correctly.

#### (a) 

**Question:** Split the data set into a training set and a test set.

**Answer:** I split the data into training and test set using 1 through the number of rows in the dataset and sampling it  nrow(College)*0.8 times which works out to taking 621.6 row samples with replacement equals FALSE, this equates to and 80%, 20% split in the training and test sets. I specify the college_training as the 80% of College data I specified as my training. Next I find the test set using the converse of the training rows.

```{r}

set.seed(9292)

train_College_numbers <- sample(1:nrow(College), nrow(College)*0.8,
                                replace = FALSE)

college_training <- College[train_College_numbers,]

college_test <- College[-train_College_numbers,]


```

#### (b) 

**Question:** Fit a linear model using least squares on the training set, and report the test error obtained.

**Fitting Linear Model on training set:** I fit a least squares on my 80% training set below using all variables since we will be using a ridge regression that later account for the number of variables we should use as the ridge moves their coefficients towards zero. In this first regression "least_squares_college" all will be included

```{r}

set.seed(9292)

least_squares_college <- glm(Apps ~. , data=college_training)

summary(least_squares_college)

```

**Obtaining Test MSE:** In order to obtain test error I use the predict function and the least_squares_college linear model in my previous code and apply it to my test data. I report test error as Mean Squared error which is calculated below as the difference between the true value of the number of applications minus the predicted number, squared. I obtain a result of 1,094,646 as my MSE.

```{r}

set.seed(9292)

pred_college_for_mse <- predict(least_squares_college, newdata=college_test)

MSE_college_test <- mean((college_test$Apps-pred_college_for_mse)^2)

MSE_college_test %>% kable()

```

#### (c) 

**Question:** Fit a ridge regression model on the training set, with lambda chosen by cross-validation. Report the test error obtained.

**Answer:** Below I make a grid of values that is a sequence of 10 to -2 of length 100 which goes from 10^10 to 10^(-2) and creates a set of selected values. In order to fit a ridge model using cross validation I split the model into training and test set. Using the glmnet() function I then plug in my x-ridge model matrix and the y-ridge being the true values from the graduation rate column of the college dataset. This results in a dimension of 18 by 100.

```{r}

set.seed(9292)

x_ridge <- model.matrix(Apps ~ ., College)[, -1] 

y_ridge <- College$Apps

grid <- 10^seq(10, -2, length = 100)

ridge.mod <- glmnet(x_ridge, y_ridge, alpha = 0, lambda = grid)

dim(coef(ridge.mod))

```

**Answer:** Below I then split my data into an 80% 20% split between training and test sets. These values are stored in the train_ridge and test_ridge variables. 

```{r}

set.seed(9292)

train_ridge <- sample(1:nrow(x_ridge), nrow(x_ridge)*0.8 ) 

test_ridge <- (-train_ridge)

y.test_ridge <- y_ridge[test_ridge]

```

**Answer:** I then use the glmnet() function with an alpha of 0 which is a ridge penalty in this case. In this scenario the function is named ridge.mod, this is then put into predict() where it predicts on the test section of the data. The resulting mean squared error is 1,082,823 which is a 11,823 unit improvement over the original GLM with no variable coefficinets moved towards zero.

```{r}

set.seed(9292)

ridge.mod <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], 
                    alpha = 0, lambda = grid, thresh = 1e-12)

ridge.pred <- predict(ridge.mod, s = 4, newx = x_ridge[test_ridge,])

mean((ridge.pred - y.test_ridge)^2) %>% kable()

```

**Answer:** Afterwards we fit the model again using the glmnet() function ridge.mod using a new x from x_ridge and our x and y from training. We then access the 18 coefficients below, as can be seen the ridge moves the coefficients it decides have little impact towards zero.

```{r}

set.seed(9292)

ridge.pred <- predict(ridge.mod, s = 0, newx = x_ridge[test_ridge, ], 
                      exact = T, x = x_ridge[train_ridge, ], y = y_ridge[train_ridge])

predict(ridge.mod, s = 0, exact = T, type = "coefficients", 
        x = x_ridge[train_ridge, ], y = y_ridge[train_ridge])[1:18, ]

```

**Answer continued:** Next after beginning these tests we use cross validation to get the optimal tuning parameters from a number of repetitions using cv.glmnet() in this case it takes our training values and an alpha of zero to indicate it is a ridge and uses a number of values of lambda to determine which minimizes mean squared errors. This is plotted below

```{r}

set.seed(9292)

cv.out <- cv.glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], alpha = 0) 

plot(cv.out)

```

**Answer continued:** The cv.out we calculated above outputs a 1se and min lambda for its mean square errors which I access below to determine the cross validated tuning parameter.

```{r}

set.seed(9292)

bestlam <- cv.out$lambda.min

bestlam

```

**Answer continued:** Finally we use out predict function to see what our mean squared error will be for our tuning parameters than minimizes MSE, in this case it is *924,234.4* as seen below compared to the grid values which yielded a *1,082,823* at best.

```{r}

set.seed(9292)

ridge.pred <- predict(cv.out, s = bestlam, newx = x_ridge[test_ridge,])

mean((ridge.pred - y.test_ridge)^2) %>% kable()

```

**Answer continued:** The coefficient outputs for the lambda that minimizes MSE are then expressed below. Here a number of coefficients remain in the function when predicting the number of applications received using a ridge regression.

```{r}
set.seed(9292)

out <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], alpha = 0)

predict(out, type = "coefficients", s = bestlam)[1:18, ]

```

#### (d) 

**Question:** Fit a lasso model on the training set, with lambda chosen by cross validation. Report the test error obtained, along with the number of non-zero coefficient estimates.

**Answer:** The process of creating the lasso is largely similar to that of the ridge though here our initial glmnet() takes an alpha of 1. The plot below this indicates that depending on our choice of tuning parameter lambda, certain coefficients will become zero.

```{r}
set.seed(9292)

lasso.mod <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], 
                    alpha = 1, lambda = grid)

plot(lasso.mod)

```


**Answer continued:**  We continue this analysis by cross validation using cv.glmnet() with an alpha of 1 to find the coefficients that again, minimize the MSE.

```{r}

set.seed(9292)

cv.out_2 <- cv.glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], alpha = 1) 

plot(cv.out_2)

```

**Answer continued:** After this cross validation has been calculated we then access the lambda that minimizes MSE. We then use the predict function to calculate predictions for our lasso using the lambda that minimizes MSE this gets a MSE of *1,082,717*.

```{r}

set.seed(9292)

bestlam <- cv.out_2$lambda.min

lasso.pred <- predict(cv.out_2, s = bestlam, newx = x_ridge[test_ridge,])

mean((lasso.pred - y.test_ridge)^2) %>% kable()

```

**Answer continued:** Finally we calculate and show the coefficients for our equation with the lasso reducing our number of parameters by moving coefficients towards zero that have low impacts.

```{r}

set.seed(9292)

out <- glmnet(x_ridge[train_ridge, ], y_ridge[train_ridge], 
              alpha = 1, lambda = grid)

lasso.coef <- predict(out, type = "coefficients", s = bestlam)[1:18, ]

lasso.coef


```

### (g) 

**Basis:** Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?

**Answer:** The results are relatively similar across the 3 models. Mean squared errors are close to a million with some methods improving by about 10,000 though still making quite high MSE. No method appears to significantly alter the results of the regression with lasso not appearing to move any coefficients towards zero and drop the impact of any particular variaable.

### Ch. 12, Exercise 8

**Basis:** In Section 12.2.3, a formula for calculating PVE was given in Equation 12.10. We also saw that the PVE can be obtained using the sdev output of the prcomp() function. On the USArrests data, calculate PVE in two ways:

#### Part (a) 

**Basis:** Using the sdev output of the prcomp() function, as was done in Section 12.2.3.

**Answer:** Using sdev and prcomp() on USA arrests on USArrests as in 12.2.3 we first start by using our principal component function prcomp(), inside of this we add USArrests and scale our variables for consistency. These princpal components are represented by pr.out

```{r} 

pr.out <- prcomp(USArrests, scale = TRUE)

pr.out

```

**Answer 2:** Next we see 4 names in the output of our principal component function results, one of which is sdev which will need to calculate our PVE.

```{r} 

names(pr.out)

```

```{r}

dim(pr.out$x)

```

```{r}

biplot(pr.out, scale = 0)

```

**Answer 3:** We see our 4 sdev, or standard deviations for Murder, Assault, UrbanPop and Rape.

```{r}
pr.out$sdev %>% kable()
```

**Answer 4:** First we square our sdevs to acquire our variance.

```{r}

pr.var <- pr.out$sdev^2

pr.var

```

**Answer 5:** Next we divide these values of our variance by their sum in the denominator.

```{r}

pve <- pr.var / sum(pr.var)

pve

```

#### (b) 

**Basis:** By applying Equation 12.10 directly. That is, use the prcomp() function to compute the principal component loadings. Then, use those loadings in Equation 12.10 to obtain the PVE.

**Answer:** In order to obtain PVE directly we must first obtain our loadings using prcomp, which I caclualted earlier as pr.out <- prcomp(USArrests, scale = TRUE). The loading's are represented as the rotations of the principal component. I then multiply these by the matrix of scaled USArrests observations which have each state represented with each of the 4 variables, Using matrix multiplication on the USArrests data and loading's we have the first part of the top of 12.10, this is then squared and summed. This is then divided by the sum of the previously calculated squared matrix multipliation. This results in identical PVE's to those obtained by code alone.

```{r}

USArrests_scaled <- scale(USArrests)

#dim(USArrests_scaled)

loadings <- pr.out$rotation

# dim(loadings)


loadings_times_var <- (USArrests_scaled %*% loadings)

sum_squares <- apply(loadings_times_var^2, 2, sum)

sum_squares / sum(sum_squares)

```


### Ch. 12, Exercise 9

**Basis:** Consider the USArrests data. We will now perform hierarchical clustering on the states.

#### (a) 

**Basis:** Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.

**Answer:** Below I use hierarchical clustering on the US states, this is done using the dist() function between states in the USArrests matrix, by default this uses eucliean distance, no diagonal or upper portion of the data which would repeat the values below the matrix diagonal. Next I use hclust() which performs hierarchical clustering with complete linkage.

```{r}

# First dist() is used on USArrests to compute euclidean 
# distance between the observations in each state.

USArrests.dist <- dist(USArrests)

# Next I acquire the labels from the dendrogram graph using the labels
# From USArrests
USArrest.labs <- rownames(USArrests)

plot(hclust(USArrests.dist, method="complete"), xlab = "", sub = "", ylab = "", main = "Complete Linkage")

```

#### (b) 

**Basis:** Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?

**Answer:** Below I have used the function cutree, which cuts a tree specified in an hclust() into a spcified number of groups, in this case 3. After running this function I manually extract the rows from the table that represent state membership into each group with having a 1, as opposed to a zero in that group representing that the observation belongs to it. 

```{r}


set.seed(8437)

# First I use clust() on our USArrests euclidean distance with complete linkage
# Next I use cutree() to cut the USArrests.dist data into 3 clusters.

hc.clusters <- cutree(hclust(USArrests.dist, method="complete"), 3)

# I then add these to a table with each observations repective label

table_with_3_clusters <- table(hc.clusters, USArrest.labs)

# I then take all 3 rows on their own

table_with_3_clusters_r1 <- table_with_3_clusters[1,]

table_with_3_clusters_r2 <- table_with_3_clusters[2,]

table_with_3_clusters_r3 <- table_with_3_clusters[3,]

# I finally take these rows and find when they are eqivalent to one since 
# each row is a group and 1 indicates the state belongs to it.

table_with_3_clusters_r1[table_with_3_clusters_r1 == 1] %>% kable()

table_with_3_clusters_r2[table_with_3_clusters_r2 == 1] %>% kable()

table_with_3_clusters_r3[table_with_3_clusters_r3 == 1] %>% kable()


```


**Answer 2:** I then plot the results below using a red line at a level which creates 3 groups.

```{r}

plot(hclust(USArrests.dist, method="complete"), labels = USArrest.labs) 
abline(h = 100, col = "red")
```

#### (c) 

**Basis:** Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.

**Answer:** In this problem I follow largely the same as before with the exception of the scaling of the USArrests data to have a mean of 0 and a standard deviation of 1. This is first done with the scale() function.

```{r}

set.seed(213)

# First I use the scale function to scale the observations to the std of 1 
# and mean of zero this is done with scale()

USArrests_scaled_c <- scale(USArrests)
 
# Next I compute the distance with dist() specifying that it will be using
# The euclidean method, though this is the default.

USArrests_scaled_c.dist <- dist(USArrests_scaled_c, method = "euclidean" )

# Next I find labels using the rownames from the dataset.

USArrests_scaled.labs <- rownames(USArrests_scaled_c)

# Finally I use the hclust() function to apply hierarchical clustering to the 
# data, in this case using complete clustering.

hclust_USArrests_scaled_c <- hclust(USArrests_scaled_c.dist, method="complete")

plot(hclust_USArrests_scaled_c, xlab = "", sub = "", ylab = "",
labels = USArrests_scaled.labs, main = "Complete Linkage")

```

#### (d) 

**Basis:** What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.

**Answer:** Scaling the variables breaks them into far more small clusters and likely distinguishes them more closely at more levels. This is likely as distinguishing between groups will be based on a common scale that reduces possible scale differences within the dataset. As a result of this I would argue that scaling is beneficial, though the data is per 100,000 residents so it is scaled to some extent, certain states have far higher density and population so the meaning of per-100,000 can have differing impact. Though it likely helps to have all variables on the same scale it is not as useful as it would be in a scenario with variables using a different basis of measurement from one another, such as total deaths and deaths per-100,000 residents, or percent of violent crimes in the state. I


### Ch. 12, Exercise 10

**Basis:** In this problem, you will generate simulated data, and then perform PCA and K-means clustering on the data.

#### (a) 

**Basis:** Generate a simulated data set with 20 observations in each of three classes (i.e. 60 observations total), and 50 variables.

Hint: There are a number of functions in R that you can use to generate data. One example is the rnorm() function; runif() is another option. Be sure to add a mean shift to the observations in each class so that there are three distinct classes.

**Answer:** In order to generate 20 observations of 3 classes I used the code and random generation functions below.

```{r}

set.seed(280)


# First I take an rnorm with 20 observation and 3 different means

# and 3 different standard deviations to observe the effect of this

# In generating principal components, I then use replicate to run this

# simulation for 50 variables so it is replacted 50 times

df_1_10 <- as.data.frame(replicate(50, rnorm(n=20, mean=0, sd = 2)))

colnames(df_1_10)[1] <- "V1"


# I repeat this again with different mean and sd

df_2_10<-as.data.frame(replicate(50, rnorm(n=20, mean=0.5, sd= 0.8)))

colnames(df_2_10)[1] <- "V1"

# I then combine data frams

df_2_10_combined <- rbind(df_1_10, df_2_10)

# I repeat this again with different mean and sd

df_3_10 <- as.data.frame(replicate(50, rnorm(n=20, mean=1)))

colnames(df_3_10)[1] <- "V1"

# All data frames are then combined, after which I label the rows as to 

# know which class they are in.

big_df_combined_10 <- rbind(df_2_10_combined, df_3_10)

big_df_combined_10$groups <- NA 

big_df_combined_10$groups[1:20] <- "1"

big_df_combined_10$groups[21:40] <- "2"

big_df_combined_10$groups[41:60] <- "3"

```


#### (b) 

**Basis:** Perform PCA on the 60 observations and plot the first two principal component score vectors. Use a different color to indicate the observations in each of the three classes. If the three classes appear separated in this plot, then continue on to part (c). If not, then return to part (a) and modify the simulation so that there is greater separation between the three classes. Do not continue to part (c) until the three classes show at least some separation in the first two principal component score vectors.

**Answer:** Next I perform PCA on the 60 randomly generated observations.

```{r}

# I perform principal components on all by column 51 as this column has the 

# Group numbers in it for later reference.

pca_10_b <- prcomp(big_df_combined_10[-51])

```

**Answer Continued:** Using the base R plot function below we see that the components have separated noticeably. The black points on the right have the highest standard deviation of 2 while the red in the middle have a smaller standard deviation of 0.8 while the green points on the far right have an standard deviation of 1.

```{r}

plot(pca_10_b$x[, 1:2], pch=17,  col = (big_df_combined_10$groups),
xlab = "P1", ylab = "P2")

```

#### (c) 

**Basis:** Perform K-means clustering of the observations with K = 3. How well do the clusters that you obtained in K-means clustering compare to the true class labels?

**Answer:** Though it is difficult to interpret in some ways due to class labels, it appears as if K-means put all of the correct observations in group 2 and 3 but mis-classified many in group 1, for this see it put 9 in the incorrect groups. This is understandable as this group was created in my simulated data using the highest variance so it is concieveable that if its standard deviation was lower the algorithm would classify it more accurately, however in this case it was often incorrect.

```{r}

set.seed(928)

km.out <- kmeans(big_df_combined_10[-51], centers=3) 

km.clusters <- km.out$cluster

table(km.clusters, big_df_combined_10$groups)

```

#### (d) 

**Basis:** Perform K-means clustering with K = 2. Describe your results. 

**Answer:** In this case using 2 cluster will force the clusters to go from 2 to 3 which we see in the km.out$cluster output being only groups one and two. 

```{r}
set.seed(75903)

km.out <- kmeans(big_df_combined_10[-51], centers=2,  nstart = 20) 

km.clusters <- km.out$cluster

table(km.clusters, big_df_combined_10$groups)

```

#### (e) 

**Basis:**Now perform K-means clustering with K = 4, and describe your results.

```{r}
set.seed(98367)

km.out <- kmeans(big_df_combined_10[-51], 4)

km.clusters <- km.out$cluster

table(km.clusters, big_df_combined_10$groups)

```

#### (f) 

**Basis:** Now perform K-means clustering with K = 3 on the first two principal component score vectors, rather than on the raw data. That is, perform K-means clustering on the 60 × 2 matrix of which the first column is the first principal component score vector, and the second column is the second principal component score vector. Comment on the results.

```{r}

k.means.pcr <- kmeans(pca_10_b$x[,1:2], 3)

km.clusters <- k.means.pcr$cluster

set.seed(98367)

big_df_no_51<-big_df_combined_10[-51]

table(km.clusters, big_df_combined_10$groups)

```


#### (g)

**Basis:** Using the scale() function, perform K-means clustering with K = 3 on the data after scaling each variable to have standard deviation one. How do these results compare to those obtained in (b)? Explain.

```{r}

km.out_g <- kmeans(scale(big_df_combined_10[-51]), centers=3) 

km_g.clusters <- km.out_g$cluster 

table(km_g.clusters, big_df_combined_10$groups )

```

## Written Questions:

### Question 1:

**Question:** Based on reading for this class, what are the main risks of algorithmic models (machine learning) from a fairness and justice perspective? What forms of modeling/learning and/or sorts of data may pose particular risks of harm? What general strategies might be employed to mitigate these risks? Can you think of any methodological advances that might be pursued by computational and statistical researchers in order to help data analysts make "ethically derived" predictions?

**Answer:** Based on the readings for this class there are a great number of potential fairness and justice risks of the use of models in many areas of life. The primary ones are the reinforcement of systems that perpetuate inequality or direct harm to a group through self-reinforcing predictions and punitive or financial outcomes. As O'neal writes the collegiate arms race has inflated prices far beyond inflation while predicted outcomes of policing models ignore white collar crimes and encourage increased policing which leads to more punitive outcomes which have far reaching social and financial consequences. 

The algorithms that appear to carry the most risk are predictive algorithms in the financial and social spaces. Predicting issues such as recidivism and crime are inevitable going to reinforce the structure in the world they are meant to predict and since algorithms are stagnant they cannot evolve to do anything but reflect what they are engineered to do.

Mitigation of risk is difficult in modeling as algorithms are inherently reductive and based on the past, as a result there are areas that they simply should not be used in, perhaps the methodological advance could be 

In earlier chapters, while no acknowledging fault fault, the author explains the widespread harm caused by predictive algorithms in finance that assisted in creating the 2008 housing bubble. When the bubble burst the burden of the fallout was felt by poor and middle class households who were given sub prime loan and subsequently foreclosed, lost their savings and credit. As O'neal points out these issues are due to the opaque nature in being often non-interpretable due to complexity and inherent mathematical structure. In addition the scale by which certain algorithms such as recidivism and policing.

(Project update) Data Exploration: Use appropriate methods to explore the data you plan to use for your project. (It's okay if you change your mind and use a different data set later; this exercise may help you decide.) Include some form of visualization(s) and indications of the unit of analysis, the levels of measurements (e.g.: "9 categorical features–7 dichotomous and 2 with more than two categories– and 14 numerical/quantitative features, for a total of 23), univariate statistics helping characterize the distribution of the features and of the outcome variable (label) you wish to predict if you will be conducting supervised learning for the project. Convey something about relationships among features (predictors). If you are going to use these for prediction, identify any preprocessing you may need to do and options available for cleaning up/transforming data and/or dimension reduction. (Submit with your group or with explanation of why you must work alone.)



